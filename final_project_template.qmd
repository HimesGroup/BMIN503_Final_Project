---
title: "Comparison of Machine Learning Models to Predict Biomarker-based PARDS Subphenotypes Using Clinical Data"
subtitle: "BMIN503/EPID600 Final Project"
author: "Daniel Balcarcel"
format: html
editor: visual
embed-resources: true
---

------------------------------------------------------------------------

### Overview

I have previously developed and validated an XGBoost machine learning model to predict biomarker-based Pediatric ARDS Subphenotypes using clinical data available in the electronic health record. For my final project, I will utilize additional supervised machine learning techniques, including k-nearest neighbors, Random Forest, and neural networks package, to evaluate the performance of these models in comparison to XGBoost.

### Introduction

Pediatric Acute Respiratory Distress Syndrome (ARDS) is a condition that affects 10% of children admitted to the Pediatric Intensive Care Unit (PICU) and carries a mortality as high as 33% in severe cases. Despite a high burden of disease and extensive research, treatment of this condition remains supportive and there is a complete lack of any targeted therapies. The lack of specific treatments is thought to be due in large part to inherent heterogeneity in this patient population. Recently, two distinct ARDS subphenotypes (hypoinflammatory and hyperinflammatory) have been identified in multiple pediatric and adult cohorts using a combination of biomarkers and clinical variables, with some evidence of differential response to therapeutic interventions. The clinical use of these subphenotypes has been limited by the cost and lack of immediate availability of biomarkers. Machine learning algorithms have been shown to predict ARDS subphenotypes in adults using exclusively electronic health record (EHR) data. We aimed to develop an EHR-based prediction model to identify Pediatric ARDS subphenotypes within 24 hours of diagnosis.

A multidisciplinary approach is necessary to untangle the heterogeneity of ARDS. Due to it's broad clinical criteria, patients with ARDS are a diverse group. Fortunately, there is a wealth of clinical data available in the EHR that can be leveraged to define more precise subtypes within ARDS. Using machine learning techniques and clinical data, models can be created to predict subtypes of ARDS without the high cost and delay of obtaining biomakers. Ultimately, the use of clinical informatics could be used to rapidly identify ARDS subphenotypes with tools embdeded in the EHR.

### Methods

Describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why.

Load packages needed for the analysis

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(here)
library(dbplyr)
library(DBI)
library(readxl)
library(glue)
library(imputeTS)
library(imputeTS)
library(xgboost)
library(Matrix)
library(tidyverse)
library(mice)
library(caret)
library(randomForest)
library(missForest)
library(pROC)
library(ROCR)
library(magrittr)
library(car)
library(kernlab)
library(glmnet)
library(tidymodels)
library(yardstick)
library(parsnip)
library(ranger)
library(kknn)
library(discrim)
library(klaR)
```

Load training data, clean it

```{r}

ARDS_train_raw <- read.csv("H:/Research/ARDS Extra cohort/Final Data/PARDS_1_Training.csv", fileEncoding="UTF-8-BOM")
  
ARDS_validate_raw <- read.csv("H:/Research/ARDS Extra cohort/Final Data/PARDS_2_Validation.csv",fileEncoding="UTF-8-BOM")

ARDS_train <- ARDS_train_raw

#Remove incomplete data
ARDS_validate <- ARDS_validate_raw |>
  drop_na(Class)

#Change Sex to an integer
#Make M=0, F=1
ARDS_train$SEX <- if_else(ARDS_train$SEX == "M", "0", "1")
ARDS_train$SEX <- as.integer(ARDS_train$SEX)

ARDS_validate$SEX <- if_else(ARDS_validate$SEX == "M", "0", "1")
ARDS_validate$SEX <- as.integer(ARDS_validate$SEX)

#Ensure all clinical variables are numeric
# Function to convert specified columns of a dataframe to numeric
convert_columns_to_numeric <- function(dataframe, columns) {
  for (column in columns) {
    dataframe[[column]] <- as.numeric(dataframe[[column]])
  }
  return(dataframe)
}

# List of column names to convert
columns_to_convert <- c("ALC_Max", "ALT_Max", "ANC_Max", "AST_Max", "BUN_Max", 
                        "Fibrinogen_Max", "Glucose_Max", "Platelets_Max", "Sodium_Max", "GGT_Max",
                        "ALC_Min", "ALT_Min", "ANC_Min", "AST_Min", "BUN_Min", 
                        "Fibrinogen_Min", "Glucose_Min", "Platelets_Min", "Sodium_Min", "GGT_Min",
                        "Systolic.max", "Diastolic.max", "SpO2.max",
                        "MAP.max", "Systolic.min", "Diastolic.min", "SpO2.min", "MAP.min")

# Convert columns for ARDS_train
ARDS_train <- convert_columns_to_numeric(ARDS_train, columns_to_convert)

# Convert columns for ARDS_validate
ARDS_validate <- convert_columns_to_numeric(ARDS_validate, columns_to_convert)

#Make CLass 0 and 1
    
ARDS_train$Class <- as.numeric(ARDS_train$Class) - 1

ARDS_validate$Class <- as.numeric(ARDS_validate$Class)

```

Impute Missing Variables

```{r}

#Training
ARDS_train.imp <- missForest(ARDS_train)

ARDS_train <- ARDS_train.imp$ximp



#Validation

ARDS_validate.imp <- missForest(ARDS_validate)

ARDS_validate <- ARDS_validate.imp$ximp

train <- ARDS_train
test <- ARDS_validate

```

#Support Vector Machine

```{r}


train_svm <- test |>
  mutate(Class = factor(Class, levels = c(0,1)))

set.seed(1234)
training.folds <- vfold_cv(train_svm, v = 10)
training.folds

svm_cls_spec <- 
  svm_linear(cost = 1) |> 
  set_engine("kernlab") |>
  set_mode("classification") 
svm_cls_spec

svm_workflow <-
  workflow() |>
  add_model(svm_cls_spec) |>
  add_formula(Class ~ .)

set.seed(1234)

svm_fit_cv <-
  svm_workflow |>
  fit_resamples(training.folds, 
                control = control_resamples(save_pred = TRUE))

collect_metrics(svm_fit_cv)

# calculate AUC
svm_auc <- svm_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))

print(svm_auc)


```

#K nearest neighbors

```{r}



# Assuming train_svm is your training data and Class is your target variable
train_knn <- test |>
  mutate(Class = factor(Class, levels = c(0,1)))

set.seed(1234)
training.folds_knn <- vfold_cv(train_knn, v = 10)

# Define the KNN model specification
knn_spec <- 
  nearest_neighbor(neighbors = 5) |> # you can change the number of neighbors
  set_engine("kknn") |>
  set_mode("classification") 
knn_spec

# Create a workflow with the KNN model
knn_workflow <-
  workflow() |>
  add_model(knn_spec) |>
  add_formula(Class ~ .)

set.seed(1234)


# Fit the KNN model using cross-validation
knn_fit_cv <-
  knn_workflow |>
  fit_resamples(training.folds_knn, 
                control = control_resamples(save_pred = TRUE))

# Collect metrics
collect_metrics(knn_fit_cv)

# calculate AUC
knn_auc <- knn_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))

print(knn_auc)


```

#XGBoost

```{r}



train_xgb <- train |>
  mutate(Class = factor(Class, levels = c(0,1)))

set.seed(1234)
training.folds_xgb <- vfold_cv(train_xgb, v = 10)


# Define the XGBoost model specification
xgb_spec <- 
  boost_tree(
    trees = 1000,                # Number of trees (adjust as needed)
    tree_depth = 6,              # Depth of trees (adjust as needed)
    min_n = 10,                  # Minimum observations in nodes (adjust as needed)
    learn_rate = 0.01            # Learning rate (adjust as needed)
  ) |> 
  set_engine("xgboost") |>
  set_mode("classification") 
xgb_spec

# Create a workflow with the XGBoost model
xgb_workflow <-
  workflow() |>
  add_model(xgb_spec) |>
  add_formula(Class ~ .)

set.seed(1234)

# Fit the XGBoost model using cross-validation
xgb_fit_cv <-
  xgb_workflow |>
  fit_resamples(training.folds_xgb, 
                control = control_resamples(save_pred = TRUE))

# Collect metrics
collect_metrics(xgb_fit_cv)


# calculate AUC
xgb_auc <- xgb_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))

print(xgb_auc)

```

#Random Forest Model

```{r}


# Assuming train_svm is your training data and Class is your target variable
train_rf <- train |>
  mutate(Class = factor(Class, levels = c(0,1)))

set.seed(1234)
training.folds_rf <- vfold_cv(train_rf, v = 10)

# Define the Random Forest model specification
rf_spec <- 
  rand_forest(
    trees = 1000,         # Number of trees (adjust as needed)
    mode = "classification"
  ) |> 
  set_engine("ranger", importance = "impurity") |> 
  set_mode("classification") 
rf_spec

# Create a workflow with the Random Forest model
rf_workflow <-
  workflow() |>
  add_model(rf_spec) |>
  add_formula(Class ~ .)

set.seed(1234)

# Fit the Random Forest model using cross-validation
rf_fit_cv <-
  rf_workflow |>
  fit_resamples(training.folds_rf, 
                control = control_resamples(save_pred = TRUE))

# Collect metrics
collect_metrics(rf_fit_cv)


# calculate AUC
rf_auc <- rf_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))

print(rf_auc)


```

#Naive Bayes

```{r}

# Assuming train_svm is your training data and Class is your target variable
train_nb <- train |>
  mutate(Class = factor(Class, levels = c(0,1)))

set.seed(1234)
training.folds_nb <- vfold_cv(train_nb, v = 10)

# Define the Naive Bayes model specification
nb_spec <- 
  naive_Bayes() |> 
  set_engine("klaR") |> 
  set_mode("classification") 
nb_spec


# Create a workflow with the Naive Bayes model
nb_workflow <-
  workflow() |>
  add_model(nb_spec) |>
  add_formula(Class ~ .)

set.seed(1234)

# Fit the Naive Bayes model using cross-validation
nb_fit_cv <-
  nb_workflow |>
  fit_resamples(training.folds_nb, 
                control = control_resamples(save_pred = TRUE))

# Collect metrics
collect_metrics(nb_fit_cv)

# calculate AUC
nb_auc <- nb_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))

print(nb_auc)

```

### Results

Plot all of the ROC Curves together

#Plot the AUROC Curves together

```{r}

#SVM
svm_results_cv <- svm_fit_cv |> 
  collect_predictions()

#ROC Curve
svm_roc_curve <- roc_curve(svm_results_cv, truth = Class, .pred_0)

#KNN
knn_results_cv <- knn_fit_cv |> 
  collect_predictions()

#ROC Curve
knn_roc_curve <- roc_curve(knn_results_cv, truth = Class, .pred_0)

#XGBoost
xgb_results_cv <- xgb_fit_cv |> 
  collect_predictions()

#Calculate the ROC curve
xgb_roc_curve <- roc_curve(xgb_results_cv, truth = Class, .pred_0)


#Random Forest
rf_results_cv <- rf_fit_cv |> 
  collect_predictions()

#Calculate the ROC curve
rf_roc_curve <- roc_curve(rf_results_cv, truth = Class, .pred_0)


#Naive Bayes
nb_results_cv <- nb_fit_cv |> 
  collect_predictions()

#Calculate the ROC curve
nb_roc_curve <- roc_curve(nb_results_cv, truth = Class, .pred_0)


#Plot
ggplot() +
    geom_line(data = svm_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "Support Vector Machine"), lwd = 1.5) +
    geom_line(data = knn_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "K nearest neighbor"), lwd = 1.5) +
    geom_line(data = xgb_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "XGBoost"), lwd = 1.5) + 
    geom_line(data = rf_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "Random Forest"), lwd = 1.5) +
    geom_line(data = nb_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "Naive Bayes"), lwd = 1.5) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("Support Vector Machine" = "red", "K nearest neighbor" = "blue", "XGBoost" = "green", "Random Forest"= "orange", "Naive Bayes" = "pink")) +
  labs(title = "ROC Curve Comparison",
       x = "1 - Specificity",
       y = "Sensitivity",
       color = "Model") +
  theme_minimal()

```

Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.

### Conclusion

My study demonstrated that the most accurate prediction model was Random FOrest, followed in order by XGBoost, Naive Bayes, Support Vector Machine, and K nearest neighbor.
