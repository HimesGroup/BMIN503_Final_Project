---
title: "Comparison of Machine Learning Models to Predict Biomarker-based PARDS Subphenotypes Using Clinical Data"
subtitle: "BMIN5030/EPID600 Final Project"
author: "Daniel Balcarcel"
format: html
editor: visual
embed-resources: true
---

------------------------------------------------------------------------

# Overview

In a previous project, I determined that XGBoost could be used to predict biomarker-based Pediatric ARDS Subphenotypes using just readily available clinical data that is easily accesible in the electronic health record. For my final project, my objective was to determine the relative accuracy of other supervised machine learning techniques, including Support Vector Machine (SVM), k-nearest neighbors, Naive Bayes, and Random Forest, in predicting ARDS subphenotype.

# Introduction

Despite a high burden of disease, more than half a century of research, and dozens of randomized control trials, the management of pediatric Acute Respiratory Distress Syndrome (ARDS) remains entirely supportive. The lack of effective, targeted therapies is partially due to the reality that patients with ARDS are a heterogenous group with a wide range of inciting factors, diverse clinical courses, and divergent outcomes. One strategy to overcome this heterogeneity, is to identify more homogenous groups within ARDS, or subtypes, which could allow us to identify more precise management strategies and aid trials through prognostic and predictive enrichment. While subtyping has been attempted many times with limited success, in 2014 Carolyn Calfee et al. developed the most promising strategy to date when they identified two inflammatory subphenotypes of ARDS, termed the hyper- and hypoinflammatory subphenotypes, using latent class analysis (LCA) and a combination of biomarkers and clinical data. These subphenotypes have been consistently identified in adult and pediatric cohorts. What is most promising about this subtyping strategy is that inflammatory subphenotype appears to correlate with response to specific therapies, such as high vs low PEEP, liberal vs conservative fluid strategy, and simvastatin vs placebo. The only barrier to using this subtyping strategy is that it requires collecting inflammatory cytokines, such as TNF-r1, IL-6, and IL-8 which requires a significant amount of time and money and limits its clinical utility. Pratik Sinha et al. identified a potential solution to this problem in 2020 when they used a machine learning clinical classifier model to predict the latent class analysis-derived, biomarker-based ARDS subphenotype using just readily available clinical data.

Untangling the heterogeneity of ARDS has been a serious challenge for intensivists over the last several decades. Fortunately, breakthroughs in laboratory medicine and the use of inflammatory cytokines have improved our understanding of ARDS and its various pathophysiology. The use of data science and machine learning could be the next step to discovering targeted therapies. Ultimately, a multidisciplinary approach that combines biomarkers, data science, and clinical informatics will be necessary to move the needle forward and improve outcomes in pediatric ARDS.

# Methods

##Load packages needed for the analysis

```{r message=FALSE, warning=FALSE}

library(tidyverse)
library(imputeTS)
library(xgboost)
library(tidyverse)
library(mice)
library(caret)
library(randomForest)
library(missForest)
library(pROC)
library(ROCR)
library(glmnet)
library(tidymodels)
library(yardstick)
library(ranger)
library(kknn)
library(discrim)
library(klaR)
library(parsnip)
library(kernlab)
```

##Loading, cleaning, organizing data

```{r warning=FALSE}

# Reading in training data from a CSV file located at a specified path. 
ARDS_train_raw <- read.csv("H:/Research/ARDS Extra cohort/Final Data/PARDS_1_Training.csv", fileEncoding="UTF-8-BOM")


# Reading in validation data from a CSV file.
ARDS_validate_raw <- read.csv("H:/Research/ARDS Extra cohort/Final Data/PARDS_2_Validation.csv",fileEncoding="UTF-8-BOM")

# Copying the raw training data into a new variable to preserve the original dat.
ARDS_train <- ARDS_train_raw

# Copying the raw validation data and then using the drop_na function from the tidyr package to remove rows where 'Class' is NA
ARDS_validate <- ARDS_validate_raw |>
  drop_na(Class)

# Converting the 'SEX' column in training and validation data from "M"/"F" to "0"/"1" and then to integers.
ARDS_train$SEX <- if_else(ARDS_train$SEX == "M", "0", "1")
ARDS_train$SEX <- as.integer(ARDS_train$SEX)

ARDS_validate$SEX <- if_else(ARDS_validate$SEX == "M", "0", "1")
ARDS_validate$SEX <- as.integer(ARDS_validate$SEX)

# Defining a function to convert specified columns in a dataframe to numeric. 
convert_columns_to_numeric <- function(dataframe, columns) {
  for (column in columns) {
    dataframe[[column]] <- as.numeric(dataframe[[column]])
  }
  return(dataframe)
}

# Specifying the columns that need to be converted to numeric.
columns_to_convert <- c("ALC_Max", "ALT_Max", "ANC_Max", "AST_Max", "BUN_Max", 
                        "Fibrinogen_Max", "Glucose_Max", "Platelets_Max", "Sodium_Max", "GGT_Max",
                        "ALC_Min", "ALT_Min", "ANC_Min", "AST_Min", "BUN_Min", 
                        "Fibrinogen_Min", "Glucose_Min", "Platelets_Min", "Sodium_Min", "GGT_Min",
                        "Systolic.max", "Diastolic.max", "SpO2.max",
                        "MAP.max", "Systolic.min", "Diastolic.min", "SpO2.min", "MAP.min")


# Applying the function to convert columns in both training and validation datasets.
ARDS_train <- convert_columns_to_numeric(ARDS_train, columns_to_convert)
ARDS_validate <- convert_columns_to_numeric(ARDS_validate, columns_to_convert)

# Converting the 'Class' column in both datasets to numeric and then defining hypoinflammatory class to 0 and hyperinflammatory class to 1. This needs to be done for the XGBoost package, but class will need to be factors for other supervised ML techniques
ARDS_train$Class <- as.numeric(ARDS_train$Class) - 1

ARDS_validate$Class <- as.numeric(ARDS_validate$Class)


```

##Imputation of missing variables

```{r warning=FALSE}

#Imputation of training data with random forest
ARDS_train.imp <- missForest(ARDS_train)
ARDS_train <- ARDS_train.imp$ximp



#Imputation of validation data with random forest
ARDS_validate.imp <- missForest(ARDS_validate)
ARDS_validate <- ARDS_validate.imp$ximp


#Create a new train and test dataframes that will be used for supervised ML
train <- ARDS_train
test <- ARDS_validate

#Turn Class into a factor
train <- train |>
  mutate(Class = factor(Class, levels = c(0,1)))

```

##Support Vector Machine training with 10-fold cross validation

```{r}
# Set the seed for random number generation to ensure reproducibility
set.seed(1234)

# Placeholder for training data. Replace 'train' with actual training dataset
train_svm <- train 

# Generate 10-fold cross-validation folds from the training dataset
training.folds <- vfold_cv(train_svm, v = 10)

# Define a support vector machine (SVM) model for classification
svm_cls_spec <- 
  svm_linear(cost = 1) |> 
  set_engine("kernlab") |>
  set_mode("classification") 

# Create a workflow with the SVM model specification
svm_workflow <-
  workflow() |>
  add_model(svm_cls_spec) |>
  add_formula(Class ~ .)

# Fit the SVM model to the training folds with resampling and save the predictions in the process
svm_fit_cv <-
  svm_workflow |>
  fit_resamples(training.folds, 
                control = control_resamples(save_pred = TRUE))

# Calculate the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC)
svm_auc <- svm_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))


```


##K nearest neighbors with 10-fold cross validation

```{r}

# Set the seed for random number generation to ensure reproducibility
set.seed(1234)

# Placeholder for training data. Replace 'train' with actual training dataset
train_knn <- train 

training.folds_knn <- vfold_cv(train_knn, v = 10)

# Define the KNN model specification
knn_spec <- 
  nearest_neighbor(neighbors = 5) |> # you can change the number of neighbors
  set_engine("kknn") |>
  set_mode("classification") 

# Create a workflow with the KNN model
knn_workflow <-
  workflow() |>
  add_model(knn_spec) |>
  add_formula(Class ~ .)

# Fit the KNN model using cross-validation
knn_fit_cv <-
  knn_workflow |>
  fit_resamples(training.folds_knn, 
                control = control_resamples(save_pred = TRUE))

# calculate AUC
knn_auc <- knn_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))


```

##XGBoost with 10-fold cross validation

```{r}

# Set the seed for random number generation to ensure reproducibility
set.seed(1234)

# Placeholder for training data. Replace 'train' with actual training dataset
train_xgb <- train 

training.folds_xgb <- vfold_cv(train_xgb, v = 10)

# Define the XGBoost model specification
xgb_spec <- 
  boost_tree(
    trees = 1000,                # Number of trees (adjust as needed)
    tree_depth = 6,              # Depth of trees (adjust as needed)
    min_n = 10,                  # Minimum observations in nodes (adjust as needed)
    learn_rate = 0.01            # Learning rate (adjust as needed)
  ) |> 
  set_engine("xgboost") |>
  set_mode("classification") 

# Create a workflow with the XGBoost model
xgb_workflow <-
  workflow() |>
  add_model(xgb_spec) |>
  add_formula(Class ~ .)

# Fit the XGBoost model using cross-validation
xgb_fit_cv <-
  xgb_workflow |>
  fit_resamples(training.folds_xgb, 
                control = control_resamples(save_pred = TRUE))

# calculate AUC
xgb_auc <- xgb_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))


```

##Random Forest Model with 10-fold cross validation

```{r}

# Set the seed for random number generation to ensure reproducibility
set.seed(1234)

# Placeholder for training data
train_rf <- train 


training.folds_rf <- vfold_cv(train_rf, v = 10)

# Define the Random Forest model specification
rf_spec <- 
  rand_forest(
    trees = 1000,         # Number of trees (adjust as needed)
    mode = "classification"
  ) |> 
  set_engine("ranger", importance = "impurity") |> 
  set_mode("classification") 

# Create a workflow with the Random Forest model
rf_workflow <-
  workflow() |>
  add_model(rf_spec) |>
  add_formula(Class ~ .)


# Fit the Random Forest model using cross-validation
rf_fit_cv <-
  rf_workflow |>
  fit_resamples(training.folds_rf, 
                control = control_resamples(save_pred = TRUE))

# calculate AUC
rf_auc <- rf_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))


```

##Naive Bayes with 10-fold cross validation

```{r}

# Set the seed for random number generation to ensure reproducibility
set.seed(1234)

# Placeholder for training data
train_nb <- train 

training.folds_nb <- vfold_cv(train_nb, v = 10)

# Define the Naive Bayes model specification
nb_spec <- 
  naive_Bayes() |> 
  set_engine("klaR") |> 
  set_mode("classification") 


# Create a workflow with the Naive Bayes model
nb_workflow <-
  workflow() |>
  add_model(nb_spec) |>
  add_formula(Class ~ .)


# Fit the Naive Bayes model using cross-validation
nb_fit_cv <-
  nb_workflow |>
  fit_resamples(training.folds_nb, 
                control = control_resamples(save_pred = TRUE))

# calculate AUC
nb_auc <- nb_fit_cv|>
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  summarize(mean_auc = mean(mean))


```


# Results

##AUROC Plots for Training Set and 10-fold cross validation

```{r}

#SVM AUROC calculations
svm_results_cv <- svm_fit_cv |> 
  collect_predictions()
svm_roc_curve <- roc_curve(svm_results_cv, truth = Class, .pred_0)

#KNN AUROC calculations
knn_results_cv <- knn_fit_cv |> 
  collect_predictions()
knn_roc_curve <- roc_curve(knn_results_cv, truth = Class, .pred_0)

#XGBoost AUROC calculations
xgb_results_cv <- xgb_fit_cv |> 
  collect_predictions()
xgb_roc_curve <- roc_curve(xgb_results_cv, truth = Class, .pred_0)


#Random Forest AUROC calculations
rf_results_cv <- rf_fit_cv |> 
  collect_predictions()
rf_roc_curve <- roc_curve(rf_results_cv, truth = Class, .pred_0)


#Naive Bayes AUROC calculations
nb_results_cv <- nb_fit_cv |> 
  collect_predictions()
nb_roc_curve <- roc_curve(nb_results_cv, truth = Class, .pred_0)

# Creating labels with AUC values
svm_label <- paste0("Support Vector Machine (AUC: ", round(svm_auc, 2), ")")
knn_label <- paste0("K nearest neighbor (AUC: ", round(knn_auc, 2), ")")
xgb_label <-paste0("XGBoost (AUC: ", round(xgb_auc, 2), ")")
rf_label <- paste0("Random Forest (AUC: ", round(rf_auc, 2), ")")
nb_label <-paste0("Naive Bayes (AUC: ", round(nb_auc, 2), ")")
  
  
# Plot

ggplot() +
    geom_line(data = svm_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "Support Vector Machine (AUC: 0.86)"), lwd = 1.5) +
    geom_line(data = knn_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "K nearest neighbor (AUC: 0.83)"), lwd = 1.5) +
    geom_line(data = xgb_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "XGBoost (AUC: 0.92)"), lwd = 1.5) + 
    geom_line(data = rf_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "Random Forest (AUC: 0.92)"), lwd = 1.5) +
    geom_line(data = nb_roc_curve, aes(x = 1 - specificity, y = sensitivity, color = "Naive Bayes (AUC: 0.88)"), lwd = 1.5) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("Support Vector Machine (AUC: 0.86)" = "red", "K nearest neighbor (AUC: 0.83)" = "blue", "XGBoost (AUC: 0.92)" = "green", "Random Forest (AUC: 0.92)"= "orange", "Naive Bayes (AUC: 0.88)" = "pink")) +
  labs(title = "AUROC Curves for Several Supervised ML models (Cross Validation)",
       x = "1 - Specificity",
       y = "Sensitivity",
       color = "Models") +
  theme_minimal() 







```



##AUROC plots for Validation Cohort 

```{r}

# Set seed for reproducibility
set.seed(1234)

# Support Vector Machine (SVM)
## Train an SVM model using the training dataset and evaluate its performance on the test dataset
svm_fit_test <- 
  svm_workflow |>
  last_fit(split = initial_split(train_svm), test = test)

## Calculate the Area Under the Curve (AUC) for Receiver Operating Characteristic (ROC) on the test set for the SVM model
svm_auc_test <- 
  svm_fit_test |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  summarize(mean_auc = mean(.estimate))

# k-Nearest Neighbors (KNN)
## Similar process as above, but for the KNN model
knn_fit_test <- 
  knn_workflow |>
  last_fit(split = initial_split(train_knn), test = test)

## Calculate AUC for the KNN model
knn_auc_test <- 
  knn_fit_test |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  summarize(mean_auc = mean(.estimate))

# XGBoost
## Train and evaluate the XGBoost model
xgb_fit_test <- 
  xgb_workflow |>
  last_fit(split = initial_split(train_xgb), test = test)

## Calculate AUC for the XGBoost model
xgb_auc_test <- 
  xgb_fit_test |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  summarize(mean_auc = mean(.estimate))

# Random Forest (RF)
## Train and evaluate the RF model
rf_fit_test <- 
  rf_workflow |>
  last_fit(split = initial_split(train_rf), test = test)

## Calculate AUC for the RF model
rf_auc_test <- 
  rf_fit_test |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  summarize(mean_auc = mean(.estimate))

# Naive Bayes (NB)
## Train and evaluate the NB model
nb_fit_test <- 
  nb_workflow |>
  last_fit(split = initial_split(train_nb), test = test)

## Calculate AUC for the NB model
nb_auc_test <- 
  nb_fit_test |>
  collect_metrics() |>
  filter(.metric == "roc_auc") |>
  summarize(mean_auc = mean(.estimate))

# ROC Curve Calculations for each model
## SVM
svm_results_test <- svm_fit_test |> 
  collect_predictions()
svm_roc_curve_test <- roc_curve(svm_results_test, truth = Class, .pred_0)

#KNN
knn_results_test <- knn_fit_test |> 
  collect_predictions()
knn_roc_curve_test <- roc_curve(knn_results_test, truth = Class, .pred_0)

#XGBoost
xgb_results_test <- xgb_fit_test |> 
  collect_predictions()
xgb_roc_curve_test <- roc_curve(xgb_results_test, truth = Class, .pred_0)


#Random Forest (RF)
rf_results_test <- rf_fit_test |> 
  collect_predictions()
rf_roc_curve_test <- roc_curve(rf_results_test, truth = Class, .pred_0)


#Naive Bayes (NB)
nb_results_test <- nb_fit_test |> 
  collect_predictions()
nb_roc_curve_test <- roc_curve(nb_results_test, truth = Class, .pred_0)


# Creating labels with AUC values for the plot
## Each label combines the model name with its corresponding AUC value
svm_label_test <- paste0("Support Vector Machine (AUC: ", round(svm_auc_test, 2), ")")
knn_label_test <- paste0("K nearest neighbor (AUC: ", round(knn_auc_test, 2), ")")
xgb_label_test <-paste0("XGBoost (AUC: ", round(xgb_auc_test, 2), ")")
rf_label_test <- paste0("Random Forest (AUC: ", round(rf_auc_test, 2), ")")
nb_label_test <-paste0("Naive Bayes (AUC: ", round(nb_auc_test, 2), ")")


ggplot() +
    geom_line(data = svm_roc_curve_test, aes(x = 1 - specificity, y = sensitivity, color = "Support Vector Machine (AUC: 0.84)"), lwd = 1.5) +
    geom_line(data = knn_roc_curve_test, aes(x = 1 - specificity, y = sensitivity, color = "K nearest neighbor (AUC: 0.80)"), lwd = 1.5) +
    geom_line(data = xgb_roc_curve_test, aes(x = 1 - specificity, y = sensitivity, color = "XGBoost (AUC: 0.95)"), lwd = 1.5) + 
    geom_line(data = rf_roc_curve_test, aes(x = 1 - specificity, y = sensitivity, color = "Random Forest (AUC: 0.91)"), lwd = 1.5) +
    geom_line(data = nb_roc_curve_test, aes(x = 1 - specificity, y = sensitivity, color = "Naive Bayes (AUC: 0.88)"), lwd = 1.5) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("Support Vector Machine (AUC: 0.84)" = "red", "K nearest neighbor (AUC: 0.80)" = "blue", "XGBoost (AUC: 0.95)" = "green", "Random Forest (AUC: 0.91)"= "orange", "Naive Bayes (AUC: 0.88)" = "pink")) +
  labs(title = "AUROC Curves for Several Supervised ML models (Validation Cohort)",
       x = "1 - Specificity",
       y = "Sensitivity",
       color = "Models") +
  theme_minimal() 


```


# Conclusion

Overall, XGBoost and Random Forest were the most accurate supervised machine learning models with a slight advantage to XGBoost when applied to the validation cohort. Naive Bayes performed slightly worse and K Neighbor and Support Vector Machine were the least accurate models. 

The performance of these XGBoost and Random Forest suggests that machine learning models could be used to predict biomarker-based Pediatric ARDS subphenotypes in the first 24 hours of diagnosis.
