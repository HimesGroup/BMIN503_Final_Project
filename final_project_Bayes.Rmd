---
title: "BMIN503/EPID600 Final Project: Air Quality"
author: "Brian Bayes"
output: 
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  
***
Use this template to complete your project throughout the course. Your Final Project presentation will be based on the contents of this document. Replace the title/name above and text below with your own, but keep the headers.

### Overview
Give a brief a description of your project and its goal(s), what data you are using to complete it, and what three faculty/staff in different fields you have spoken to about your project with a brief summary of what you learned from each person. Include a link to your final project GitHub repository.


### Introduction 
Describe the problem addressed, its significance, and some background to motivate the problem.

Explain why your problem is interdisciplinary, what fields can contribute to its understanding, and incorporate background related to what you learned from meeting with faculty/staff.


### Methods
Describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why. 


Collect daily PM2.5 pollutant data from across the lower 48 states for 2019.

```{r eval=FALSE}

library(httr)
library(jsonlite)
library(tidyverse)

# Retrieve environment variables with EPA AQS API credentials
api.email <- Sys.getenv("EPA_AQS_EMAIL")
api.key   <- Sys.getenv("EPA_AQS_KEY")

# Function to create a data frame based on an API call
download.AQS <- function(whichData,customParams) { 
  rootpath <- "https://aqs.epa.gov/data/api/"
  morepath <- paste0(rootpath,whichData)
  fullParams <- append(list(email = api.email, key = api.key),customParams)
  step1.json <- httr::GET(url = morepath, query = fullParams)
  step2.parsed <- jsonlite::fromJSON(content(step1.json, "text"), simplifyVector = FALSE)
  step3.df <- tibble(Data = step2.parsed$Data) %>% unnest_wider(.,Data)
  return(step3.df)
}

# Documentation for daily data summaries
dailyData.dictionary <- download.AQS("metaData/fieldsByService",list(service = "dailyData"))
View(dailyData.dictionary)

# Pull daily data nationwide (lower 48 states) for 2019
list.states <- download.AQS("list/states",list())
list.states
list.states.lower48 <- list.states %>% filter(!(code %in% c("02","15","66","72","78","80","CC")))
list.states.lower48
lower48.codes <- as.vector(as.matrix(list.states.lower48[,1]))
# 48 states + D.C.

get.daily.lower48 <- function(b,e){
collector <- data.frame()
for (s in seq_along(lower48.codes)) {
  newstate <- download.AQS("dailyData/byState", list(param="88101", bdate=b, edate=e, state=list.states.lower48[s,1]))
  collector <- bind_rows(collector,newstate)
  }
  return(collector)
}

# dailyPM2.5_201901 <- get.daily.lower48("20190101","20190131")
# dailyPM2.5_201902 <- get.daily.lower48("20190201","20190228")
# dailyPM2.5_201903 <- get.daily.lower48("20190301","20190331")
# dailyPM2.5_201904 <- get.daily.lower48("20190401","20190430")
# dailyPM2.5_201905 <- get.daily.lower48("20190501","20190531")
# dailyPM2.5_201906 <- get.daily.lower48("20190601","20190630")
# dailyPM2.5_201907 <- get.daily.lower48("20190701","20190731")
# dailyPM2.5_201908 <- get.daily.lower48("20190801","20190831")
# dailyPM2.5_201909 <- get.daily.lower48("20190901","20190930")
# dailyPM2.5_201910 <- get.daily.lower48("20191001","20191031")
# dailyPM2.5_201911 <- get.daily.lower48("20191101","20191130")
# dailyPM2.5_201912 <- get.daily.lower48("20191201","20191231")
# 
# all.2019.dailyPM2.5 <- bind_rows( dailyPM2.5_201901
#                                  ,dailyPM2.5_201902
#                                  ,dailyPM2.5_201903
#                                  ,dailyPM2.5_201904
#                                  ,dailyPM2.5_201905
#                                  ,dailyPM2.5_201906
#                                  ,dailyPM2.5_201907
#                                  ,dailyPM2.5_201908
#                                  ,dailyPM2.5_201909
#                                  ,dailyPM2.5_201910
#                                  ,dailyPM2.5_201911
#                                  ,dailyPM2.5_201912)
# 
# saveRDS(all.2019.dailyPM2.5, file = "all2019dailyPM25.RDS") 

```


Clean the file - summarize by testing site

```{r eval=TRUE}
all.2019.dailyPM2.5 <- readRDS("all2019dailyPM25.RDS")
str(all.2019.dailyPM2.5)
```








```{r eval=TRUE}
table(dailyPM2.5_2019$state)

View(PA.daily.1901)

x <- PA.daily.1901 %>%
  mutate(site_id = paste0(state_code,county_code,site_number))



View(x)

# site_number
# The 4-digit number used to uniquely identify the air monitoring site within a state-county combination or tribal area.  
# The values are always numeric, but are treated as a string and padded with leading zeroes so they always have 4 digits.

# poc
# This is the "Parameter Occurrence Code" used to distinguish different instruments that measure 
# the same parameter at the same site.  There is no meaning to the POC (e.g. POC 1 does not indicate the primary monitor).  
# For example, the first monitor established to measure carbon monoxide (CO) at a site could have a POC of 1. 
# If an additional monitor were established at the same site to measure CO, that monitor could have a POC of 2. 
# However, if a new instrument were installed to replace the original instrument used as the first monitor, 
# that would be the same monitor and it would still have a POC of 1.

# datum
# The Datum associated with the Latitude and Longitude measures.

# sample_duration
# The length of time that air passes through the monitoring device before it is analyzed (measured). 
# So, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration 
# draws ambient air over a collection filter for 24 straight hours). For continuous monitors, it can 
# represent an averaging time of many samples (for example, a 1-hour value may be the average of four 
# one-minute samples collected during each quarter of the hour).

# pollutant_standard
# A description of the ambient air quality standard rules used to aggregate statistics.  
# A pollutant standard will include the year of promulgation and the form of the standard.

date_local
The date the sample was taken in Local Standard Time.  This time reflects the beginning of the sample duration.  That is, if the time is 2:00 and the duration is 1-hour, then sampling happened from 2:00 - 3:00.

units_of_measure
The unit of measure for all statistics on the same row.  Every parameter has a standard unit of measure.  Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations.

event_type
Indicates whether data measured during exceptional events are included in the summary. 
A wildfire is an example of an exceptional event; it is something that affects air quality, 
but the local agency has no control over. No Events means no events occurred. Events Included 
means events occurred and the data from them is included in the summary. Events Excluded means 
that events occurred but data form them is excluded from the summary. Concurred Events Excluded 
means that events occurred but only EPA concurred exclusions are removed from the summary. 
If an event occurred for the parameter in question, the data will have multiple records for each monitor.


observation_count
The number of observations (samples) taken during the averaging period.

observation_percent
The percent of sample values that were reported compared to the number of data values scheduled to have been reported for the 24-hour (midnight to midnight local time) period.

validity_indicator
An indicator whether the calculated value meets all completeness criteria to be considered valid.

arithmetic_mean
The measure of central tendency obtained from the sum of the observed pollutant data values or National Ambient Air Quality Standards (NAAQS) averages in the daily data set divided by the number of values that comprise the sum for the daily data set. For criteria pollutants, the sum of values only adds the values with the appropriate flagging and concurrence for the exceptional data type.
19
first_max_value
The highest value for the ***indicated year***.  This only includes data at the selected duration or standard (e.g., it may be the maximum daily value).  For seasonal monitoring, it only includes data during the effective monitoring season.
20
first_max_hour
The time (on a 24-hour clock) when the highest value for the day was taken.
21
aqi
The Air Quality Index for the day for the pollutant, if applicable.  The air quality index is a unitless measure of the amount of pollutant that can be used to relate the pollutant to the healthy levels and indicate possible health concerns with elevated levels. The Air Quality Index (AQI) is a measure for reporting daily air quality. It focuses on health effects that may be experienced within a few hours or days after breathing polluted air. AQI is calculated for the following major air pollutants regulated by the Clean Air Act: Ozone, PM 2.5, PM 10, carbon monoxide, sulfur dioxide, and nitrogen dioxide. The AQI is a mapping from pollutant concentrations to the common index. The index is based on defining seven levels of concentration/index values that are classified as follows: <ul> <li>Good (with AQI values from 0 to 50),</li> <li>Moderate (with AQI values from 51 to 100), <li>Unhealthy for Sensitive Groups (with AQI values from 101 to 150),</li> <li>Unhealthy (with AQI values from 151 to 200),</li> <li>Very Unhealthy (with AQI values from 201 to 300),</li> <li>Hazardous (with AQI values above 301).</li> </ul> The upper and lower bounds of each AQI level classification are called “breakpoints” for the level. The EPA defines specific pollutant concentrations to be associated with each breakpoint; e.g. for Ozone, the “Moderate” classification level has the concentration of 0.060 ppm associated with its lower AQI value of 50, and the concentration of 0.075 associated with its higher AQI value of 100. AQI values in this concentration range are then computed by linear interpolation.
22
method_code
A three-digit code representing the measurement method. A method code is only unique within a parameter (that is, method 132 for ozone is not the same as method 123 for benzene).
23
method
A short description of the processes, equipment, and protocols used in gathering and measuring the sample.  This field is a concatenation of the method of collection and the method of analysis.
24
local_site_name
The identifier of the site in the onwning agency's (e.g., not US EPA) nomenclature.
25
site_address
The street address giving an approximate location of the site.
26
state
The name of the state where the monitoring site is located.
27
county
The name of the county where the monitoring site is located.
28
city
The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas.
29
cbsa_code
The code of the core based statistical area (metropolitan area) where the monitoring site is located.
30
cbsa
The name of the core based statistical area (metropolitan area) where the monitoring site is located.
31
date_of_last_change
This represents the date the most relevant underlying data in AQS was last changed.  That is, for annual summary data, it is the date these values were last affected by a change in raw data.  If the AQCR code on the annual summary view changed, the date of last change would not be updated.
Showing 1 to 19 of 31 entries, 2 total columns





PA.daily.1901 <- download.AQS("dailyData/byState", list(param = "88101",bdate = "20190101",edate = "20190131",state = "42"))
PA.daily.1902 <- download.AQS("dailyData/byState", list(param = "88101",bdate = "20190201",edate = "20190228",state = "42"))


```


```{r eval=TRUE}
test5 <- download.AQS("sampleData/byCounty",
                      list(param = "88101",bdate = "20160101",edate = "20160228",state = "37",county = "183"))


PA.particles.2019.01 <- download.AQS("sampleData/byState",
                      list(param = "88101",bdate = "20190102",edate = "20190201",state = "42"))

starter <- as.Date("2019-01-01")
s1 <- format(starter,"%Y%m%d")

fiveDay <- function(p,start,end,s){
  s.dt <- as.Date(start)
  e.dt <- as.Date(end)
  now.dt <- s.dt
  now.dt.e <- now.dt + 4
  now.char <- format(now.dt,"%Y%m%d")
  now.char.e <- format(now.dt.e,"%Y%m%d")
  build1 <- download.AQS("sampleData/byState",list(param = p, bdate = now.char, edate = now.char.e, state = s))
  now.dt <- now.dt + 5
  now.dt.e <- now.dt + 4
  while (now.dt < e.dt){
    now.char <- format(now.dt,"%Y%m%d")
    now.char.e <- format(now.dt.e,"%Y%m%d")
    build2 <- download.AQS("sampleData/byState",list(param = p, bdate = now.char, edate = now.char.e, state = s))
    build1 <- bind_rows(build1,build2)
    now.dt <- now.dt + 5
    now.dt.e <- now.dt + 4
  } 
  return(build1)
}

PA.2019.PM2.5 <- fiveDay("88101","2019-01-01","2019-12-31","42")
table(PA.2019.PM2.5$date_local)

# fiveDay("2019-01-01","2019-02-15")

str(PA.2019.PM2.5)

# revHist <- download.AQS("metaData/revisionHistory",list())
# head(revHist)


# First question: For averaging pollutant values across time, should we consider the hour of day? 

table(PA.2019.PM2.5$time_local)

library(dplyr)
library(ggplot2)
 
table(PA.2019.PM2.5$units_of_measure, useNA = TRUE)
# All Micrograms/cubic meter (LC) 

table(round(PA.2019.PM2.5$sample_measurement))

# Plot distribution of time-of-day-of-measurement by testing site, 
# to determine whether an overall plot of measurement x time of day will be representative

table(PA.2019.PM2.5$site_number)

PA.testing.sites <- PA.2019.PM2.5 %>%
                      select(state_code, state, county_code, county, cbsa_code, site_number, latitude, longitude) %>%
                      unique() %>%
                      arrange(site_number)

dim(PA.testing.sites)
# 47 rows

View(PA.testing.sites)

# Site_number is not unique. Is county (or county_code) x site_number unique? 
PA.testing.sites %>% 
  select(county, site_number) %>%
  unique() %>%
  dim()

# Yes. Create county (1st 4 letters) + site_number to have something unique and also sort of readable.

PA.testing.sites <- PA.testing.sites %>%
                      mutate(site_cty = paste0(substring(toupper(county),1,4),site_number))

table(PA.testing.sites$site_cty)
length(unique(PA.testing.sites$site_cty))

# Apply to full data set

PA.2019.PM2.5 <- PA.2019.PM2.5 %>%
                  mutate(site_cty = paste0(substring(toupper(county),1,4),site_number))

table(PA.2019.PM2.5$site_cty)

# What do the counts look like by site?

PA.site.counts <- PA.2019.PM2.5 %>%
                    group_by(site_cty) %>%
                    summarize(obs = n(), .groups = "keep") %>%
                    arrange(-obs)

head(PA.site.counts)

PA.site.counts %>% filter(obs > 10000)
# WASH0005	17885			
# CAMB0011	17520	

# Look more closely - should these actually be 2 sites (with the same long/lat?)?
PA.2019.PM2.5 %>% filter(site_cty == "WASH0005") %>% View()

table(PA.2019.PM2.5$method_code,PA.2019.PM2.5$site_cty)
# Some sites use different methods at different points in the year
# The 2 highest-count sites appear to use 2 methods simultaneously throughout the year
# One method appears seems likely to be a daily measurement which should be ignored 
# in considering trends by time of day

ggplot(data = PA.2019.PM2.5, aes(x = factor(substring(time_local,1,2)), fill = factor(method_code))) + 
    geom_bar(position = "stack")

# Yes. Ignore method_code = "145" for considering time-of-day. Also ignore time_local = "00:14"

PA.2019.PM2.5 %>%
  filter(method_code != "145" & time_local != "00:14") %>%
  ggplot(data = ., aes(x = site_cty, fill = factor(substring(time_local,1,2)))) + 
    geom_bar(position = "stack")

# Also drop sites that have less than approx 1 measurement/hour for the year (365 x 24 = 8760)
PA.site.counts %>% filter(obs < 8000)
# LYCO0419	3840			
# WYOM0010	672			
# ALLE0067	121			
# ALLE1008	121			
# ALLE1301	121			
# ALLE0093	61			
# ALLE3007	61	

PA.2019.PM2.5 %>%
  filter(method_code != "145" & time_local != "00:14" & !(site_cty %in% c("LYCO0419","WYOM0010"))) %>%
  ggplot(data = ., aes(x = site_cty, fill = factor(substring(time_local,1,2)))) + 
    geom_bar(position = "stack")

# Now, ready to make violin plots for sample_measurement by time_local

# #########################################
# NEED TO REMOVE NA MEASUREMENT VALUES!!!
# #########################################

PA.2019.PM2.5 %>% filter(is.na(sample_measurement) == TRUE) %>% View()
# Consider the `qualifier` field - seems to explain why no measurement was collected 

# table(PA.2019.PM2.5$qualifier,useNA = "always")
# PA.2019.PM2.5 %>% select(qualifier) %>% unique() %>% arrange(qualifier)

PA.qualifier.counts <- PA.2019.PM2.5 %>% 
                        mutate(any_measure = !is.na(sample_measurement)) %>%
                        group_by(any_measure,qualifier) %>% 
                        summarize(obs = n(), .groups = "keep") %>% 
                        arrange(-any_measure, -obs)

PA.qualifier.counts

PA.2019.PM2.5 %>% filter(!is.na(qualifier) & !is.na(sample_measurement)) %>% View()
# Not many in total and most are type 145 anyway

PA.2019.PM2.5 %>% filter(!is.na(qualifier) & !is.na(sample_measurement) & method_code != "145") %>% View()
# 5 records, all flagged as "Outlier" - no need to exclude here

PA.2019.PM2.5 %>%
  filter(method_code != "145" & time_local != "00:14" & !(site_cty %in% c("LYCO0419","WYOM0010"))) %>%
  select(sample_measurement) %>%
  summary()


table(round(PA.2019.PM2.5$sample_measurement))

# 75th pct 11.0 but max 217.9 !! 
# And negative concentrations are impossible; code to 0

PA.2019.PM2.5 %>%
  filter(method_code != "145" & time_local != "00:14" & !(site_cty %in% c("LYCO0419","WYOM0010")) & sample_measurement < 0) %>% 
  View()

PA.2019.PM2.5 %>%
  filter(method_code != "145" & time_local != "00:14" & !(site_cty %in% c("LYCO0419","WYOM0010")) & sample_measurement >= 100) %>% 
  View()

clean.PA.2019.PM2.5 <- PA.2019.PM2.5 %>%
                        filter(method_code != "145" 
                               & time_local != "00:14" 
                               & !(site_cty %in% c("LYCO0419","WYOM0010"))
                               & !is.na(sample_measurement)) %>%
                        mutate(sample_measurement = case_when(sample_measurement < 0 ~ 0, TRUE ~ sample_measurement))

dim(clean.PA.2019.PM2.5)
table(round(clean.PA.2019.PM2.5$sample_measurement),useNA = "always")

ggplot(data = clean.PA.2019.PM2.5, aes(x = site_cty, fill = factor(substring(time_local,1,2)))) + 
    geom_bar(position = "stack")

ggplot(data = clean.PA.2019.PM2.5, aes(x = factor(substring(time_local,1,2)), y = sample_measurement)) + 
    geom_violin(fill = "lightblue", draw_quantiles = c(0.10,0.30, 0.50, 0.70, 0.90)) +
    ylim(c(0,25))

# There is some fluctuation by time of day -- 
# But, all of the site left in have ~hourly readings throughout the year. 
# So it would still be reasonable to compute annual percentiles and means by site 
# for an overall sense of typical particulate conditions by geography. 

clean.PA.site.summary <- clean.PA.2019.PM2.5 %>%
                          group_by(site_cty,county,county_code,site_number,latitude,longitude) %>%
                          summarise(pm2.5_mean = mean(sample_measurement),
                                    pm2.5_p10 = quantile(sample_measurement, 0.10), 
                                    pm2.5_p25 = quantile(sample_measurement, 0.25), 
                                    pm2.5_p50 = quantile(sample_measurement, 0.50), 
                                    pm2.5_p75 = quantile(sample_measurement, 0.75), 
                                    pm2.5_p90 = quantile(sample_measurement, 0.90), 
                                    .groups = "keep") %>%
                          arrange(-pm2.5_p50)

View(clean.PA.site.summary)
summary(clean.PA.site.summary$longitude)
summary(clean.PA.site.summary$latitude)

# library(usmap)
# 
# 
# plot_usmap(data = statepop, values = "pop_2015", color = "red") + 
#   scale_fill_continuous(low = "white", high = "red", name = "Population (2015)", label = scales::comma) + 
#   theme(legend.position = "right")
# 
# plot_usmap(data = clean.PA.site.summary, aes(x = longitude, y = latitude, color = pm2.5_p50)) + 
#     geom_point() + 
#     scale_color_gradient(low = "#ffff00", high = "#0000ff")

library(maps)
# ?map_data
# pa <- map("state","pennsylvania")

ggplot(mtcars, aes(wt, mpg, label = rownames(mtcars))) + 
  geom_text(check_overlap = TRUE)

x <- map.cities(us.cities, country="PA")



ggplot(data = clean.PA.site.summary, aes(x = longitude, y = latitude, color = pm2.5_p50)) + 
    geom_point() + 
    borders("state","pennsylvania") + 
    scale_color_gradient(low = "#ffff00", high = "#0000ff") +
    labs(title = "Median PM2.5 by testing site") +
    labs(x = "Longitude", y = "Latitude") + 
    xlim(c(-81.00, -74.50)) +
    ylim(c( 39.50,  42.50)) +
    theme_bw() 


# ############################

ggplot(data = clean.PA.site.summary, aes(x = longitude, y = latitude, color = pm2.5_p50)) + 
    geom_point() + 
    scale_color_gradient(low = "#ffff00", high = "#0000ff") +
    labs(title = "Median PM2.5 by testing site") +
    labs(x = "Longitude", y = "Latitude") + 
    xlim(c(-81.00, -74.50)) + 
    ylim(c( 39.50,  42.50)) + 
    theme_bw() 

ggplot(data = clean.PA.site.summary, aes(x = longitude, y = latitude, color = county, size = pm2.5_p50)) + 
    geom_point() + 
    labs(title = "Median PM2.5 by testing site") +
    labs(x = "Longitude", y = "Latitude") + 
    xlim(c(-81.00, -74.50)) + 
    ylim(c( 39.50,  42.50)) + 
    theme_bw() 


ggplot(data = clean.PA.site.summary, aes(x = longitude, y = latitude, color = pm2.5_p90)) + 
    geom_point() + 
    scale_color_gradient(low = "#ffff00", high = "#0000ff") +
    labs(title = "90th pct PM2.5 by testing site") +
    labs(x = "Longitude", y = "Latitude") + 
    xlim(c(-81.00, -74.50)) + 
    ylim(c( 39.50,  42.50)) + 
    theme_bw() 

```



```{r eval=TRUE}

AQS.signup <- "https://aqs.epa.gov/data/api/signup?email=api.email"
# response <- GET(AQS.signup)
response
http_type(response)
# application/json -- as expected

parse.response <- fromJSON(content(response, "text"), simplifyVector = FALSE)
parse.response

test <- GET("https://aqs.epa.gov/data/api/metaData/isAvailable")
test

# Try out function(s) exemplified in the httr vignette
call.AQS <- function(path, query=NULL) {
  if (!is.null(query)) {
    url <- modify_url("https://aqs.epa.gov", path = path, query = query)
  } else {
    url <- modify_url("https://aqs.epa.gov", path = path) 
  }
  
  resp <- GET(url)
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }
  parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
  return(parsed)
}

isAvail <- call.AQS("/data/api/metaData/isAvailable")
isAvail

stateList.raw <- call.AQS("/data/api/list/states?email=api.email&key=api.key")
stateList <- tibble(Data = call.AQS("/data/api/list/states?email=api.email&key=api.key")$Data) %>%
                unnest_wider(.,Data)

get.AQS.data <- function(path, query=NULL) {
  if (!is.null(query)) {
    url <- modify_url("https://aqs.epa.gov", path = path, query = query)
  } else {
    url <- modify_url("https://aqs.epa.gov", path = path) 
  }
  
  resp <- GET(url)
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }
  
  parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
  df <- tibble(Data = parsed$Data) %>% unnest_wider(.,Data)
  return(df)
}

# How well does last year's data predict this year's data? 
# How well does last month's data predict this month's data?
# How well does yesterday's data predict today's data?


sampleTest <- get.AQS.data("/data/api/sampleData/byCounty?email=api.email&key=api.key&param=88101&bdate=20160101&edate=20160228&state=37&county=183")

rootpath <- "https://aqs.epa.gov/data/api/"
cred <- "?email=api.email&key=api.key"
bind_rows(c("param","88101"),c("bdate","20160101"))

sampleTest2 <- httr::GET(
  url = "https://aqs.epa.gov/data/api/sampleData/byCounty",
  query = list(
    email = api.email,
    key = api.key,
    param = "88101",
    bdate = "20160101",
    edate = "20160228",
    state = "37",
    county = "183"
  )
)

sampleTest22 <- list(email = api.email, key = api.key, 
                     param = "88101",bdate = "20160101",edate = "20160228",state = "37",county = "183") %>%
                httr::GET(url = "https://aqs.epa.gov/data/api/sampleData/byCounty",query = .)
  

customPart <- list(param = "88101",bdate = "20160101",edate = "20160228",state = "37",county = "183")
fullParams <- append(list(email = api.email, key = api.key),customPart)
sampleTes <- httr::GET(url = "https://aqs.epa.gov/data/api/sampleData/byCounty", query = fullParams)
  




whichData <- "sampleData/byCounty"
customParams <- list(param = "88101",bdate = "20160101",edate = "20160228",state = "37",county = "183")
rootpath <- "https://aqs.epa.gov/data/api/"
morepath <- paste0(rootpath,whichData)
fullParams <- append(list(email = api.email, key = api.key),customParams)
sampleTest4 <- httr::GET(url = morepath, query = fullParams)
sT4.parsed <- jsonlite::fromJSON(content(sampleTest4, "text"), simplifyVector = FALSE)
sT4.df <- tibble(Data = sT4.parsed$Data) %>% unnest_wider(.,Data)




download.AQS <- function(whichData,customParams) { 
  rootpath <- "https://aqs.epa.gov/data/api/"
  morepath <- paste0(rootpath,whichData)
  fullParams <- append(list(email = api.email, key = api.key),customParams)
  step1.json <- httr::GET(url = morepath, query = fullParams)
  step2.parsed <- jsonlite::fromJSON(content(step1.json, "text"), simplifyVector = FALSE)
  step3.df <- tibble(Data = step2.parsed$Data) %>% unnest_wider(.,Data)
  return(step3.df)
}

test5 <- download.AQS("sampleData/byCounty",
                      list(param = "88101",bdate = "20160101",edate = "20160228",state = "37",county = "183"))

revHist <- download.AQS("metaData/revisionHistory",list())
head(revHist)



```

```{r eval=TRUE}

stateList.oneStep <- get.AQS.data("/data/api/list/states?email=api.email&key=api.key")
stateList.oneStep

isAvail.Data <- get.AQS.data("/data/api/metaData/isAvailable")
# Generates an empty data frame

issues <- call.AQS("/data/api/metaData/issues?email=api.email&key=api.key")
issues.data <- get.AQS.data("/data/api/metaData/issues?email=api.email&key=api.key")
head(issues.data)

revision.history <- get.AQS.data("/data/api/metaData/revisionHistory?email=api.email&key=api.key")
revision.history

fieldsByService.sampleData <- get.AQS.data(
  "/data/api/metaData/fieldsByService?email=api.email&key=api.key&service=sampleData"
)
fieldsByService.list       <- get.AQS.data(
  "/data/api/metaData/fieldsByService?email=api.email&key=api.key&service=list"
)

fieldsByService.list
fieldsByService.sampleData

parameterClasses <- get.AQS.data(
  "/data/api/list/classes?email=api.email&key=api.key"
)
# View(parameterClasses)

parameters.AQIpollutants <- get.AQS.data(
  "/data/api/list/parametersByClass?email=api.email&key=api.key&pc=AQI%20POLLUTANTS"
)
View(parameters.AQIpollutants)

parameters.Forecast <- get.AQS.data(
  "/data/api/list/parametersByClass?email=api.email&key=api.key&pc=FORECAST"
)
View(parameters.Forecast)

parameters.All <- get.AQS.data(
  "/data/api/list/parametersByClass?email=api.email&key=api.key&pc=ALL"
)
View(parameters.All)

parameters.AirNowMaps <- get.AQS.data(
  "/data/api/list/parametersByClass?email=api.email&key=api.key&pc=AIRNOW%20MAPS"
)
View(parameters.AirNowMaps)

# Try getting 1 month of state-level daily summary data - Jan 2017, Pennsylvania
# 88101	: PM2.5 - Local Conditions
dSum.PA201701.PM2.5Local <- get.AQS.data(
    "/data/api/dailyData/byState?email=api.email&key=api.key&param=88101&bdate=20170101&edate=20170131&state=42"
)

head(dSum.PA201701.PM2.5Local)
table(dSum.PA201701.PM2.5Local$county_code)
summary(dSum.PA201701.PM2.5Local)
View(dSum.PA201701.PM2.5Local)

table(dSum.PA201701.PM2.5Local$local_site_name,useNA="ifany") 
# 1,445 NA
dSum.PA201701.PM2.5Local %>% filter(is.na(local_site_name)) %>% View()

dSum.PA201701.PM2.5Local %>% 
  group_by(county_code,site_number,cbsa_code,site_address,local_site_name) %>% 
  summarise(count=n(), .groups="keep")

length(unique(dSum.PA201701.PM2.5Local$local_site_name))
# 38 unique sites

table(dSum.PA201701.PM2.5Local$observation_count)
# Ranges from 1 to 24
table(dSum.PA201701.PM2.5Local$units_of_measure) 
# Micrograms/cubic meter (LC) for all




dSum.PA201701.PM2.5Local %>% filter(observation_count > 20) %>% View()

```


```{r EVAL=TRUE}
str(dSum.PA201701.PM2.5Local)

test.PM2.5 <- call.AQS(
    "/data/api/dailyData/byState?email=api.email&key=api.key&param=88101&bdate=20190101&edate=20190131&state=42"
)
test.PM2.5 <- 

paste0(
  '"'
  ,"/data/api/dailyData/byState"
  ,"?email=api.email"
  ,"&key=api.key"
  ,"&param=88201"
  ,"&bdate=20170101"
  ,"&edate=20170131"
  ,"&state=42"
  ,'"'
)

testDaily <- call.AQS(paste0(
  '"'
  ,"/data/api/dailyData/byState"
  ,"?email=api.email"
  ,"&key=api.key"
  ,"&param=45201"
  ,"&bdate=20170101"
  ,"&edate=20170131"
  ,"&state=42"
  ,'"'
))

testDaily <- call.AQS("/data/api/dailyData/byState?email=api.email&key=api.key&param=45201&bdate=20170101&edate=20170131&state=42")

testDaily <- get.AQS.data(
  "/data/api/dailyData/byState?email=api.email&key=api.key&param=45201&bdate=20170101&edate=20170131&state=42"
)



dSum.PA.2017.01.all <- get.AQS.data(
  "/data/api/dailyData/byState?email=api.email&key=api.key&param=#####&bdate=20170101&edate=20170131&state=42"
)

View(testDaily)

paste0("the quoted string "
  ,'"'
  ,"/data/api/dailyData/byState"
  ,"?email=api.email"
  ,"&key=api.key"
  ,"&param=45201"
  ,"&bdate=20170101"
  ,"&edate=20170131"
  ,"&state=42"
  ,'"'
  ," was created"
)


```


### Results
Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.
