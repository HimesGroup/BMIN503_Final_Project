---
title: "BMIN503 Final Project: Air Quality in American Communities"
author: "BayesB"
output: 
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  

```{r all-libraries, eval=TRUE, echo=FALSE}
library(httr)
library(jsonlite)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(tigris)
library(sf)
library(tidycensus)
```

***

### Overview
*Give a brief a description of your project and its goal(s), what data you are using to complete it, and what three faculty/staff in different fields you have spoken to about your project with a brief summary of what you learned from each person. Include a link to your final project GitHub repository.*

This project considers the burdens of poor air quality and its health consequences, and how they fall on different American communities. To explore and understand these relationships, I use daily site-level testing data from the EPA's Air Quality System for small particulate matter (PM2.5) from 2019, linked with census data for the tract surrounding each testing site. I am still determining how I can best incorporate data on the near-term health effects of poor air quality, although my main goal is not to use air quality to predict those health outcomes, but rather to determine whether there are differences (e.g. in extent/significance) between how community characteristics predict air quality and how those same characteristics predict the prevalence of events like emergency department presentations for asthma exacerbation. In developing a plan for this project I met with Drs. Anil Vachani, Gary Weissman, and John P. Reilly, who provided insights on the causal pathway from pollution sources to specific EPA-measured pollutants to short- and long-term health outcomes; potential data sources and analytic approaches; the ways in which "natural experiments" related to major changes in pollutant generation have been used to compare the effects of perturbation from a steady state; and more. Materials for this project can be found in [this GitHub repository](https://github.com/bayesb/BMIN503_Final_Project). 

### Introduction 
*Describe the problem addressed, its significance, and some background to motivate the problem. Explain why your problem is interdisciplinary, what fields can contribute to its understanding, and incorporate background related to what you learned from meeting with faculty/staff.*

Poor air quality can have a substantial negative influence on health and quality of life, but different people will have different resources, incentives, and tradeoffs to consider in determining where to live, and we might expect that the relationship between who lives where and air quality is more complex than people with more wealth, income, and social power concentrating in places with better air quality. By considering many different community characteristics in building models to predict poor air quality, it is possible to learn which factors are most strongly associated with air quality and how successfully models trained on community characteristics can predict poor air quality and related health events.

### Methods
*Describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why.* 

Retrieve daily PM2.5 pollutant data for the lower 48 states for 2019 from the EPA's Air Quality System database via their API.

```{r AQS-function, eval=TRUE}
# Retrieve environment variables with EPA AQS API credentials
api.email <- Sys.getenv("EPA_AQS_EMAIL")
api.key   <- Sys.getenv("EPA_AQS_KEY")

# Function to create a data frame based on an API call
download.AQS <- function(whichData,customParams) { 
  rootpath <- "https://aqs.epa.gov/data/api/"
  morepath <- paste0(rootpath,whichData)
  fullParams <- append(list(email = api.email, key = api.key),customParams)
  step1.json <- httr::GET(url = morepath, query = fullParams)
  step2.parsed <- jsonlite::fromJSON(httr::content(step1.json,"text"), simplifyVector = FALSE)
  step3.df <- tibble(Data = step2.parsed$Data) %>% unnest_wider(.,Data)
  return(step3.df)
}

# Fetch documentation for daily data summaries, state code list
dailyData.dictionary <- download.AQS("metaData/fieldsByService",list(service = "dailyData"))
list.states <- download.AQS("list/states",list())

# Omitting state codes other than 48 states + D.C.
list.states.lower48 <- list.states %>% filter(!(code %in% c("02","15","66","72","78","80","CC")))
list.states.lower48
lower48.codes <- as.vector(as.matrix(list.states.lower48[,1]))
```

```{r AQS-download-2019, eval=FALSE, echo=TRUE}
# Pull daily PM2.5 (parameter 88101) data for these states, with arguments for date range
get.daily.lower48 <- function(b,e){
collector <- data.frame()
for (s in seq_along(lower48.codes)) {
  newstate <- download.AQS("dailyData/byState", list(param="88101", bdate=b, edate=e, state=list.states.lower48[s,1]))
  collector <- bind_rows(collector,newstate)
  }
  return(collector)
}

# Run in monthly batches
dailyPM2.5_201901 <- get.daily.lower48("20190101","20190131")
dailyPM2.5_201902 <- get.daily.lower48("20190201","20190228")
dailyPM2.5_201903 <- get.daily.lower48("20190301","20190331")
dailyPM2.5_201904 <- get.daily.lower48("20190401","20190430")
dailyPM2.5_201905 <- get.daily.lower48("20190501","20190531")
dailyPM2.5_201906 <- get.daily.lower48("20190601","20190630")
dailyPM2.5_201907 <- get.daily.lower48("20190701","20190731")
dailyPM2.5_201908 <- get.daily.lower48("20190801","20190831")
dailyPM2.5_201909 <- get.daily.lower48("20190901","20190930")
dailyPM2.5_201910 <- get.daily.lower48("20191001","20191031")
dailyPM2.5_201911 <- get.daily.lower48("20191101","20191130")
dailyPM2.5_201912 <- get.daily.lower48("20191201","20191231")
 
# Concatenate monthly batch files into a single file for 2019
all.2019.dailyPM2.5 <- bind_rows( dailyPM2.5_201901
                                 ,dailyPM2.5_201902
                                 ,dailyPM2.5_201903
                                 ,dailyPM2.5_201904
                                 ,dailyPM2.5_201905
                                 ,dailyPM2.5_201906
                                 ,dailyPM2.5_201907
                                 ,dailyPM2.5_201908
                                 ,dailyPM2.5_201909
                                 ,dailyPM2.5_201910
                                 ,dailyPM2.5_201911
                                 ,dailyPM2.5_201912)

# Save full 2019 raw file
saveRDS(all.2019.dailyPM2.5, file = "all2019dailyPM25.RDS")

```


Create a unique list of testing site locations and a corresponding `site_id`. 

```{r AQS-site-id, eval=TRUE}
# Load the full 2019 raw file
all.2019.dailyPM2.5 <- readRDS("all2019dailyPM25.RDS")
str(all.2019.dailyPM2.5)

# Create a file of all unique combinations of geographic indicators for testing sites
testing.sites <- all.2019.dailyPM2.5 %>%
                      dplyr::select(state_code, state, 
                             county_code, county,
                             city, cbsa_code, cbsa,
                             site_number, local_site_name, site_address,
                             latitude, longitude) %>%
                      unique() %>%
                      arrange(state_code,county_code,site_number)
dim(testing.sites)
# 965 combinations

# Create a single unique ID column for site
testing.sites %>% dplyr::select(site_number) %>% unique() %>% dim()
testing.sites %>% dplyr::select(state_code,site_number) %>% unique() %>% dim()
testing.sites %>% dplyr::select(state_code,county_code,site_number) %>% unique() %>% dim()
# Only 253 unique site numbers. 770 state-site combos. 
# State + county + site number *is* unique (965 combos)

clean.2019.dailyPM2.5 <- all.2019.dailyPM2.5 %>%
  mutate(site_id = paste0(state_code,county_code,site_number))

```

Get the geometries and tract GEOIDs for the testing sites. 

```{r spatial-join, eval=FALSE, echo=TRUE}
# Prepare for spatial merge. Create sf file from testing site coordinates
testing.sites.sf.points <- st_as_sf(testing.sites, coords = c('longitude','latitude'), crs = 4269)

# Download census tract sf shapefiles, one state at a time.
# Perform spatial joins one state at a time to avoid having to build the national tract shapefile.
for (s in seq_along(lower48.codes)) {
  newstate <- lower48.codes[s]
  newtracts <- tracts(newstate)
  newjoin <- st_join(newtracts, testing.sites.sf.points, join = st_contains, left=FALSE)
  if (s==1){
    testing.sites.tracts <- newjoin
  } else {
    testing.sites.tracts <- bind_rows(testing.sites.tracts,newjoin)
  }
}

# Save file with testing sites <> census tract
saveRDS(testing.sites.tracts, file = "testing_sites_tracts.RDS")
```


Get the census variable list and select a range of variables of interest.

```{r census-var-list, eval=TRUE}
testing.sites.tracts <- readRDS("testing_sites_tracts.RDS")
head(testing.sites.tracts)

census.1yr.vars <- load_variables(dataset = "acs5",year = 2018)
head(census.1yr.vars)

census.1yr.concept.counts <- 
  census.1yr.vars %>% 
  mutate(root.concept = str_split_fixed(concept," BY",2)[,1]) %>%
  mutate(root.concept = str_split_fixed(root.concept," \\(",2)[,1]) %>% 
      # Double escape needed to match open parenthesis!
  mutate(root.name = substring(name,1,6)) %>%
  group_by(root.name,root.concept) %>%
  summarise(obs=n(), .groups="keep") %>%
  arrange(root.name,-obs)

# head(census.1yr.concept.counts)
# View(sort(unique(census.1yr.concept.counts$root.concept)))

# After reviewing variable list, grab a number of measures of interest
final.acs.var.list <- census.1yr.vars %>% filter(name %in% c(
     "B01002_001" # median age
    ,"B03002_003","B03002_001" # Hon-Hisp-White-alone // Denom
    ,"B02009_001","B02001_001" # Black race, alone or in combination // Denom
    ,"B06008_003","B06008_001" # Now married // Denom
    ,"B23007_002","B23007_001" # Children under 18yrs in household // Denom
    ,"B15003_022","B15003_023","B15003_024","B15003_025","B15003_001" # Bachelors // Masters // Professional // Doctorate // Denom
    ,"B23022_027","B23022_026","B23022_003","B23022_002" # Worked in past 12 months-Female // Denom-Female // p12-Male // Denom-Male
    ,"B21001_002","B21001_001" # Veteran // Denom
    ,"B17001_002","B17001_001" # Past 12mo income at or below poverty level, Denom
    ,"B22010_002","B22010_001" # Received SNAP in past 12mo, Denom
    ,"B22008_001"  # Median household income, past 12mo
    ,"B25008_003","B25008_001" # Renter occupied, Denom
    ,"B25024_002","B25024_003","B25024_001" # Single-fam, detached // single-fam, attached // Denom
    ,"B25064_001" # Median gross rent
    ,"B25088_002" # Median monthly owner costs for households with a mortgage
    ,"B08126_004","B08126_002","B08126_001" # INDUSTRY: Manufacturing // Agriculture, fishing, mining // Denom
    ,"B08006_003","B08006_001" # Drove to work alone // Denom
    ,"B08013_001" # Aggregate travel time to work in minutes
  )) %>% arrange(name)

final.acs.var.list

```

Retrieve and linking census variables of interest to the testing site file. 

```{r census-download, eval=FALSE, echo=TRUE}
head(testing.sites.tracts)

# Keep only what is needed to make the ACS calls in a small data frame
tst.minimal <- testing.sites.tracts %>% 
                mutate(site_id = paste0(state_code,county_code,site_number)) %>%
                dplyr::select(site_id, GEOID, STATEFP, COUNTYFP)
st_geometry(tst.minimal) <- NULL

# Loop through existing list of state codes
lower48.codes

# Make ACS calls one state at a time, only for counties with AQS testing sites
# Preserve estimate columns, ditch MOE
# Join to testing site list
# Stack ACS data together in a single data frame

acs.loop <- function(){
  for (st in seq_along(lower48.codes)) {
    
    newstate <- lower48.codes[st]
    
    county.list <- tst.minimal %>% 
                  filter(STATEFP==newstate) %>% 
                  dplyr::select(COUNTYFP) %>%
                  arrange(COUNTYFP) %>%
                  as.matrix() %>% as.vector() %>% as.list()
    
    results.acs <- get_acs( geography = "tract"
                          ,variables=as.list(final.acs.var.list$name)
                          ,state = newstate
                          ,county = county.list
                          ,output = "wide")
    
    smaller.acs <- results.acs %>% dplyr::select(GEOID,matches(".*_.*E$"))
  
    linked.acs <- tst.minimal %>% 
                filter(STATEFP==newstate) %>%
                left_join(.,smaller.acs,by="GEOID")
    
    if (st==1){
      collect <- linked.acs
    } else {
      collect <- bind_rows(collect,linked.acs)
    }
  }
  return(collect)
}

testing.sites.acs <- acs.loop()
testing.sites.acs

saveRDS(testing.sites.acs, file = "testing_sites_acs.RDS")
```

Compute versions of ACS variables to use in modeling. 

```{r census-cleaning, eval=TRUE}
testing.sites.acs <- readRDS("testing_sites_acs.RDS")
head(testing.sites.acs)

testing.sites.acs.ready <- testing.sites.acs %>%
  dplyr::rename( age.median       = B01002_001E
                ,income.hh.median = B22008_001E 
                ,rent.median      = B25064_001E
                ,home.pmt.median  = B25088_002E
                ,commute.time.agg = B08013_001E) %>%
  mutate( race.black.any          = 100*B02009_001E/B02001_001E
         ,ethn.nhisp.white.alone  = 100*B03002_003E/B03002_001E
         ,married                 = 100*B06008_003E/B06008_001E
         ,kids                    = 100*B23007_002E/B23007_001E
         ,bachelor.plus           = 100*(B15003_022E + B15003_023E + B15003_024E + B15003_025E)/B15003_001E
         ,veteran                 = 100*B21001_002E/B21001_001E
         ,manufacturing           = 100*B08126_004E/B08126_001E
         ,farm.fish.mining        = 100*B08126_002E/B08126_001E
         ,commutes.car.alone      = 100*B08006_003E/B08006_001E
         ,renter                  = 100*B25008_003E/B25008_001E
         ,single.fam.home         = 100*(B25024_002E + B25024_003E)/B25024_001E
         ,worked.12mo             = 100*(B23022_027E + B23022_003E)/(B23022_026E + B23022_002E) 
         ,poverty.12mo            = 100*B17001_002E/B17001_001E
         ,snap.12mo               = 100*B22010_002E/B22010_001E
         ,denominator             = B02001_001E) %>%
  dplyr::select(-matches("^B[012].*"))

head(testing.sites.acs.ready)
summary(testing.sites.acs.ready)
# testing.sites.acs.ready %>% dplyr::filter(denominator < 1000) %>% View()
# testing.sites.acs.ready %>% dplyr::filter(denominator < 500 | !complete.cases(.)) %>% View()

length(unique(testing.sites.acs.ready$GEOID))
# 953 - not unique by tract

# 34 tracts to exclude based on incomplete data and total denominator
# Bring the aggregates and medians to a comparable scale
testing.sites.acs.complete <- 
  testing.sites.acs.ready %>% 
  mutate(commute.time.agg  = commute.time.agg/10000
         ,income.hh.median = income.hh.median/10000
         ,rent.median      = rent.median/100
         ,home.pmt.median  = home.pmt.median/100
         ) %>%
  dplyr::rename(commute.time.agg.10k  = commute.time.agg
                ,income.hh.median.10k = income.hh.median
                ,rent.median.100      = rent.median
                ,home.pmt.median.100  = home.pmt.median) %>%
  dplyr::filter(denominator >= 500 & complete.cases(.))

summary(testing.sites.acs.complete)
length(unique(testing.sites.acs.complete$GEOID))
```


Join testing site locations data with census tracts. (Still to come)
Label the PM2.5 data with census tract. (Still to come)
Create outcomes summarized by census tract based on PM2.5 readings for the year.

```{r AQS-outcomes, eval=TRUE}
summary(clean.2019.dailyPM2.5)

# Create a file of site IDs with GEOIDs that have complete ACS data.
site_id.GEOID.xwalk <- testing.sites.acs.complete %>%
                        dplyr::select(site_id,GEOID) %>%
                        unique()

GEOID.dailyPM2.5 <- inner_join( site_id.GEOID.xwalk
                               ,clean.2019.dailyPM2.5
                               ,by="site_id")

dim(GEOID.dailyPM2.5)
# 1,396,734 rows

dim(unique(GEOID.dailyPM2.5[,c("GEOID","date_local")]))
# 243,809 tract-days

table(GEOID.dailyPM2.5$event_type, useNA="always")
# Included, Excluded, None
table(GEOID.dailyPM2.5$validity_indicator, useNA="always")
table(GEOID.dailyPM2.5$validity_indicator, GEOID.dailyPM2.5$event_type, useNA="always")
# Y, N. 4282 x N
# Excluding validity_indicator == "N" does not get rid of a meaningful % of events
# GEOID.dailyPM2.5 %>% dplyr::filter(validity_indicator == "N") %>% View()

GEOID.dailyPM2.5 %>% 
  group_by(method_code, method) %>%
  summarise(obs = n(), .groups = "keep")
  
GEOID.dailyPM2.5 %>% 
  group_by(aqi, arithmetic_mean) %>%
  summarise(obs = n(), .groups = "keep")
  
# 0 to 50	Good	Green
# 51 to 100	Moderate	Yellow 
# >>>>>>>>>>> YELLOW CORRESPONDS TO MEAN > 12 -- https://aqicn.org/faq/2013-09-09/revised-pm25-aqi-breakpoints/
# 101 to 150	Unhealthy for Sensitive Groups	Orange
# 151 to 200	Unhealthy	Red
# 201 to 300	Very Unhealthy	Purple
# 301 to 500	Hazardous	Maroon

summary(GEOID.dailyPM2.5$arithmetic_mean)

GEOID.daily.summary <-
  GEOID.dailyPM2.5 %>%
  dplyr::filter(validity_indicator=="Y") %>%
  mutate(arithmetic_mean = case_when(arithmetic_mean < 0 ~ 0, TRUE ~ arithmetic_mean)
        ,event           = case_when(event_type %in% c("Included","Excluded") ~ 1L, TRUE ~ 0L)
        ) %>%
  group_by(GEOID,date_local) %>%
  summarise( mean      = mean(arithmetic_mean)
            ,any_event = max(event)
            ,.groups = "keep")

table(GEOID.daily.summary$any_event)
summary(GEOID.daily.summary$mean)

GEOID.AQI.outcomes <-
  GEOID.daily.summary %>%
  mutate(ungreen = case_when(mean > 12.0 ~ 1L, TRUE ~ 0L)) %>%
  group_by(GEOID) %>%
  summarise( pm2.5_days         = n()
            ,pm2.5_p50          = quantile(mean, 0.50)
            ,pm2.5_p90          = quantile(mean, 0.90)
            ,pm2.5_ungreen_days = sum(ungreen)
            ,pm2.5_event_days   = sum(any_event)
            ,.groups="keep")

head(GEOID.AQI.outcomes)
summary(GEOID.AQI.outcomes)

table(GEOID.AQI.outcomes$pm2.5_days)
table(GEOID.AQI.outcomes$pm2.5_ungreen_days)
table(GEOID.AQI.outcomes$pm2.5_event_days)

AQI.outcomes.final <-
  GEOID.AQI.outcomes %>%
  dplyr::filter(pm2.5_days >= 50) %>%
  mutate( pm2.5_pct_ungreen    = 100 * pm2.5_ungreen_days / pm2.5_days
         ,pm2.5_flag10_ungreen = case_when(pm2.5_ungreen_days * 10 > pm2.5_days ~ 1L, TRUE ~ 0L)
         ,pm2.5_flag20_ungreen = case_when(pm2.5_ungreen_days *  5 > pm2.5_days ~ 1L, TRUE ~ 0L)
         ) %>%
  dplyr::select(GEOID, pm2.5_p50, pm2.5_pct_ungreen, pm2.5_flag10_ungreen, pm2.5_flag20_ungreen)

head(AQI.outcomes.final)
summary(AQI.outcomes.final$pm2.5_p50)
summary(AQI.outcomes.final$pm2.5_pct_ungreen)
table(AQI.outcomes.final$pm2.5_flag10_ungreen)
table(AQI.outcomes.final$pm2.5_flag20_ungreen)

```


```{r old-AQS-data-cleaning, eval=FALSE, echo=TRUE}
# Look at attributes of arithmetic_mean
summary(clean.2019.dailyPM2.5$arithmetic_mean)
dim(clean.2019.dailyPM2.5)
sum(!is.na(clean.2019.dailyPM2.5$arithmetic_mean))
sum(clean.2019.dailyPM2.5$arithmetic_mean >= 0)

#################################################
# More data cleaning and exploration needed here! Including plots
#################################################

# Create summary metrics
site.summary.PM2.5 <- clean.2019.dailyPM2.5 %>%
                        mutate(arithmetic_mean = case_when(arithmetic_mean < 0 ~ 0, TRUE ~ arithmetic_mean)) %>%
                        group_by(site_id,
                             state_code, state, 
                             county_code, county,
                             city, cbsa_code, cbsa,
                             site_number, local_site_name, site_address,
                             latitude, longitude) %>%
                        summarise(pm2.5_mean = mean(arithmetic_mean),
                                    pm2.5_p10 = quantile(arithmetic_mean, 0.10), 
                                    pm2.5_p25 = quantile(arithmetic_mean, 0.25), 
                                    pm2.5_p50 = quantile(arithmetic_mean, 0.50), 
                                    pm2.5_p75 = quantile(arithmetic_mean, 0.75), 
                                    pm2.5_p90 = quantile(arithmetic_mean, 0.90), 
                                    .groups = "keep") %>%
                        arrange(-pm2.5_p50)

head(site.summary.PM2.5)
tail(site.summary.PM2.5)
          
```


Visualizations of 2019 air quality summaries across the U.S.

```{r simple-outcome-maps, eval=TRUE}

library(maps)

ggplot(data = site.summary.PM2.5, aes(x = longitude, y = latitude, color = pm2.5_p50)) + 
    geom_point() + 
    borders("state") + 
    scale_color_gradient(low = "#ffff00", high = "#0000ff") +
    labs(title = "Median daily PM2.5 reading for 2019 by testing site") +
    labs(x = "Longitude", y = "Latitude") + 
    theme_bw() 

ggplot(data = site.summary.PM2.5, aes(x = longitude, y = latitude, color = pm2.5_p90)) + 
    geom_point() + 
    borders("state") + 
    scale_color_gradient(low = "#ffff00", high = "#ff0000") +
    labs(title = "90th percentile daily PM2.5 reading for 2019 by testing site") +
    labs(x = "Longitude", y = "Latitude") + 
    theme_bw() 

```



```{r junk-just-in-case, eval=FALSE, echo=FALSE}

#################################################
# Scraps from previous attempts I'm keeping in case I need them later...
#################################################

table(all.2019.dailyPM2.5$units_of_measure, useNA = TRUE)
# All Micrograms/cubic meter (LC) 

table(round(all.2019.dailyPM2.5$sample_measurement))

# Plot distribution of time-of-day-of-measurement by testing site, 
# to determine whether an overall plot of measurement x time of day will be representative



# Try getting 1 month of state-level daily summary data - Jan 2017, Pennsylvania

# 88101	: PM2.5 - Local Conditions

dSum.PA201701.PM2.5Local <- get.AQS.data(
    "/data/api/dailyData/byState?email=api.email&key=api.key&param=88101&bdate=20170101&edate=20170131&state=42"
)

head(dSum.PA201701.PM2.5Local)
table(dSum.PA201701.PM2.5Local$county_code)
summary(dSum.PA201701.PM2.5Local)
View(dSum.PA201701.PM2.5Local)

table(dSum.PA201701.PM2.5Local$local_site_name,useNA="ifany") 
# 1,445 NA
dSum.PA201701.PM2.5Local %>% filter(is.na(local_site_name)) %>% View()

dSum.PA201701.PM2.5Local %>% 
  group_by(county_code,site_number,cbsa_code,site_address,local_site_name) %>% 
  summarise(count=n(), .groups="keep")

length(unique(dSum.PA201701.PM2.5Local$local_site_name))
# 38 unique sites

table(dSum.PA201701.PM2.5Local$observation_count)
# Ranges from 1 to 24
table(dSum.PA201701.PM2.5Local$units_of_measure) 
# Micrograms/cubic meter (LC) for all




dSum.PA201701.PM2.5Local %>% filter(observation_count > 20) %>% View()


# Site_number is not unique. Is county (or county_code) x site_number unique? 
PA.testing.sites %>% 
  select(county, site_number) %>%
  unique() %>%
  dim()

# Yes. Create county (1st 4 letters) + site_number to have something unique and also sort of readable.

PA.testing.sites <- PA.testing.sites %>%
                      mutate(site_cty = paste0(substring(toupper(county),1,4),site_number))

table(PA.testing.sites$site_cty)
length(unique(PA.testing.sites$site_cty))

# Apply to full data set

all.2019.dailyPM2.5 <- all.2019.dailyPM2.5 %>%
                  mutate(site_cty = paste0(substring(toupper(county),1,4),site_number))

table(all.2019.dailyPM2.5$site_cty)

# What do the counts look like by site?

PA.site.counts <- all.2019.dailyPM2.5 %>%
                    group_by(site_cty) %>%
                    summarize(obs = n(), .groups = "keep") %>%
                    arrange(-obs)

head(PA.site.counts)

PA.site.counts %>% filter(obs > 10000)
# WASH0005	17885			
# CAMB0011	17520	

# Look more closely - should these actually be 2 sites (with the same long/lat?)?
all.2019.dailyPM2.5 %>% filter(site_cty == "WASH0005") %>% View()

table(all.2019.dailyPM2.5$method_code,all.2019.dailyPM2.5$site_cty)
# Some sites use different methods at different points in the year
# The 2 highest-count sites appear to use 2 methods simultaneously throughout the year
# One method appears seems likely to be a daily measurement which should be ignored 
# in considering trends by time of day

ggplot(data = all.2019.dailyPM2.5, aes(x = factor(substring(time_local,1,2)), fill = factor(method_code))) + 
    geom_bar(position = "stack")

# Yes. Ignore method_code = "145" for considering time-of-day. Also ignore time_local = "00:14"

all.2019.dailyPM2.5 %>%
  filter(method_code != "145" & time_local != "00:14") %>%
  ggplot(data = ., aes(x = site_cty, fill = factor(substring(time_local,1,2)))) + 
    geom_bar(position = "stack")

# Also drop sites that have less than approx 1 measurement/hour for the year (365 x 24 = 8760)
PA.site.counts %>% filter(obs < 8000)
# LYCO0419	3840			
# WYOM0010	672			
# ALLE0067	121			
# ALLE1008	121			
# ALLE1301	121			
# ALLE0093	61			
# ALLE3007	61	

all.2019.dailyPM2.5 %>%
  filter(method_code != "145" & time_local != "00:14" & !(site_cty %in% c("LYCO0419","WYOM0010"))) %>%
  ggplot(data = ., aes(x = site_cty, fill = factor(substring(time_local,1,2)))) + 
    geom_bar(position = "stack")

# Now, ready to make violin plots for sample_measurement by time_local

# #########################################
# NEED TO REMOVE NA MEASUREMENT VALUES!!!
# #########################################

all.2019.dailyPM2.5 %>% filter(is.na(sample_measurement) == TRUE) %>% View()
# Consider the `qualifier` field - seems to explain why no measurement was collected 

# table(all.2019.dailyPM2.5$qualifier,useNA = "always")
# all.2019.dailyPM2.5 %>% select(qualifier) %>% unique() %>% arrange(qualifier)

PA.qualifier.counts <- all.2019.dailyPM2.5 %>% 
                        mutate(any_measure = !is.na(sample_measurement)) %>%
                        group_by(any_measure,qualifier) %>% 
                        summarize(obs = n(), .groups = "keep") %>% 
                        arrange(-any_measure, -obs)

PA.qualifier.counts

all.2019.dailyPM2.5 %>% filter(!is.na(qualifier) & !is.na(sample_measurement)) %>% View()
# Not many in total and most are type 145 anyway

all.2019.dailyPM2.5 %>% filter(!is.na(qualifier) & !is.na(sample_measurement) & method_code != "145") %>% View()
# 5 records, all flagged as "Outlier" - no need to exclude here

all.2019.dailyPM2.5 %>%
  filter(method_code != "145" & time_local != "00:14" & !(site_cty %in% c("LYCO0419","WYOM0010"))) %>%
  select(sample_measurement) %>%
  summary()


table(round(all.2019.dailyPM2.5$sample_measurement))

# 75th pct 11.0 but max 217.9 !! 
# And negative concentrations are impossible; code to 0

all.2019.dailyPM2.5 %>%
  filter(method_code != "145" & time_local != "00:14" & !(site_cty %in% c("LYCO0419","WYOM0010")) & sample_measurement < 0) %>% 
  View()

all.2019.dailyPM2.5 %>%
  filter(method_code != "145" & time_local != "00:14" & !(site_cty %in% c("LYCO0419","WYOM0010")) & sample_measurement >= 100) %>% 
  View()

clean.all.2019.dailyPM2.5 <- all.2019.dailyPM2.5 %>%
                        filter(method_code != "145" 
                               & time_local != "00:14" 
                               & !(site_cty %in% c("LYCO0419","WYOM0010"))
                               & !is.na(sample_measurement)) %>%
                        mutate(sample_measurement = case_when(sample_measurement < 0 ~ 0, TRUE ~ sample_measurement))

dim(clean.all.2019.dailyPM2.5)
table(round(clean.all.2019.dailyPM2.5$sample_measurement),useNA = "always")

ggplot(data = clean.all.2019.dailyPM2.5, aes(x = site_cty, fill = factor(substring(time_local,1,2)))) + 
    geom_bar(position = "stack")

ggplot(data = clean.all.2019.dailyPM2.5, aes(x = factor(substring(time_local,1,2)), y = sample_measurement)) + 
    geom_violin(fill = "lightblue", draw_quantiles = c(0.10,0.30, 0.50, 0.70, 0.90)) +
    ylim(c(0,25))

# There is some fluctuation by time of day -- 
# But, all of the site left in have ~hourly readings throughout the year. 
# So it would still be reasonable to compute annual percentiles and means by site 
# for an overall sense of typical particulate conditions by geography. 

clean.PA.site.summary <- clean.all.2019.dailyPM2.5 %>%
                          group_by(site_cty,county,county_code,site_number,latitude,longitude) %>%
                          summarise(pm2.5_mean = mean(sample_measurement),
                                    pm2.5_p10 = quantile(sample_measurement, 0.10), 
                                    pm2.5_p25 = quantile(sample_measurement, 0.25), 
                                    pm2.5_p50 = quantile(sample_measurement, 0.50), 
                                    pm2.5_p75 = quantile(sample_measurement, 0.75), 
                                    pm2.5_p90 = quantile(sample_measurement, 0.90), 
                                    .groups = "keep") %>%
                          arrange(-pm2.5_p50)

View(clean.PA.site.summary)
summary(clean.PA.site.summary$longitude)
summary(clean.PA.site.summary$latitude)

# more to come
```

Retrieve a range of census descriptive data by census tract. 
Join with air quality summary data.

```{r explore-linked-data, eval=FALSE}

# still to come 

```


### Results
*Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.*

Perform bivariate tests of community characteristics vs air quality (continous measures and 1/0 for poor air quality above threshold). Create multivariate models to predict air quality from community characteristics. Construct a classifier using community characteristics and evaluate its performance against the real data. 

```{r actual-data-science, eval=FALSE}

# still to come

```




