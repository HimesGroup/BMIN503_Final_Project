---
title: "Exploratory and Predictive Analysis of Animal Use in Biomedical Research in the US"
author: "Deeksha Hegde"
output: html_document
bibliography: references.bib
---

## Acknowledgement

I want to thank my mentors for their valuable inputs to this project:

Dr. Blanca Himes, Associate Professor, Biostatistics, Epidemiology and Informatics; for helping me come up with the objectives of my project and timely feedback,

Dr. Jesse Hsu, Assistant Professor, Biostatistics, Epidemiology and Informatics; for guiding me through the descriptive statistics and data presentation,

Sherrie Xie, VMD-PhD Candidate, Epidemiology; for helping me with the GIS maps.

## Overview

My project aims to summarize and visualize the trends in the use of animals in biomedical research by species, pain levels and state in the US the over the period from 2008 to 2019. The data is obtained from the US Department of Agricultureâ€™s [APHIS annual reports](https://www.aphis.usda.gov/aphis/ourfocus/animalwelfare/sa_obtain_research_facility_annual_report/ct_research_facility_annual_summary_reports). Following this exploratory analysis, a prediction for the number of animals that will be used over the next 5 years will be made.

## Introduction

The use of animals in biomedical research is ubiquitous. There are two main categories for animal use: basic biomedical sciences research and (mimicking a human disease in an animal model and studying gene expression, molecular mechanisms, etc.) and drug testing (toxicity and efficacy of an experimental drug). A growing body of studies and voices from the scientific community have pointed out the poor reliability and predictive value of animal models for human outcomes and for understanding human physiology. [@akhtar-2015] In 2004, the FDA estimated that 92% of drugs that pass preclinical tests in animal models fail in human clinical trials. [@fda-2004] More recent analysis suggests that, in spite of efforts to improve the predictability of animal testing, the failure rate has increased to 96%. [@pippin-2012]

In 1938, Congress passed the U.S. Federal Food, Drug, and Cosmetic Act, mandating animal toxicity testing. As of October 7, 2021, Congress introduced the bipartisan FDA Modernization Act to end animal testing mandates. This comes after the European Parliament resoundingly passed a resolution on September 16, 2021, with a vote of 667 to 4 to phase out animal testing. How has animal use in research changed over the past few years in the US? What is the significance of species, state, pain levels? Using the limited USDA data, I aim to answer these questions. Furthermore, a regression model will predict the number of animals that will be needed in the next few years if the same trends continue. This analysis could be of interest to biomedical companies to reduce their time and resources spent on animal models, FDA for revision of regulatory requirements, bioethics specialists and animal advocacy groups.

The trends in animal use could be influenced by technologies that enhanced the ease of building animal models, technologies that performed better than animal models, change in bioethical standards, change in public sentiment about the topic and even opinions voiced by the heads of government regulatory bodies. I am personally interested in this analysis because one of my career goals is to work towards developing and commercializing alternatives to animal methods in biomedical research.

## Loading required packages

```{r, echo = TRUE, error = FALSE, warning = FALSE, message = FALSE}
options(scipen = 999)
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("dplyr")
library("ggplot2")
library("gridExtra")
library("sf")
library("spData")
library("mapview")
library("leaflet")
```

## Cleaning and Loading Data

USDA publishes an annual report of the number of animals used by state, species and pain category. The report for every year is published in January two years later. Hence the latest data I could obtain was from 2019. The reports were published starting 2008, hence I have data for a 12 year period.

The data is in the form of several PDFs. I cleaned the data manually into .txt format. The nomenclature is year_col_X where X is the pain category. The pain categories are:

 - Column B: Animals held by a facility but not used in any research that year
 
 - Column C: Animals used in research; no pain involved; no pain drugs administered
 
 - Column D: Animals used in research; pain involved; pain drugs administered
 
 - Column E: Animals used in research; pain involved; no pain drugs administered.
 
The following code chunk loads the data as a dataframe, stacked by year and column. The loaded data has 53 rows per year per column. This includes the 50 states, District of Columbia, Puerto Rico, and "REPORT TOTAL" which is the sum of all columns (species). Note that the species column is not an exhaustive list of all animals used in biomedical research but only those covered by the Animal Welfare Act. The AWA does not cover rats, mice, birds and reptiles, which happen to be over 95% of all animals used. Columns B, C, D and E are assigned values 0, 1, 2 and 3 to reflect pain levels in a more intuitive way.

```{r, error = FALSE, warning = FALSE, message = FALSE}

#Loading the data using nested functions
datafr <- purrr::map_dfr(
  .x = c(2008:2019), 
  .f = function(x){
    purrr::map_dfr(
  .x = c("B", "C", "D", "E"),
  .f = function(x, y){
    dat <- read.table(file = paste0("C:/Users/Deeksha Hegde/Downloads/BMIN503_Final_Project/USDA_Data/",y,"_col_",x,".txt"), header = TRUE, sep = "\t")
    dat %>%
      mutate(across(.cols = All_Other_Covered_Species:Total, .fns =~ as.numeric(str_remove_all(string = .x, ","))), 
        Year = y,
        Column = x
            )
    }, y=x
    )
  }
)

#Checking if the data loaded is correct
datafr %>% count(Year, Column)

datafr <- mutate(datafr, pain.level = factor(Column, levels = c("B", "C", "D", "E"), labels = c(0, 1, 2, 3)))
head(datafr)
```

## Methods

This project is divided into two main parts: descriptive statistics and prediction model. In the first part, I performed an exploratory analysis of the data, visualizing by year, state, and species. In the second part, a model to predict the numbers for the next 5 years was created.

## Data Visualization and Exploratory Analysis

Now let us visualize the numbers! 

### The numbers by year
The first plot shows the total animals (all categories combined) by year. The second plot shows the same but with the breakup by column.

```{r, fig.height = 4, fig.width = 12, error = FALSE, warning = FALSE, message = FALSE}
data.total <- filter(datafr, State == "REPORT TOTAL")

#Total by year
data.total1 <- summarise(group_by(data.total, Year), Total = sum(Total))

total.plot1 <- ggplot(data = data.total1, aes(x = Year, y = Total)) + geom_line(size = 1) + scale_x_continuous(breaks = c(2008:2019)) + scale_y_continuous(breaks = seq(900000, 1300000, by = 100000), labels = seq(9, 13, 1)) + xlab(label = "Year") + ylab(label = "Total in hundred thousands") + ggtitle("Total number of animals vs. Year") +
theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

#Total by year and column
total.plot2 <- ggplot(data = data.total, aes(x = Year, y = Total, color = pain.level)) + geom_line(size = 1) + scale_x_continuous(breaks = c(2008:2019)) + scale_y_continuous(breaks = seq(100000, 700000, by = 100000), labels = seq(1, 7, 1)) + xlab(label = "Year") + ylab(label = "Total in hundred thousands") + ggtitle("Total number of animals vs. Year by Pain Levels") +
theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )
grid.arrange(total.plot1, total.plot2, ncol = 2)
```

We see that there is a huge rise in the total from 2009 to 2010, and a steady decline since then. Overall, there is a significant decline over the 12 year period.

From the second plot, we see that pain category 1, which involves no pain and no pain drugs, is the highest in magnitude throughout the study period. We also see that the decline in the total animals is mainly due to the decline in animals at pain level 1.

### The numbers by state
Now we will examine the total numbers by state.

```{r, fig.height = 6, fig.width = 9, error = FALSE, warning = FALSE, message = FALSE}
#Total by column by state
data.by.state <- datafr %>%
  filter(State != "REPORT TOTAL") %>%
  group_by(State, pain.level) %>%
  summarise(Total = sum(Total))

data.by.state.total <- group_by(data.by.state, State) %>%
  summarize(Total.all = sum(Total))
state.plot.1 <- ggplot(data.by.state.total, aes(x = State, y = Total.all)) + geom_bar(stat = "identity") + ggtitle("Total number of animals used by state") + ylab(label = "Total across all pain levels in hundred thousands") + scale_y_continuous(breaks = seq(100000, 1300000, by = 100000), labels = seq(1, 13, 1)) +
theme(
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 12, angle = 90, vjust = 0.5, hjust = 0.5),
        axis.text.y = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

state.plot.2 <- ggplot(data.by.state, aes(x = State, y = Total, color = pain.level)) + geom_point(size = 4) + ggtitle("Total number of animals used by state by pain level") + scale_y_continuous(breaks = seq(100000, 700000, by = 100000), labels = seq(1, 7, 1)) + ylab(label = "Total in hundred thousands") +
theme(
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 12, angle = 90, vjust = 0.5, hjust = 0.5),
        axis.text.y = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

state.plot.1
state.plot.2
```

From the graphs, we get the following information:

States using the highest number of animals overall across all pain levels:
CA, MA, NJ

States using the highest number of animals by pain level:

- 0 CA, MD, TX 

- 1 CA, MA, OH 

- 2 CA, TX, MA 

- 3 MO, MI, IA 

An arguably better representation of the same data is the use of interactive maps.

```{r}

usa <- st_read("C:/Users/Deeksha Hegde/Downloads/us-state-boundaries/us-state-boundaries.shp") %>%
  rename(State = stusab)
head(usa)
to_map <- inner_join(usa, data.by.state.total, by = "State")

pal_fun <- colorNumeric("Blues", NULL) #Pick a different pal, not starting from white
pu_message <- paste0(to_map$State,  
                     "<br>Number of animals used: ",       
                     prettyNum(to_map$Total.all, big.mark = ","))

leaflet(to_map) %>%
  addPolygons(stroke = FALSE,
              fillColor = ~pal_fun(Total.all),
              fillOpacity = 0.8, smoothFactor = 0.5, 
              popup = pu_message) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addLegend("bottomright",                           
            pal=pal_fun,                             
            values=~Total.all,                 
            title = 'Total number of animals used',                  
            opacity = 1) %>%                         
  addScaleBar()
```

Now, since we're in Pennsylvania, let us see what the plots for PA look like.

```{r, fig.height = 4, fig.width = 12, error = FALSE, warning = FALSE, message = FALSE}
#Total for PA
data.PA <- filter(datafr, State == "PA")
PA.plot.1 <- ggplot(data = summarise(group_by(data.PA, Year), Total = sum(Total)), aes(x = Year, y = Total)) + geom_line(size = 1) + scale_x_continuous(breaks = c(2008:2019)) + ggtitle("Total number of animals vs. Year in Pennsylvania") + theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

#Total by column for PA
PA.plot.2 <- ggplot(data = data.PA, aes(x = Year, y = Total, color = pain.level)) + geom_line(size = 1) +
  scale_x_continuous(breaks = c(2008:2019)) + ggtitle("Total number of animals vs. Year by Pain Levels in Pennsylvania") + theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

grid.arrange(PA.plot.1, PA.plot.2, ncol = 2)
```

We see a huge decline from 2009 to 2015.

Next, I want to find out which states contributed the most to the decline observed in the first plot. I calculated the mean and standard deviation over the years for each state and sorted them in decreasing order of SD. Noticing that Arizona seems to have a higher SD than mean, which raised suspicion, I calculated the coefficient of variability (CV) and sorted in the decreasing order of CV.

```{r, error = FALSE, warning = FALSE, message = FALSE}

#Std dev of total for all states
data.by.state.year <- datafr %>%
  filter(State != "REPORT TOTAL") %>%
  group_by(State, Year) %>%
  summarise(Total = sum(Total))
  
state.std.dev <- summarise(data.by.state.year, mean = mean(Total), sd = sd(Total))
state.std.dev[order(state.std.dev$sd, decreasing = TRUE),]
state.std.dev <- state.std.dev %>% mutate(cv = sd/mean) %>% 
                  arrange(desc(cv))
state.std.dev
```

The states having CV > 1 will be examined further.

```{r, error = FALSE, warning = FALSE, message = FALSE}
#Total by column for AZ

data.AZ <- filter(datafr, State == "AZ")
ggplot(data = data.AZ, aes(x = Year, y = Total, color = pain.level)) + geom_line(size = 1)+
  scale_x_continuous(breaks = c(2008:2019)) + ggtitle("Total number of animals vs. Year by Pain Levels in Arizona")
```

Column C (pain level 1) of the year 2010 is clearly an outlier. I verified the data again from the USDA report to check if there was an error in data loading. It is most likely a typographical error in the USDA report. On examination, I found that the outlier is coming from the All_Other_Covered_Species column. I first replaced this value with NA and found the mean along the column for the rest of the years. I replaced the NA with the rounded mean and finally updated the total. The updated plot for AZ is also shown.

```{r}
#Setting the outlier point to NA
data.AZ[, 2][data.AZ['Column'] == "C" & data.AZ['Year'] == 2010] <- NA

#Setting the outlier point to the mean of the column over the years excluding outlier year
data.AZ[, 2][data.AZ['Column'] == "C" & data.AZ['Year'] == 2010] <- round(mean(data.AZ[, 2][data.AZ['Column'] == "C"], na.rm = TRUE), digits = 0)

#Updating the Total column for the year
data.AZ[, 'Total'][data.AZ['Column'] == "C" & data.AZ['Year'] == 2010] <- sum(data.AZ[2:12][data.AZ['Column'] == "C" & data.AZ['Year'] == 2010])

#Updated plot with outlier adjusted
ggplot(data = data.AZ, aes(x = Year, y = Total, color = pain.level)) + geom_line(size = 1)+
  scale_x_continuous(breaks = c(2008:2019)) + ggtitle("Total number of animals vs. Year by Pain Levels in Arizona (Outlier adjusted)")
```

The second state having CV > 1 is Kansas. Let's look at its plot.

```{r, error = FALSE, warning = FALSE, message = FALSE}
#Total by column for KS

data.KS <- filter(datafr, State == "KS")
ggplot(data = data.KS, aes(x = Year, y = Total, color = pain.level)) + geom_line(size = 1)+
  scale_x_continuous(breaks = c(2008:2019)) + ggtitle("Total number of animals vs. Year by Pain Levels in Kansas")
```

2019 pain level 1 is likely to be an outlier but unlike the case with Arizona, we cannot be sure since we don't have the data for the years after 2019. I will not alter this point but exclude it in some analysis.

Finally, we have West Virginia.

```{r, error = FALSE, warning = FALSE, message = FALSE}
#Total by column for WV

data.WV <- filter(datafr, State == "WV")
ggplot(data = data.WV, aes(x = Year, y = Total, color = pain.level)) + geom_line(size = 1)+
  scale_x_continuous(breaks = c(2008:2019)) + ggtitle("Total number of animals vs. Year by Pain Levels in West Virginia")
```

WV does not seem to have outliers, since there is an increase and decrease in all columns over 4 years from 2015-2019.

Now that we have investigated outliers, let us go back to the Total number of animals vs. Year plots and revise them.

```{r, fig.height = 8, fig.width = 12, error = FALSE, warning = FALSE, message = FALSE}
#Updating dataframe and total plots

datafr[datafr['State'] == "AZ" & datafr['Column'] == "C" & datafr['Year'] == 2010, ] <- data.AZ[data.AZ['Column'] == "C" & data.AZ['Year'] == 2010, ]
datafr[530, 2:13] <- colSums(datafr[478:529, 2:13])

data.total <- filter(datafr, State == "REPORT TOTAL")

#Total by year
data.total1 <- summarise(group_by(data.total, Year), Total = sum(Total))
total.plot.1.updated <- ggplot(data = data.total1, aes(x = Year, y = Total)) + geom_line(size = 1) + scale_x_continuous(breaks = c(2008:2019)) + scale_y_continuous(breaks = seq(900000, 1300000, by = 100000), labels = seq(9, 13, 1)) + xlab(label = "Year") + ylab(label = "Total in hundred thousands") + ggtitle("Total number of animals vs. Year") +
theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

#Total by year and column
total.plot.2.updated <- ggplot(data = data.total, aes(x = Year, y = Total, color = pain.level)) + geom_line(size = 1) + scale_x_continuous(breaks = c(2008:2019)) + scale_y_continuous(breaks = seq(100000, 700000, by = 100000), labels = seq(1, 7, 1)) + xlab(label = "Year") + ylab(label = "Total in hundred thousands") + ggtitle("Total number of animals vs. Year by Pain Levels") +
theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )
grid.arrange(total.plot1, total.plot2, total.plot.1.updated, total.plot.2.updated, ncol = 2, left = "Before and after outlier adjustment")
```

Now we observe a more steady decline in the total number of animals over the years.

Next, let us look at a heat map of the percentage change by state over the 12 years. Kansas was excluded from this (set to NA) because it is a potential outlier that interefers with the data presentation done here.

```{r, error = FALSE, warning = FALSE, message = FALSE}
data.state.percentage <- data.by.state.year[data.by.state.year['Year'] == 2008 | data.by.state.year['Year'] == 2019, ] %>%
group_by(State) %>%
summarize(percent = 100*(Total - first(Total))/first(Total)) %>%
filter(percent != 0)
data.state.percentage[data.state.percentage$State == "KS", "percent"] <- NA
data.state.percentage

to_map1 <- inner_join(usa, data.state.percentage, by = "State")

#Gradient for -ve and +ve
#KS taken as NA because cannot conclude as outlier but very high value

minval <- min(data.state.percentage$percent, na.rm = TRUE)
maxval <- max(data.state.percentage$percent, na.rm = TRUE)
domain <- c(minval, maxval)
colorPal <- c(colorRampPalette(colors = c("red", "yellow"), space = "Lab")(abs(minval)), colorRampPalette(colors = c("yellow", "green"), space = "Lab")(maxval))
##b2182b, #2166ac
pal_fun <- colorNumeric("BuPu", NULL)
pu_message <- paste0(to_map1$State,  
                     "<br>Percentage change in total animals used from 2008 to 2019 (%): ",       
                     round(to_map1$percent, digits = 0))

leaflet(to_map1) %>%
  addPolygons(stroke = FALSE,
              color = 'white',
              fillOpacity = 0.8, smoothFactor = 0.5,
              fillColor = ~get('colorBin')(colorPal, domain)(percent),
              popup = pu_message) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addLegend("bottomright",
            pal=colorBin(colorPal, domain = domain + 1),                             
            values=~domain,                 
            title = 'Percentage change in total animals used from 2008 to 2019 by state (%)',
            opacity = 1) %>%
  addScaleBar()
```

Next, I want to visualize the patterns in each of the states. The following chunk plots the data for each of the states over the 12 year period.

```{r, fig.height = 60, fig.width = 50, error = FALSE, warning = FALSE, message = FALSE}

ggplot(data = datafr %>% filter(State != "REPORT TOTAL"), aes(x = Year, y = Total, color = pain.level)) + geom_line(size = 2.5) + ggtitle("Total number of animals used by each state by pain levels") +
  scale_x_continuous(breaks = c(2008:2019)) + facet_wrap( ~ State, ncol = 5, scales = "free") + theme(
        strip.text.x = element_text(size = 40, color = "blue", face = "bold"),
        plot.title = element_text(size = 50, face = "bold", hjust = 0.5),
        axis.title = element_blank(),
        axis.text = element_text(size = 8),
        legend.position = "bottom",
        legend.justification = "right",
        legend.title = element_text(size = 50),
        legend.text = element_text(size = 50),
        legend.key.width = unit(x = 3, units = "in")
      )
```

From this grid, we see that out of the top 10 states using the highest number of animals, 6 of them saw a decreasing trend in the study period.

### The numbers by species

To visualize the numbers by species, I first performed the same standard deviation analysis as before for species. The following chunks plot the mean and standard deviation over the years by species for the top 3 states, California, Massachusetts and New Jersey.

```{r, fig.height = 12, fig.width = 6, error = FALSE, warning = FALSE, message = FALSE}

#Std dev of species for top states
data.CA <- filter(datafr, State == "CA") %>%
  filter(State != "REPORT TOTAL") %>%
  select(-State) %>%
  group_by(Year) %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sum, na.rm = TRUE)) %>%
  ungroup() %>%

  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sd, na.rm = TRUE)) %>%
  pivot_longer(cols = All_Other_Covered_Species:Total, names_to = "species", values_to = "sd") %>%
  arrange(desc(sd)) %>% full_join(

filter(datafr, State == "CA") %>%
  filter(State != "REPORT TOTAL") %>%
  select(-State) %>%
  group_by(Year) %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sum, na.rm = TRUE)) %>%
  ungroup() %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = ~mean(x = .x, na.rm = TRUE))) %>%
  pivot_longer(cols = All_Other_Covered_Species:Total, names_to = "species", values_to = "mean"), by = "species") %>% 
  arrange(sd)

CA.species <- ggplot(data.CA %>% mutate(species = factor(species, levels = data.CA$species)), aes(x = species, y = sd)) + geom_bar(stat="identity", fill="steelblue", width=0.5) + geom_bar(mapping = aes(y = mean), stat="identity", fill="steelblue", width=0.5, alpha = 0.5, position = "dodge") + ggtitle("Mean and Standard deviation of species over 12 years in CA") + ylab("Mean (light blue) and SD (dark blue)") + xlab("Species") +
theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 0.5),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

#Std dev of species for top states - MA
data.MA <- filter(datafr, State == "MA") %>%
  filter(State != "REPORT TOTAL") %>%
  select(-State) %>%
  group_by(Year) %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sum, na.rm = TRUE)) %>%
  ungroup() %>%

  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sd, na.rm = TRUE)) %>%
  pivot_longer(cols = All_Other_Covered_Species:Total, names_to = "species", values_to = "sd") %>%
  arrange(desc(sd)) %>% full_join(

filter(datafr, State == "MA") %>%
  filter(State != "REPORT TOTAL") %>%
  select(-State) %>%
  group_by(Year) %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sum, na.rm = TRUE)) %>%
  ungroup() %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = ~mean(x = .x, na.rm = TRUE))) %>%
  pivot_longer(cols = All_Other_Covered_Species:Total, names_to = "species", values_to = "mean"), by = "species") %>% 
  arrange(sd)

MA.species <- ggplot(data.MA %>% mutate(species = factor(species, levels = data.MA$species)), aes(x = species, y = sd)) + geom_bar(stat="identity", fill="steelblue", width=0.5) + geom_bar(mapping = aes(y = mean), stat="identity", fill="steelblue", width=0.5, alpha = 0.5, position = "dodge") + ggtitle("Mean and Standard deviation of species over 12 years in MA") + ylab("Mean (light blue) and SD (dark blue)") + xlab("Species") +
theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 0.5),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

#Std dev of species for top states - NJ
data.NJ <- filter(datafr, State == "NJ") %>%
  filter(State != "REPORT TOTAL") %>%
  select(-State) %>%
  group_by(Year) %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sum, na.rm = TRUE)) %>%
  ungroup() %>%

  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sd, na.rm = TRUE)) %>%
  pivot_longer(cols = All_Other_Covered_Species:Total, names_to = "species", values_to = "sd") %>%
  arrange(desc(sd)) %>% full_join(

filter(datafr, State == "NJ") %>%
  filter(State != "REPORT TOTAL") %>%
  select(-State) %>%
  group_by(Year) %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sum, na.rm = TRUE)) %>%
  ungroup() %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = ~mean(x = .x, na.rm = TRUE))) %>%
  pivot_longer(cols = All_Other_Covered_Species:Total, names_to = "species", values_to = "mean"), by = "species") %>% 
  arrange(sd)

NJ.species <- ggplot(data.NJ %>% mutate(species = factor(species, levels = data.NJ$species)), aes(x = species, y = sd)) + geom_bar(stat="identity", fill="steelblue", width=0.5) + geom_bar(mapping = aes(y = mean), stat="identity", fill="steelblue", width=0.5, alpha = 0.5, position = "dodge") + ggtitle("Mean and Standard deviation of species over 12 years in NJ") + ylab("Mean (light blue) and SD (dark blue)") + xlab("Species") +
theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 8, angle = 90, vjust = 0.5, hjust = 0.5),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )

grid.arrange(CA.species, MA.species, NJ.species, ncol = 1)
```

These plots show us the variation in each of the species by state. We see that in CA, rabbits are the most commonly used species and they have a high standard deviation. Relative to their numbers, farm animals and guinea pigs also showed high variation. In MA, hamsters showed the highest variation while guinea pigs were the highest used by number. In NJ, hamsters were the top species ad also showed high standard deviation. Relative to their numbers, non-human primates also showed high variation in NJ.

Next I want to visualize the patterns for each of the species over the 12 year period. The following chunk plots the same in one grid.

```{r, fig.height = 6, fig.width = 15,}
data.species.pl <- data.total %>%
  group_by(pain.level) %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sum, na.rm = TRUE))

data.species.year <- data.total %>%
  group_by(Year) %>%
  summarise(across(.cols = All_Other_Covered_Species:Total, .fns = sum, na.rm = TRUE)) %>%
  pivot_longer(cols = All_Other_Covered_Species:Total, names_to = "species", values_to = "number")

#Plot grid for each species
ggplot(data = data.species.year, aes(x = Year, y = number)) + geom_line(size = 1) + ggtitle("Number of animals by species between 2008-2019") +
  scale_x_continuous(breaks = c(2008:2019)) + facet_wrap( ~ species, ncol = 4, scales = "free") + theme(
        strip.text.x = element_text(size = 8, color = "blue", face = "bold"),
        plot.title = element_text(size = 15, face = "bold", hjust = 0.5),
        axis.title = element_blank(),
        axis.text = element_text(size = 7),
        legend.position = "bottom",
        legend.justification = "right",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10),
        legend.key.width = unit(x = 1, units = "in")
      )

#% change for each species
data.species.percentage <- filter(data.species.year, (Year == 2008 | Year == 2019)) %>%
  group_by(species) %>%
  summarise(percent = 100*(number - first(number))/first(number)) %>%
  filter(percent != 0) %>%
  mutate(pos = percent >= 0) %>%
  arrange(desc(percent)) 
  
#Bar plot of % change for each species
ggplot(data.species.percentage %>% mutate(species = factor(species, levels = data.species.percentage$species)), aes(x = species, y = percent, fill = pos)) + geom_bar(width = 0.8, position = position_dodge(width = 0.8), stat = "identity") + geom_text(data = data.species.percentage %>% filter(pos == FALSE), aes(label = round(percent, digits = 0)), color = 'red', size = 6, hjust=1.2, vjust=0.5) + geom_text(data = data.species.percentage %>% filter(pos == TRUE), aes(label = round(percent, digits = 0)), color = 'cyan', size = 6, hjust=-0.2, vjust=0.5) + xlab(label = "Species") + ylab(label = "Percentage change 2008-2019") + ggtitle("Percentage change from 2008 to 2019 by species") + coord_flip() +
theme(
        plot.title = element_text(size = 15, face = "bold", hjust = 0.5),
        axis.title = element_blank(),
        axis.text = element_text(size = 12),
        legend.position = "none"
      )
```

From the above plots, we have a clear picture of what trends were for each of the species as well as their percentage change over the 12 year period. We see that almost all categories saw a decrease. Among these, dogs, hamsters, non-human primates and rabbits saw a steep decline.

## Predictive Analysis

I want to find what the numbers would be for the next 5 years if the same trend continues. I fit a linear model to the total number of animals vs. year graph and also made a table to reflect the figures.

```{r}
m1 <- lm(Total ~ Year, data = data.total1)
summary(m1)

predict(m1, newdata = data.total1)
data.total.1.predict <- data.frame(year = 2019:2024, Total = NA_integer_, round(predict(m1, newdata = tibble(Year = 2019:2024), interval = "confidence"), digits = 0))
data.total.1.predict

ggplot(data = data.total1, aes(x = Year, y = Total)) + geom_line() + geom_smooth(formula = "y~x", method = "lm") + geom_line(data = data.total.1.predict, mapping = aes(x = year, y = fit), linetype = "dashed", color = "red") + geom_ribbon(data = data.total.1.predict, mapping = aes(ymin = lwr, ymax = upr, x = year), fill = "red", alpha = 0.2) + scale_x_continuous(breaks = c(2008:2024)) + scale_y_continuous(breaks = seq(700000, 1300000, by = 100000), labels = seq(7, 13, 1)) + xlab(label = "Year") + ylab(label = "Total in hundred thousands") + ggtitle("Total number of animals vs. Year") + theme(
        plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
        axis.title = element_text(size = 10),
        axis.text = element_text(size = 10),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10)
      )
```

## Conclusion

To summarize, I found that the number of animals used in biomedical research has seen a decreasing trend over the years 2008-2019, with a reduction of 24%. A point to note is that this is the data only for species covered under the Animal Welfare Act and hence it may not be representative of all the animals used. It is very likely that in reality the number of increased. Improvements in genetic engineering lead to easy creation of transgenic mice and rats in the 1980s which are estimated to be over 95% of the animals used in research today. This could have resulted in an ongoing decline in the usage of other mammals, as seen in the patterns for dogs and non-human primates for example.

California is the top state in terms of total numbers as well as across most pain categories. Among the top 10 states, 6 of them show a decreasing trend, including California.

The improvements in non-animal methods are also likely to have caused the decline in animal usage. oOr example, in the cosmetic industry, the "cruelty-free" tag gained traction in the public eye and led to an ongoing transformation in the beauty industry to shift away from performing toxicity testing on rabbits to using in vitro human skin models.

Through this project, I used many of the concepts I learned in BMIN 503. I was able to appreciate the work that goes into cleaning and transforming data to a usable form. I was able to make custom plots and GIS maps to reflect my data and build a simple linear model.

## References