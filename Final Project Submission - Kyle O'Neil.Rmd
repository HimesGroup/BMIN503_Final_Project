---
title: "An Analysis of the Effects of Operational Factors on the Success of Phase II Industry Sponsored Studies"
author: "Kyle O'Neil"
output:
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

#Overview
###Biopharmaceutical development faces low success rates, with drug approval dependent on clinical studies that demonstrate therapeutic benefits outweigh health risks. With an overall success rate of 14% in clinical development (1), evaluating the numerous factors that play a role in clinical studies is crucial. In this study, I evaluated 20 operational parameters to determine their effect on clinical trial success using clinicaltrials.gov registered trials, a repository of more than 290,000 research studies. 

#Introduction
###The full costs of drug development in the pharmaceutical industry have been rising rapidly, with recent estimates from 2016 suggesting the cost of bringing a drug to market is a staggering $2.5 billion (2). Similar findings have established that drug discovery productivity has been declining steadily, leading to the cost of new drug development doubling approximately every nine years since the 1950s (3).  Clinical trials are significant portion of the costs of drug development, with pivotal studies for FDA approval costing approximately $20 million (4). Therefore, this study aims to assess the factors within trial designs that contribute to clinical trial success. Phase II studies were assessed because research has established Phase II studies have the lowest success rate (2). This may because Phase I studies are generally required only to establish safety, whereas Phase II studies must establish efficacy and safety in a larger patient cohort. 

###Recent research has attempted to look at the key factors that predict clinical failure and success in oncology. In a sample of 42 cancer therapies that failed in Phase III (compared with 37 successful therapies) between 2009 and 2014 there was no pattern of study sites, trial design or funding characteristics that were associated with trial failure (5). However, the sample did show that the utilization of biomarkers for patient selection (with inclusion and exclusion criteria) and statistically significant findings in Phase II were associated with later success (5). In contrast, one study found four-factors (activity, number of patients in the pivotal phase II trial, phase II duration, and a prevalence-related measure) in Phase II cancer studies of 62 oncology drugs from the top 50 pharmaceutical companies (between 1999 and 2007) were highly predictive (AUC = 0.92) of later regulatory approval using machine learning (6). However, questions have since emerged about the robustness of the predictive model (7). 

###Analyzing clinical study data incorporates multiple fields of knowledge ranging from biostatistics to physiology. With certain operational parameters defined in clinical trials and others uncertain, feature selection from available clinicaltrials.gov data was a critical step for the project. Variables were chosen that were consistently available although variables more predictive of success may exist in alternative data formats. 

#Methods
###50 Phase II studies from 10 large biopharmaceutical companies were selected for analysis on clinicaltrials.gov with results posted in the last 5 years (since January 2014). The 10 biopharmaceutical companies included GSK, Janssen, Gilead, Novartis, Amgen, Abbvie, Genentech, Merck, Pfizer, and Regeneron. Research has estimated that industry studies account for 35% of all registered clinical trials (1). Studies were chosen at random across a range of indications and data points were collected manually from each study and compiled in an Excel CSV file. The data points recorded included whether there was a comparison group for the main therapeutic, the trial masking, primary endpoints, number of endpoints (primary and secondary), primary endpoint values, number of study sites and participants, the length of study, number of inclusion and exclusion criteria, any p values, and whether the study was advanced into Phase III. Linear regressions and machine learning models were used on the variables to determine if there were significant associations. 


#Results - Initial Data Exploration
###A first look at the assembled dataset of 50 trials showed a diverse set of studies. Aprroximately half of the studies reported a p value for the primary outcome, were open label, advanced to Phase III, used a placebo comparison, or demonstrated success meeting the primary endpoint. In contrast, only 5 studies compared a therapeutic to the standard of care, while only 16 studies were testing a treatment for the first time in Phase II (representative of the fact that treatments are increasing tested in multiple indications by large pharmaceutical companies). 
```{r}
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)
library(ggplot2)
library(tidyverse)
#install.packages("GGally")
library(GGally)
library(dplyr)
library(boot)
library(pROC)
dataset <- read.csv("Phase II Study Catalog CSV.csv", header=TRUE)
str(dataset)
summary(dataset)
dataset %>%
    group_by(SOC.Comparison) %>%
    summarise(n())

dataset %>%
    group_by(Masking) %>%
    summarise(n())

dataset %>%
    group_by(Primary.Endpoint.Success) %>%
    summarise(n())

dataset %>%
    group_by(Moved.to.Phase.III) %>%
    summarise(n())

dataset %>%
    group_by(First.time.in.Phase.II) %>%
    summarise(n())

dataset %>%
    group_by(P.Value) %>%
    summarise(n())
```

#Results - Exploratory Data Analysis
###The effect of 9 quantitative variables on Primary Endpoint Success was visually observed using ggplot and grid.arrange. 
```{r}
#Exploration of the trial length,  subject number, and trial sites numbers
primaryendpointtimeplot <- ggplot(dataset,               
    aes(x=Primary.Endpoint.Success, 
    y=Time.for.Primary.Endpoint..yrs.)) +
    geom_boxplot() + ggtitle("Trial Time vs. Primary 
    Endpoint Sucess") + theme(plot.title = element_text(hjust= 0.5))
studysitesplot <- ggplot(dataset, aes(x=Primary.Endpoint.Success,      y=Number.of.Study.Sites)) +
  geom_boxplot() + ggtitle("Study Site # vs. Primary 
  Endpoint Sucess") + theme(plot.title = element_text(hjust = 0.5))
studysubjectplot <- ggplot(dataset, aes(x=Primary.Endpoint.Success,    y=Number.of.Subjects.Enrolled.in.the.Main.Treatment.Arm)) +
  geom_boxplot() +
  ggtitle("# of Subjects v. Primary 
  Endpoint Sucess") +      
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(primaryendpointtimeplot, studysitesplot, studysubjectplot, nrow = 1)

#Exploration of Inclusion/Exclusion Criteria Metrics
numberofinclusionplot <- ggplot(dataset,
    aes(x=Primary.Endpoint.Success, y=Number.of.Inclusion.Criteria))     + geom_boxplot() + ggtitle("# of IC vs. Prim. Endpoint 
    Sucess") + theme(plot.title = element_text(hjust = 0.5))
numberofexclusionplot <- ggplot(dataset, 
    aes(x=Primary.Endpoint.Success, y=Number.of.Exclusion.Criteria))     + geom_boxplot() + ggtitle("# of EC vs. Prim. Endpoint 
    Sucess") + theme(plot.title = element_text(hjust = 0.5))
ratioinclusionexclusionplot <- ggplot(dataset, 
    aes(x=Primary.Endpoint.Success,      
    y=Ratio.of.Inclusion.to.Exclusion.Criteria)) +
    geom_boxplot() + 
    ggtitle("IC:EC vs. Prim. Endpoint  
    Success") +       
    theme(plot.title = element_text(hjust = 0.5))
grid.arrange(numberofinclusionplot, numberofexclusionplot, ratioinclusionexclusionplot, nrow = 1)

#Exploration of Number of Endpoint Metrics
primendptsplot <- ggplot(dataset, aes(x=Primary.Endpoint.Success,          y=Number.of.Primary.Endpoints)) +
      geom_boxplot() + 
      ggtitle("Pri Endpt # vs. Primary 
   Endpoint Sucess") + theme(plot.title = element_text(hjust =             0.5))
secendpointsplot <- ggplot(dataset, aes(x=Primary.Endpoint.Success, 
      y=Number.of.Secondary.Endpoints)) +
      geom_boxplot() + 
      ggtitle("Sec Endpt # vs. Primary 
  Endpoint Sucess") + theme(plot.title = element_text(hjust =              0.5))
endpointsratioplot <- ggplot(dataset, aes(x=Primary.Endpoint.Success,       y=Ratio.of.Sec..Endpts..To.Prim..Endpts.)) +
      geom_boxplot() + 
      ggtitle("SE:PE Ratio vs. Primary 
  Endpoint Sucess") +        
      theme(plot.title = element_text(hjust = 0.5))
grid.arrange(primendptsplot, secendpointsplot, endpointsratioplot, nrow = 1)
```

#Results - Linear Regression
###Linear regressions were performed on the full dataset to see if quantitative variables correlated with primary outcome success. One variable, the number of secondary endpoints, was significantly associated with success with p = 0.046. Linear regressions and data visualizations were generated for the subset of the dataset that reported p values, which had 14 primary outcome successes and 10 failures. Two variables were significantly correlated with success, both the ratio of inclusion to exclusion criteria (p = 0.0065) and the ratio of secondary endpoints to primary endpoints (p = 0.020). The ggpairs function was used to visualize associates between the quantitative variables assessed and the effect of Masking and the Standard of Care comparison on p values was explored. 
```{r}
smalldataset <- dataset %>%
    select(Primary.Endpoint.Success, Time.for.Primary.Endpoint..yrs., Number.of.Study.Sites, Number.of.Subjects.Enrolled.in.the.Main.Treatment.Arm,  Number.of.Inclusion.Criteria, Number.of.Exclusion.Criteria,  Ratio.of.Inclusion.to.Exclusion.Criteria, Number.of.Primary.Endpoints, Number.of.Secondary.Endpoints, Ratio.of.Sec..Endpts..To.Prim..Endpts.) 
ggpairs(smalldataset)
names = character()
for (i in 2:10){
  names[i] = colnames(smalldataset)[i]
} 
model = numeric()
for (i in 2:10) {
model[i] <- summary(lm(get(names[i])~Primary.Endpoint.Success, data=smalldataset))$coefficients[2,4]
}
variableswp <- cbind(names, model)
variableswp

pvaluedataset <- subset(dataset, P.Value == "Yes")
pvaluedataset2 <- pvaluedataset %>%
    select(Primary.Endpoint.Success, Time.for.Primary.Endpoint..yrs., Number.of.Study.Sites, Number.of.Subjects.Enrolled.in.the.Main.Treatment.Arm,  Number.of.Inclusion.Criteria, Number.of.Exclusion.Criteria,  Ratio.of.Inclusion.to.Exclusion.Criteria, Number.of.Primary.Endpoints, Number.of.Secondary.Endpoints, Ratio.of.Sec..Endpts..To.Prim..Endpts., Number)

pvaluedataset2 %>%
    group_by(Primary.Endpoint.Success) %>%
    summarise(n())

namesp = character()
for (i in 2:11){
  namesp[i] = colnames(pvaluedataset2)[i]
} 
modelp = numeric()
for (i in 2:11) {
modelp[i] <- summary(lm(get(namesp[i])~Primary.Endpoint.Success, data=pvaluedataset2))$coefficients[2,4]
}
variableswp2 <- cbind(namesp, modelp)
variableswp2
inclusionratioplot2 <- ggplot(pvaluedataset2, aes(x=Primary.Endpoint.Success, 
      y=Ratio.of.Inclusion.to.Exclusion.Criteria)) +
      geom_boxplot() + 
                          ggtitle("IC:EC Ratio vs. Primary 
Endpoint Sucess") + theme(plot.title = element_text(hjust =              0.5))
endpointsratioplot2 <- ggplot(pvaluedataset2, aes(x=Primary.Endpoint.Success,       y=Ratio.of.Sec..Endpts..To.Prim..Endpts.)) +
      geom_boxplot() + 
                          ggtitle("SE:PE Ratio vs. Primary
Endpoint Sucess") +        
      theme(plot.title = element_text(hjust = 0.5))
grid.arrange(inclusionratioplot2, endpointsratioplot2, nrow = 1)

pvaluedataset3 <- pvaluedataset2 %>%
    select(Primary.Endpoint.Success, Number.of.Study.Sites, Number.of.Exclusion.Criteria,  Ratio.of.Inclusion.to.Exclusion.Criteria, Ratio.of.Sec..Endpts..To.Prim..Endpts., Number)
ggpairs(pvaluedataset3)

#hist(dataset$Number) #to check correlation, likely need to transform this variable 
#studyadvancementfullplot <- plot(dataset$Primary.Endpoint.Success, dataset$Moved.to.Phase.III, xlab = "Primary Endpoint Success", ylab = "Moved to Phase III", main = "Association between Primary Endpoint Success and Further Study Advancement") 
#studyadvancementrestrictedplot <- plot(pvaluedataset$Primary.Endpoint.Success, pvaluedataset$Moved.to.Phase.III, xlab = "Primary Endpoint Success", ylab = "Moved to Phase III", main = "Association between Primary Endpoint Success and Further Study Advancement 
#in Studies Reporting a P Value") 

pvaluedataset %>%
    group_by(Masking) %>%
    summarise(n())

pvaluedataset %>%
    group_by(SOC.Comparison) %>%
    summarise(n())

maskingplot <- ggplot(pvaluedataset, aes(x=Masking, 
      y=Number)) +
      geom_boxplot() + 
      ggtitle("Masking vs. P Val") + theme(plot.title =                    element_text(hjust = 0.5))
socplot <- ggplot(pvaluedataset, aes(x=SOC.Comparison, y=Number)) +
      geom_boxplot() + 
      ggtitle("SOC Test vs. P Val") +        
      theme(plot.title = element_text(hjust = 0.5))
grid.arrange(maskingplot, socplot, nrow = 1)
```

#Results - Modeling
###Three machine learning models were built to analyze the predictive value of the quantitative variables collected for primary outcome success. A logistic regression model was created using the two most significantly correlated variables with success from the linear regression analysis in the subsection of the dataset reporting p-values. The resulting logistic regression model had a cross validation estimate of prediction error of approximately 0.2. Random forest models were created for both the full dataset and subsection of the dataset reporting p-values. In the subsection of the dataset reporting p-values the estimate of error is approximately 0.375, with both ratios having the most accuracy and highest MeanDecreaseGini. In the entire dataset the estimate of error is approximately 0.5, with minimal accuracy reported across all variables and the largest MeanDecreaseGini associated with the number of study sites. 
```{r}
#Creating a logistic regression model - ratio of inclusion to exclusion criteria and secondary enpoints to primary endpoints
pvaluedataset4 <- pvaluedataset2 %>%
    select(Primary.Endpoint.Success, Ratio.of.Inclusion.to.Exclusion.Criteria, Ratio.of.Sec..Endpts..To.Prim..Endpts.)
pvaluedataset4.glm <- glm(Primary.Endpoint.Success~., data=pvaluedataset4, family=binomial(logit), maxit = 100)
glm.prediction <- summary(pvaluedataset4.glm)
glm.prediction
glm.prediction2 <- predict(pvaluedataset4.glm, pvaluedataset2, type="response")
cost <- function(r, pi=0) mean(abs(r-pi) > 0.5)
pvaluedataset4.cv.glm <- cv.glm(pvaluedataset4, pvaluedataset4.glm, cost, K=10)
pvaluedataset4.cv.glm$delta
#Creating a random forest model for the p value dataset
library(randomForest)
pvaluedataset5 <- pvaluedataset %>%
    select(Primary.Endpoint.Success, Time.for.Primary.Endpoint..yrs., Number.of.Study.Sites, Number.of.Subjects.Enrolled.in.the.Main.Treatment.Arm,  Number.of.Inclusion.Criteria, Number.of.Exclusion.Criteria,  Ratio.of.Inclusion.to.Exclusion.Criteria, Number.of.Primary.Endpoints, Number.of.Secondary.Endpoints, Ratio.of.Sec..Endpts..To.Prim..Endpts.)
dataset.rf <- randomForest(Primary.Endpoint.Success ~ ., data=pvaluedataset5, ntree=100, importance=TRUE)
dataset.rf
dataset.rf$importance
rf.pred <- predict(dataset.rf, dataset, type="prob")
#Creating a random forest model for the entire dataset
dataset.rf.entire <- randomForest(Primary.Endpoint.Success ~ ., data=smalldataset, ntree=100, importance=TRUE,  na.action=na.roughfix)
dataset.rf.entire
dataset.rf.entire$importance
```

#Results - Model Cross Validation
###The two most predictive models were chosen for cross validation, both within the subsection of the dataset reporting p-values (logistic regression = LR, Random Forest = RF). Five-fold, rather than ten-fold, cross-validation was performed since the dataset only consisted of 24 studies. Data was trained on studies where the number of primary endpoints was equal to 1 (15 studies) and tested on the remaining 9 studies. The AUC for the logistic regression model was 0.93 whereas the AUC for the random forest model was 0.63. 
```{r}
N = nrow(pvaluedataset5)
K = 5
set.seed(1234)
s = sample(1:K, size=N, replace=T)
pred.outputs.glm <- vector(mode="numeric", length=N)
obs.outputs <- vector(mode="numeric", length=N)
pred.outputs.rf <- vector(mode="numeric", length=N)
offset <- 0
for(i in 1:K){
    train <- filter(pvaluedataset5, Number.of.Primary.Endpoints == 1)
    test <- filter(pvaluedataset5, Number.of.Primary.Endpoints != 1)
    obs.outputs[1:length(s[s==i]) + offset] <- pvaluedataset5$Primary.Endpoint.Success
    
    #Logistic regression train/test
    glm <- glm(Primary.Endpoint.Success~., data=pvaluedataset5, family=binomial(logit))
    glm.pred.curr <- predict(glm, test, type="response")
    pred.outputs.glm[1:length(s[s==i]) + offset] <- glm.pred.curr

    #RF train/test
    rf <- randomForest(Primary.Endpoint.Success~., data=pvaluedataset5, ntree=100)
    rf.pred.curr <- predict(rf, newdata=test, type="prob") 
    pred.outputs.rf[1:length(s[s==i]) + offset] <- rf.pred.curr[,2]
    
    offset <- offset + length(s[s==i])
}
#Generating ROC curves and error estimates
roc(pvaluedataset4$Primary.Endpoint.Success, glm.prediction2, ci=TRUE)
roc(obs.outputs, pred.outputs.rf, ci=TRUE)
rf.pred.control <- rf.pred[1:24, 2]
plot.roc(pvaluedataset4$Primary.Endpoint.Success, glm.prediction2, ci=TRUE, col="red")
plot.roc(obs.outputs, pred.outputs.glm, ci=TRUE, col="blue", add=TRUE)
plot.roc(pvaluedataset4$Primary.Endpoint.Success, rf.pred.control, col="aquamarine", add=TRUE)
plot.roc(obs.outputs, pred.outputs.rf, ci=TRUE, col="chartreuse", add=TRUE)
legend("bottomright", legend=c("Training LR", "Cross-Validation LR", "Training RF", "Cross-Validation RF"), col=c("red", "blue", "aquamarine", "chartreuse"), lwd=1)
```

#Discussion and Future Work
###Altogether, the machine learning models and regression analyses developed in this study suggest that several operational factors may have a significant impact on the success of the industry sponsored Phase II clinical trials. However, the scope of the findings and accuracy of the predictive models was limited by the small and nonrepresentative dataset. Similarly, the conclusions that can be drawn by subgroup analyses should not be extrapolated. Nonetheless, the findings suggest several interesting hypotheses, including that the number of secondary endpoints and the ratio of exclusion to inclusion criteria may have a large impact on the success of Phase II clinical trials. Both hypotheses seem intuitive, with restricted populations offering more controlled results and better supported studies providing higher quality. However, both hypotheses also underscore large concerns in clinical research about the representativeness of clinical studies and increasing amounts of data collected, ultimately leading to the cost increases. One other interesting finding is that the classification of success may have been confounded in studies without a p-value reported for the primary endpoint, as most predictive factors were seen in the subsection of studies reporting p-values. 

###This study also suggests the need for a tool that effectively aggregates data from clinicaltrials.gov and enables further analyses. Since trials are often only compared within fields of study, analyses of the utility of the entire clinical research ecosystem are challenging. As technology and new study designs are increasingly incorporated into research, it would be extremely exciting to be able to analyze the quality of different research approaches and techniques across multiple fields. One potential way to accomplish this might be to develop a clinical research quality score, which scores the utility of a clinical trial. However, the current approach of analyzing studies within therapeutic areas, such as cancer, may be the best for the quality of medical research. Ultimately, more high-quality data sharing, such as Project Data Sphere and Vivli (7), will enable researchers and clinicians to determine the best approach.

###Future work will focus on analyzing more studies and redefining the categorical variables collected for this analysis, such as converting indications into therapeutic areas. This will enable the utilization of categorical data, as even though categorical data was collected for this study it was not used due to the number of different categories approaching the number of studies in the sample. In addition, studies will be compared against studies of a similar type and endpoint to determine whether the operational factors that impacted this analysis were more impactful in different study types. Along with other analyses, the impact of Phase II design on regulatory approval can also be assessed. 

#References
###1) Wong, C.H.; Siah, K.W.; Lo, A.W. Estimation of clinical trial success rates and related 	parameters. Biostatistics 2018. 
###2) DiMasi JA, Grabowski HG, Hansen RW. Innovation in the pharmaceutical industry: new estimates of R&D costs. J Health Econ. 2016;47:20-33.
###3) Scannell, J., Blanckley, A., Boldon, H. & Warrington, B. (2012). Diagnosing the decline in	pharmaceutical R&D efficiency. Nature Reviews Drug Discovery 11, 191–200.
###4) Moore TJ, Zhang H, Anderson G, Alexander GC. Estimated Costs of Pivotal Trials for Novel 	Therapeutic Agents Approved by the US Food and Drug Administration, 2015-2016. JAMA Intern Med. 2018;178(11):1451–1457. doi:10.1001/jamainternmed.2018.3931
###5) Jardim, D. L., Groves, E. S., Breitfeld, P. P., & Kurzrock, R. (2017). Factors associated with 	failure of oncology drugs in late-stage clinical development: A systematic review. Cancer 	Treatment Reviews, 52, 12-21. 
###6) DiMasi, J. A., Hermann, J. C., Twyman, K., Kondru, R. K., Stergiopoulos, S., Getz, K. A., & 	Rackoff, W. (2015). A Tool for Predicting Regulatory Approval After Phase II Testing of 	New Oncology Compounds. Clinical Pharmacology & Therapeutics, 98(5), 506-513.
###7) Lo, Andrew W. and Siah, Kien Wei and Wong, Chi Heem, Machine-Learning Models for Predicting Drug Approvals (October 1, 2018). Available at SSRN: 	https://ssrn.com/abstract=2973611 or http://dx.doi.org/10.2139/ssrn.2973611

#Acknowledgements
###I would like to thank Drs. Himes, Urbanowicz, and Grant for all their advice about this project. In addition, I want to thank Sherrie for all her help throughout the course. Happy holidays to all! 

