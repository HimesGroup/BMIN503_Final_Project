---
title: "Skilled Nursing Facilities"
author: "David Roberts"
#bibliography: references.bib
output: 
  html_document:
    theme: paper
    highlight: tango
    toc: true
    toc-title: Contents
    toc-location: left
    number-sections: true
    number-depth: 2
---

### Todo
- Add highlights of the facility level exploration...
- Add logistic regression latex
- facets on for profit?
- Add correlation heatmap, different staffing ratios
- Assess reliability of data calculated from pbj detail
- [bibliography](https://quarto.org/docs/authoring/footnotes-and-citations.html#sec-citations)

***
Use this template to complete your project throughout the course. Your Final Project presentation will be based on the contents of this document. Replace the title/name above and text below with your own, but keep the headers. Feel free to change the theme and other display settings, but this is not required.

# Overview
With a rapidly aging population in the United States (cite pop pyramid), skilled nursing facilities (hereafter, SNF) have garnered increased importance in American life. The sector is a magnet for controversy (series of Nytimes articles), an understandable fact given the vulnerability of elderly residents and numerous studies tracing the influence of business models in compromising quality of care (cite MA reduced quality, private equity). Recognizing the asymmetry of information for patients and caregivers choosing a facility, the Centers for Medicare (CMS) introduced [Nursing Home Compare](https://www.medicare.gov/care-compare/?providerType=NursingHome&redirect=true), a public website synthesizing a complex quality reporting program to simple, five point "star" ratings. At a high level, a SNF's overall star rating combines information regarding staffing ratios (per resident per hour), annual inspection results, and claims-based metrics, i.e. readmission rates. Studies illustrate that quality ratings are influential in consumer choice, with each additional star correlating to higher profit margins (cite). Therefore, nursing home administrators have a strong incentive to maximize their scores, which has predictably resulted in shenanigans. Notably, one state inspector plead guilty to receiving $500k in bribes for information regarding when an inspection would occur (cite, nytimes). Tellingly, ~800 of 15k nursing homes logged their highest staffing ratios on the day of their "random" inspection (cite, nytimes). Because quality scores affect resident census and profit margin [cite], administration are sensitive to quality downgrades.

The public image of nursing homes was further exacerbated during the COVID-19 pandemic, which resulted in tragic outbreaks with high mortality rates given the vulnerable population. These outbreaks were largely responsible for a reduction in the nursing home population by [~200k between 2020 and 2021](https://www.kff.org/other/state-indicator/number-of-nursing-facility-residents/?activeTab=graph&currentTimeframe=0&startTimeframe=7&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D). As one might expect, staffing ratios were an important predictor of the occurrence and severity of outbreaks [cite]. Intuitively, staffing ratios are fundamental to safe patient care. Residents will have more bedsores if they are turned less often. Infection control is more challenging with fewer people engaged in the maintenance of a clean facility. Indeed, unsafe staffing ratios are the chief grievance in recent nursing strikes at Temple Health System and elsewhere in the United States.

Detail CMS changes to staffing quality measurements... (adding turnover etc.)

In this project, I consulted three former colleagues from the Delaware Valley Accountable Care Organization (DVACO), Beth Souder DPT, Liz Todd DPT, and Imelda Vasquez Flores PHD. Beth and Liz are both experts in post-acute operations, while Imelda is an economist specializing in health services research. For advice in statistical modeling, I consulted Dr. Jesse Hsu, a bio-statistician at UPENN.

# Part 1: Impact of "Accountable Care" Penetration on SNF Staffing Ratios
## Introduction
In the first part of this project, I sought to expand work assessing the impact of business models on SNF operations. Specifically, I estimate the impact of two Medicare contracting models, Medicare Advantage (MA) and Accountable Care Organizations (ACO), on reported staffing ratios in SNF's. CMS asserts that these models create an "accountable care relationship", in that they [cite](https://innovation.cms.gov/strategic-direction-whitepaper):
> ...give all participating providers the incentives and tools to deliver high-quality, coordinated, team-based care that promotes health, thereby reducing fragmentation and costs for people and the health system

The mantra of accountable care models is to provide "high quality" care at a low cost. In practice, CMS puts healthcare organizations (health systems, insurance companies, and provider groups) at risk for total cost of patient care to incentivize efficient care delivery. CMS then implements expansive quality reporting programs to ensure cost reductions are not achieved by simply stripping away services. A multi-decade expansion of such models is set to continue, with CMS setting a goal of all Medicare enrollees in accountable care relationships by 2030  [cite](https://innovation.cms.gov/strategic-direction-whitepaper).

SNF's characterized as the "piggy bank" for ACO's (cite). They are not typically affiliated with a large health system or owned outright by vertically integrated insurance companies (cite).

Interestingly, all three of my former colleagues from DVACO expressed doubt that any relationship exists. They indicated that while ACO's and MA health plans exert influence on SNF's via direct contracting (MA) or preferred referral networks (MA and ACO's), their levers to directly influence SNF operations are limited. In addition, SNF's exhibit a "water balloon effect, in which lost revenues in one area may be recouped elsewhere. Therefore, many SNF's are resilient to financial shocks which would otherwise result in a reduction of staffing ratios. I forged ahead unfazed!

## Methods

Describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why. 

### Data
To model the relationship between "accountable care" penetration and SNF staffing ratios, I extracted the following publicly available CMS data-sets between 2017-2021.

* [Number of Accountable Care Organization Assigned Beneficiaries by County](https://data.cms.gov/medicare-shared-savings-program/number-of-accountable-care-organization-assigned-beneficiaries-by-county)
* [Medicare Monthly Enrollment by County, disaggregated by MA and Fee for Services](https://data.cms.gov/summary-statistics-on-beneficiary-enrollment/medicare-and-medicaid-reports/medicare-monthly-enrollment)
* [Nursing Home Compare Facility level data](https://data.cms.gov/provider-data/archived-data/nursing-homes)

Python scripts to extract the files, concatenate across time periods, filter null data, and harmonize disparate county dimensions to use FIPS code are available in the `/transform` directory.

### Proposed Model
To investigate the relationship between staffing ratios and accountable care penetration, I propose a linear regression model as follows:
$$SR{y,c} = \alpha_{y} * \alpha_{s} + \beta_1{MA_{y,c}} + \beta_2{ACO_{y,c}} + \beta_3{CM_{y,c}} + \epsilon_{y,c}$$ 
where:

* $SR{y,c}$ is the reported nursing staff ratio, per resident per month in a given county and year. This is a weighted average computed from facility level data, with average daily residents as weights.
* $\alpha_{y} * \alpha_{s}$ represent interacted year-state level fixed effects. These control for variation in state regulations across time, as well as the differential impact of the COVID-19 pandemic in different regions.
* ${MA_{y,c}}$ is the percentage of total medicare enrollees which have MA health plans in a given county and year.
* $ACO_{y,c}$ is the percentage of total medicare enrollees which are ["assigned"](https://www.naacos.com/aco-assignment-in-the-medicare-share-savings-program) to ACO's in a given county and year.
* $CM_{y,c}$ is the average resident case-mix of in a given county and year. This is a weighted average computed from facility level data, with average daily residents as the weight. It is included to control for variance in resident populations across counties and time.

I hypothesize that ${MA_{y,c}}$ and $ACO_{y,c}$ is associated with a clinically meaningful reduction in $SR{y,c}$.

### Exploration
First, let's bring in the facility level SNF data. Note, this is a condensed version of the R script `analysis/nhc_compare.qmd`

```{r eval = TRUE, message = FALSE}
library(dplyr)
library(ggplot2)
library(here)
library(readr)

# Load concatenated output from `transform/nhc_provider_info.py`
provider_info_tmp <- read_delim(here("data", "interim", "provider_info_tmp.csv"), 
     delim = "|", escape_double = FALSE, trim_ws = TRUE)
provider_info_tmp$processing_date[provider_info_tmp$processing_date == "10/1/2019"] <- "2019-10-01" # correct parsing issue
provider_info_tmp$processing_date <- as.Date(provider_info_tmp$processing_date)
problems(provider_info_tmp) # data ingestion issue affecting two rows...
```

Remove rows where FIPS code is unknown

```{r}
count(provider_info_tmp) # 1 million rows pools data from ~15k facilities from 68 reporting dates (processing_date)
count(filter(provider_info_tmp, SSA_Code == "Unknown"))
count(filter(provider_info_tmp, StateAbbrev == "Unknown"))
# Drop 758 obs without FIPS mapping...
provider_info_tmp <- filter(provider_info_tmp, StateAbbrev != "Unknown")
```

Compute descriptive stats for relevant dimensions.

```{r}
library(gtsummary)
select(provider_info_tmp, 
      ownership_type:provider_resides_in_hospital,
      provider_changed_ownership_in_last_12_months:adjusted_total_nurse_staffing_hours_per_resident_per_day) %>% tbl_summary()
```
A single time slice should have similar distributions. Looks reasonable....
```{r}
select(provider_info_tmp, processing_date,
      ownership_type:provider_resides_in_hospital,
      provider_changed_ownership_in_last_12_months:adjusted_total_nurse_staffing_hours_per_resident_per_day) %>%
filter(processing_date == "2022-04-01") %>%
tbl_summary()
```

Unfortunately, the day the report is cut (processing_date) does not correspond to the data collection time periods. Many measure values stay the same month over month because there is no new data to report. Despite many disruptions in reporting, SNF staffing ratios are reported consistently, with each finalized quarter reported four months post-hoc (more or less). I use the lag function below to identify when data is changing for all facilities. Based on the scatter plot, Q4 results (reported in April) are the best option to create a complete data series from 2017-2021. Ideally, I would use a full years data to account for seasonality in SNF staffing, which varies with flu season. Alas...

```{r}
lagged_data <- select(provider_info_tmp, 
                 c(
                   "federal_provider_number", 
                   "processing_date", 
                   "reported_total_nurse_staffing_hours_per_resident_per_day"
                   )) %>%
  group_by(federal_provider_number) %>%
  dplyr::mutate(lag1 = lag(reported_total_nurse_staffing_hours_per_resident_per_day, n = 1, default = NA)) %>% 
  as.data.frame()

lagged_data <- lagged_data %>% dplyr::mutate(
    change= (ifelse(lag1 != reported_total_nurse_staffing_hours_per_resident_per_day, 1, 0)
    ))
# View(filter(lagged_data, federal_provider_number == "396001")) make sure it worked...

by_date <- lagged_data %>%
  group_by(processing_date) %>%
  summarize(pct_with_change = round(mean(change, na.rm=TRUE), 2))

plt <- ggplot(by_date, mapping = aes(x=processing_date, y=pct_with_change)) + geom_point()
plt

filter(by_date, pct_with_change == 1)$processing_date
```

Filter to observations with Q4 staffing data for 2018-2021. Facility count is reasonable, slowly decreasing each year.

```{r}
filter_dates = as.Date(c("2018-04-01", "2019-04-01", "2020-04-01", "2021-04-01", "2022-04-01"))
q4_snf_data <- filter(provider_info_tmp, processing_date %in% filter_dates) %>%   mutate(collection_yr=(as.integer(format(processing_date, format="%Y"))) - 1)
ggplot(data = q4_snf_data, aes(x = collection_yr)) + geom_bar()
```

#### Trending Staffing Ratios
Reported staffing ratios vary each year, noticeably spiking in 2020 during the height of the delta wave. Per advisers, this is due to a concerted effort by hospital systems to divert lower acuity patients to the home setting in order to minimize infection risk. The data do not appear normally distributed, exhibiting a positive skew.

```{r}
ggplot(data = q4_snf_data, aes(factor(collection_yr), reported_total_nurse_staffing_hours_per_resident_per_day)) + geom_violin()
```

The spike in staffing ratios and corresponding drop in average daily residents are visible in below table. Confusingly, the case mix variable does not corroborate increased acuity. It decreases in 2020 and 2021.

```{r}
select(q4_snf_data,
      c("collection_yr", "reported_total_nurse_staffing_hours_per_resident_per_day","adjusted_total_nurse_staffing_hours_per_resident_per_day","case-mix_total_nurse_staffing_hours_per_resident_per_day","average_number_of_residents_per_day","provider_changed_ownership_in_last_12_months")) %>% rename(yr = collection_yr, reported_staff_prpd = reported_total_nurse_staffing_hours_per_resident_per_day, adjusted_staff_prpd = adjusted_total_nurse_staffing_hours_per_resident_per_day, case_mix = `case-mix_total_nurse_staffing_hours_per_resident_per_day`, avg_rpd = average_number_of_residents_per_day, chow = provider_changed_ownership_in_last_12_months) %>% 
  tbl_summary(by=yr)
```

Since my main explanatory variable, accountable care penetration is not observed at the facility level, I aggregate staffing ratios to the county level for symmetry. Dr. Hsu indicated that variables from high level dimensions are often included as fixed effects, but not the primary variable of interest. As an example, census tract level social determinants of health are often projected to the individual level. However, these are typically used as controls, rather than the primary variable of interest.

```{r eval = TRUE, message = FALSE}
library(tidyr)
group_cols <- c("FIPS_Code", "StateAbbrev", "StdCountyName", "collection_yr")

q4_snf_data <- q4_snf_data %>% rename(case_mix_total_nurse_prpd = `case-mix_total_nurse_staffing_hours_per_resident_per_day`,
case_mix_cna_prpd = `case-mix_nurse_aide_staffing_hours_per_resident_per_day`,
case_mix_rn_prpd = `case-mix_rn_staffing_hours_per_resident_per_day`,
chow = provider_changed_ownership_in_last_12_months)
county_staffing <- q4_snf_data %>%
  group_by(across(all_of(group_cols))) %>%
  summarize(
    row_cnt = n(),
    cerfied_beds = sum(number_of_certified_beds, na.rm=TRUE),
    sum_avg_daily_residents = sum(average_number_of_residents_per_day, na.rm=TRUE),
    mean_cna_reported_hprd = weighted.mean(reported_nurse_aide_staffing_hours_per_resident_per_day, average_number_of_residents_per_day, na.rm=TRUE),
    mean_lpn_reported_hprd = weighted.mean(reported_lpn_staffing_hours_per_resident_per_day, average_number_of_residents_per_day, na.rm=TRUE),
    mean_rn_reported_hprd = weighted.mean(reported_rn_staffing_hours_per_resident_per_day, average_number_of_residents_per_day, na.rm=TRUE),
    mean_licensed_reported_hprd = weighted.mean(reported_licensed_staffing_hours_per_resident_per_day, average_number_of_residents_per_day, na.rm=TRUE),
    mean_total_reported_hprd = weighted.mean(reported_total_nurse_staffing_hours_per_resident_per_day, average_number_of_residents_per_day, na.rm=TRUE),
    mean_total_cm_hprd = weighted.mean(case_mix_total_nurse_prpd, average_number_of_residents_per_day, na.rm=TRUE),
    mean_cna_cm_hprd = weighted.mean(case_mix_cna_prpd, average_number_of_residents_per_day, na.rm=TRUE),
    mean_rn_cm_hprd = weighted.mean(case_mix_rn_prpd, average_number_of_residents_per_day, na.rm=TRUE)
)

save(county_staffing, file=here("data", "interim", "county_level_staffing.Rda"))
save(q4_snf_data, file=here("data", "interim", "snf_provider_info.Rda"))
```

Merge MA and ACO enrollment data to create the explanatory variables.

*One important note on this data-set.* In both the Medicare enrollment by county file and the ACO enrollment file, CMS does not provide a specific value when enrollee count is less than 10 in a given year. This is an effort to protect the privacy of members. Unfortunately, this obfuscation effectively zeros out accountable care penetration for the least populous counties, which tend to be rural. Without a member level dataset, I cannot address this issue.

```{r eval = TRUE, message = FALSE}
library(stringr)
aco_enroll <- read_delim(here("data", "interim", "aco_enrollment.csv"), 
     delim = "|", escape_double = FALSE, trim_ws = TRUE) # created by `transform/aco_beneficiaries_by_county.py`
med_enroll <- read_delim(here("data", "interim", "medicare_enrollment.csv"), 
     delim = "|", escape_double = FALSE, trim_ws = TRUE) # created by  `transform/medicare_benes_by_county.py`
med_enroll <- rename(med_enroll, Year = YEAR)
```

Some data cleaning required to merge counties. Overall, great correspondence when using FIPS code to merge.

```{r eval = TRUE, message = FALSE}
aco_enroll[is.na(aco_enroll$Tot_AB_Psn_Yrs), "Tot_AB_Psn_Yrs"] <- 0

# Group aco data to the county level
aco_by_county <- aco_enroll %>%
  group_by(FIPS_Code, Year) %>%
  summarize(ACOBenePsnYrs = sum(Tot_AB_Psn_Yrs))

pene_df <- merge(
  med_enroll,
  aco_by_county,
  by=c("FIPS_Code", "Year"),
  all.x = TRUE
)
pene_df <- rename(
  pene_df,
  AllMedBenePsnYrs = TOT_BENES,
  OrigMedBenePsnYrs = ORGNL_MDCR_BENES,
  MA_BenePsnYrs = MA_AND_OTH_BENES,
)
pene_df <- select(
  pene_df,
  c("FIPS_Code","StateAbbrev", "Year", "AllMedBenePsnYrs", "OrigMedBenePsnYrs", "MA_BenePsnYrs", "ACOBenePsnYrs")
)

# filter out obs where total medicare enroll is unknown. This denominator is required to compute penetration figure.
print(count(pene_df))
pene_df <- filter(
  pene_df, 
  !is.na(pene_df$AllMedBenePsnYrs)
)

# assume if there is no ACO record, that there were no assigned enrollees
pene_df[is.na(pene_df$ACOBenePsnYrs), "ACOBenePsnYrs"] <- 0
```

Calculate accountable care penetration figures by county and year
```{r}
pene_df <- pene_df %>%
  mutate(ACOPenetration = ACOBenePsnYrs / AllMedBenePsnYrs) %>%
  mutate(MAPenetration = MA_BenePsnYrs / AllMedBenePsnYrs) %>%
  mutate(AccountableCarePenetration = MAPenetration + ACOPenetration) %>%
  mutate(LogAllMedBenePsnYrs = log(AllMedBenePsnYrs))
```

There appears to be a positive relationship between accountable care penetration and the size of the county. This may be introduced by our obfuscated data issue described above. In 2019, there are also penetration values above 1, which should not be possible, given that MA and ACO enrollees are drawn from the set of total medicare beneficiaries. This issue appears to affect 63 counties, solely in 2019.

```{r}
plt <- ggplot(filter(pene_df, Year == 2019), mapping=aes(x=AccountableCarePenetration, y=LogAllMedBenePsnYrs)) +
  geom_point()
plt

# 63 observations in 2019
unique(filter(pene_df, AccountableCarePenetration > 1)$Year)
```

Calculated MA penetration figures are corroborated by [KFF Report](https://www.kff.org/medicare/issue-brief/medicare-advantage-in-2022-enrollment-update-and-key-trends/#:~:text=In%202022%2C%20nearly%20half%20of,(19%25%20to%2048%25).), consistently 1-2% below. Not perfect, but good enough.

```{r}
pene_df %>%
  group_by(Year) %>%
  summarize(ma_sum = sum(MA_BenePsnYrs, na.rm = TRUE), med_sum = sum(AllMedBenePsnYrs), 
            pene = sum(MA_BenePsnYrs, na.rm=TRUE) / sum(AllMedBenePsnYrs, na.rm=TRUE))
```

```{r}
county_pene <- pene_df
rm(pene_df)
save(county_pene, file=here("data", "interim", "county_vbc_penetration.Rda"))
```

```{r}
load(here("data", "interim", "county_vbc_penetration.Rda"))
load(here("data", "interim", "county_level_staffing.Rda"))

county_staffing <- rename(
  county_staffing,
  Year = collection_yr
)

merged <- merge(
  county_pene,
  county_staffing,
  by=c("FIPS_Code", "Year"),
  all.x = TRUE
)

# multiply pene figures by 100 to ease interpretation
merged$AccountableCarePenetration = merged$AccountableCarePenetration * 100
merged$MAPenetration = merged$MAPenetration * 100
merged$ACOPenetration = merged$ACOPenetration * 100
merged$YearFactor = as.factor(merged$Year)
```

Visualizing all years, the 2019 strangeness for Accountable Care Penetration is apparent. There is significantly higher dispersion for this observation. Could this be due to ACO dropout at high rate in 2019? Skewed small ACO's, unaffliated with health system.
```{r}
plt <- ggplot(filter(merged, Year == 2017), mapping=aes(x = AccountableCarePenetration, y = mean_cna_reported_hprd)) + geom_point() + xlim(0,100) + ylim(0,6) + ggtitle(label="Reported Nursing Aid Hours Per Resident Per Day By VBC Penetration (2017)")
plt
plt <- ggplot(filter(merged, Year == 2018), mapping=aes(x = AccountableCarePenetration, y = mean_cna_reported_hprd)) + geom_point()  + xlim(0,100) + ylim(0,6) + ggtitle(label="Reported Nursing Aid Hours Per Resident Per Day By VBC Penetration (2018)")
plt
plt <- ggplot(filter(merged, Year == 2019), mapping=aes(x = AccountableCarePenetration, y = mean_cna_reported_hprd)) + geom_point()  + xlim(0,100) + ylim(0,6) + ggtitle(label="Reported Nursing Aid Hours Per Resident Per Day By VBC Penetration (2019)")
plt
plt <- ggplot(filter(merged, Year == 2020), mapping=aes(x = AccountableCarePenetration, y = mean_cna_reported_hprd)) + geom_point()  + xlim(0,100) + ylim(0,6)+ ggtitle(label="Reported Nursing Aid Hours Per Resident Per Day By VBC Penetration (2020)")
plt
plt <- ggplot(filter(merged, Year == 2021), mapping=aes(x = AccountableCarePenetration, y = mean_cna_reported_hprd)) + geom_point() + xlim(0,100) + ylim(0,6) + ggtitle(label="Reported Nursing Aid Hours Per Resident Per Day By VBC Penetration (2021)")
plt
```

How to explain the spike in VBC penetration in 2019? Something appears to be wrong with the data... Seeing rates \>1

[Drop in ACO's in 2019](https://www.naacos.com/press-release--medicare-aco-participation-flat-in-2022 ). 
  * So why would pene increase in this year?
  * Dropout due to 5 year contracts, enforcing down-risk for some organizations.

## Results
Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.

There was no strong relationship between staffing ratios and accountable care penetration, as my expert advisers graciously informed me a priori. The estimated reduction in staffing ratios is negligible regardless of which specific role is considered (CNA, RN, all nursing staff). An additional pct point of MA penetration is assocaited with a decrease of ~10 seconds per resident per hour in total nursing staff. Even assuming +10 pct point MA pene increase, this would imply a 1 min 40 second decrease in total nurse staffing ratios. The estimated association of ACO penetration is even smaller. These results are insensitive to excluding data from 2019.

```{r}
options(max.print=30) # omit distracting fixed effects
model <- lm(mean_total_reported_hprd ~ MAPenetration + ACOPenetration + StateAbbrev.x:YearFactor + mean_total_cm_hprd, merged)
print(summary(model))
print(-2.813e-03 * 60 * 60) # second reduction

#model <- lm(mean_cna_reported_hprd ~ MAPenetration + ACOPenetration + StateAbbrev.x:YearFactor + mean_cna_cm_hprd, merged)
#print(summary(model))

#model <- lm(mean_cna_reported_hprd ~ MAPenetration + ACOPenetration + StateAbbrev.x:YearFactor + mean_cna_cm_hprd, #filter(merged, YearFactor != 2019))
#print(summary(model))

#model <- lm(mean_rn_reported_hprd ~ MAPenetration + ACOPenetration + StateAbbrev.x:YearFactor + mean_rn_cm_hprd, merged)
#print(summary(model))

# county level fixed effects - did not produce any useful output due to singularities
#model <- lm(mean_cna_reported_hprd ~ MAPenetration + ACOPenetration + FIPS_Code:YearFactor + mean_cna_cm_hprd, merged)
#print(summary(model))
```
In addition, I have also omitted important controls which are unavailable in public datasets. Notably, Atul et al. include controls for the proportion of residents with Medicaid coverage, as well as metrics to represent market dynamics, including SNF concentration within a given hospital referral region. I would expect Medicaid coverage in particular to confound the relationship between MA penetration and staffing ratios.
![Swing and a miss. But I'm glad I asked the question!](golf-kid.gif)

What counter-balancing forces may contribute to this outcome?

*   The water balloon effect. SNF's receiving pressure in Part A space may respond by increasing services in Part B.
*   Some SNF's receive financial supports via integration in larger system. For instance, a SNF onsite at a continuing care retirement community, or owned by a hospital may be better equipped in negotiation with MA health plans.
*   SNF contracting with MA health plans may be supportive in some respects. Depending on contract parameters, it may bring revenue predictability that a fee for service arrangement does not.
*   ACO's exert very little direct influence over SNF operations. They simply do not have many levers to pull in order to affect behavior. This outcome contradicts portrayals of SNF's as the "ACO piggybank".
*   Accountable care penetration might increase concentration of referrals. This could adversely affect some SNF's, but support others, resulting in a neutral aggregate impact.


# Part 2
## Introduction
In this part, I investigate claims in a [2021 NyTimes article](https://www.nytimes.com/2021/03/13/business/nursing-homes-ratings-medicare-covid.html) that some SNF's have padded their public star ratings by exploiting a nuance in staffing ratio calculations. Specifically, nursing staff "with administrative duties" may be counted alongside direct care nursing staff in calculating staffing ratios. As an example, per the CMS data payroll submission manual, an "LPN with administrative duties" is defined as:
"""
...other nurses whose principal duties are spent conducting administrative functions. For example, the LPN Charge Nurse 
is conducting educational/in-service, or other duties which are not considered to be direct care giving.
"""

Such a nurse may engage in direct resident care, as long as it's not their "principal duties". *Or, they might sit at the front desk and fill out paperwork for their entire shift.* The category is unfortunately ambiguous.

## Methods
To assess whether nursing homes use "admin duties" to pad their quality scores, I used publicly available, daily  [payroll-based staffing  journals](https://data.cms.gov/quality-of-care/payroll-based-journal-daily-nurse-staffing) submitted to CMS. I then construct an "administrative intensity" metric as the percentage of total nursing hours which are reported "with admin duties". Finally, using a case / control design, I use logistic regression to model if a large year-over-year change in admin intensity is predictive of year-over-year staffing rate increases.

*Case Definition*
I define a case as a nursing home with a year-over-year (YOY) change in staffing star rating from <4 to >= 4. This reflects my assumption that shenanigan-prone facilities might play games with CMS's overall star rating methodology. Specifically, in the time period studied (2017-2021), a facility with >= 4 staffing rating received a +1 star increase to their overall star rating. As an example, the facility described in the Nytimes article, Sun Terrace in FL (CCN = 105319), increased their staffing star rating from 2->5 between 2017 and 2018, which yielded an extra star in their overall star rating. This increase coincided with a 18 pct point increase in admin intensity, from 6% to 24%.

*Control Definition*
Controls are all yoy observations which:
  1. Do not fit the case definition
  2. Have complete year over year observations for both daily payroll based admin intensity and publically reported staffing ratios. Note, when a facility misses or incorrectly reports staffing data, their staffing rating drops to 1. I have ommitted these observations because it implies that CMS rejected the data I use to calculate administrative intensity.

```{r}
load(here("data", "interim", "snf_provider_info.Rda"))
pbj <- read.csv(here("data", "interim", "pbj_facility_level.csv"))
```

Create YOY staffing ratios and quality star changes using existing dataset and add the case definition.

```{r}
staffing_next_yr <- select(q4_snf_data, c("federal_provider_number", "staffing_rating", "collection_yr", "adjusted_rn_staffing_hours_per_resident_per_day", "adjusted_total_nurse_staffing_hours_per_resident_per_day", "ownership_type", "average_number_of_residents_per_day", "chow", "continuing_care_retirement_community","average_number_of_residents_per_day",
"overall_rating", "health_inspection_rating", "qm_rating"))
staffing_next_yr$collection_yr = staffing_next_yr$collection_yr - 1

# left join assigns x and y to shared column names
# x suffix = y1
# y suffix = y2
snf_level_data <- q4_snf_data %>%
  left_join(staffing_next_yr, by=c("federal_provider_number", "collection_yr"))
staffing_yoy <- select(snf_level_data,
    c("federal_provider_number", "staffing_rating.x", "collection_yr",
       "staffing_rating.y","adjusted_rn_staffing_hours_per_resident_per_day.x", "adjusted_total_nurse_staffing_hours_per_resident_per_day.x",
      "adjusted_rn_staffing_hours_per_resident_per_day.y", "adjusted_total_nurse_staffing_hours_per_resident_per_day.y",
      "ownership_type.x", "ownership_type.y", "average_number_of_residents_per_day.x",
      "average_number_of_residents_per_day.y",
      "chow.y", "continuing_care_retirement_community.y",
      "average_number_of_residents_per_day.x", "average_number_of_residents_per_day.y",
      "overall_rating.x", "overall_rating.y", 
      "health_inspection_rating.x", "health_inspection_rating.y", 
      "qm_rating.x", "qm_rating.y")) %>%
  
  # remove cases where no staffing rating is reported...
  filter(is.na(staffing_rating.y) == FALSE & is.na(staffing_rating.x) == FALSE) %>%
  
  # add YOY columsn
  mutate(overall_star_change=overall_rating.y - overall_rating.x) %>%
  mutate(inspect_star_change=health_inspection_rating.y - health_inspection_rating.x) %>%
  mutate(qm_star_change=qm_rating.y - qm_rating.x) %>%
  mutate(staff_star_change=staffing_rating.y - staffing_rating.x) %>%
  mutate(rn_hr_prpd_change=adjusted_rn_staffing_hours_per_resident_per_day.y - adjusted_rn_staffing_hours_per_resident_per_day.x) %>%
  mutate(total_hr_prpd_change=adjusted_total_nurse_staffing_hours_per_resident_per_day.y - adjusted_total_nurse_staffing_hours_per_resident_per_day.x) %>%
  
  # add case control definition
  mutate(status=as.factor(
           ifelse(staff_star_change > 0 & staffing_rating.x < 4 & staffing_rating.y >=4,
          "Case", "Control")))
```

Assess missingness... 

```{r}
staffing_yoy %>% count(status, staff_star_change)
filter(staffing_yoy, is.na(rn_hr_prpd_change)) %>% count(status, staff_star_change)

# Dropped \~3k observations with at least one reported staffign ratios missing. 
staffing_yoy.complete <- filter(staffing_yoy, !(is.na(rn_hr_prpd_change) | is.na(total_hr_prpd_change)))
```

Create corresponding PBJ (Payroll Based Journal) dataframe, also Peanut Butter and Jelly

```{r}
pbj.q4 <- pbj %>%
  mutate(year = as.integer(substr(CY_Qtr, 1, 4))) %>%
  mutate(quarter = as.integer(substr(CY_Qtr, 6, 6))) %>%
  filter(quarter==4) # align with staffing metrics in nursing home compare dataset.

pbj.q4.yoy <- pbj.q4 %>%
  mutate(year = year + 1) %>%
  left_join(pbj.q4, by=c("PROVNUM", "year")) %>%
  filter(!is.na(quarter.y)) %>%
  
  # calculate YOY admin intensity change
  mutate(admin_change_pct_pt = (AdminIntensity.y - AdminIntensity.x) * 100)
```

Merge staffing YOY and payroll based journal datasets

55939 obs in pbj.q4.merged
55473 obs in staffing_yoy 
54807 obs in final dataframe

```{r}
library(stringr)
staffing_yoy.complete$year = staffing_yoy.complete$collection_yr + 1
pbj.q4.yoy <- rename(pbj.q4.yoy, federal_provider_number = PROVNUM )
final.df <- staffing_yoy.complete %>%
  inner_join(pbj.q4.yoy, by=c("year", "federal_provider_number"))
final.df <- subset(final.df, select=-c(
  collection_yr,
  CY_Qtr.x,
  CY_Qtr.y,
  quarter.x,
  quarter.y
))

# Create final data frame for logistic regression model
final.df$year_factor <- as.factor(final.df$year)
final.df$status <- factor(final.df$status, levels=c("Control", "Case")) # easier to interpret
final.df <- final.df %>%
  mutate(for_profit = ifelse(str_detect(ownership_type.y, 'For profit'), TRUE, FALSE)) %>%
  mutate(top_5_percentile_admin_change = ifelse(admin_change_pct_pt >= quantile(admin_change_pct_pt, .95), TRUE, FALSE)) %>%
  mutate(staffing_rating_seq=paste(staffing_rating.x, staffing_rating.y))
```

Visualize

```{r}
plt <- ggplot(final.df, aes(x=year_factor, y=admin_change_pct_pt)) + geom_violin()
plt
```

Analysis: Large staffing star changes, +3 and +4 do appear more likely to be cases.

TODO add hard line at 0...
Prune to + and - 50 pct points. Add staff star change as a facet...
Can I visualize to see the relationship with other quality scores?
```{r}
plt <- ggplot(final.df, aes(x=admin_change_pct_pt, y=staff_star_change, color=status)) + geom_point()
plt
```

```{r}
plt <- ggplot(final.df, aes(x=as.factor(staffing_rating.x), y=admin_change_pct_pt)) + geom_violin()
plt
```

```{r}
select(final.df, 
      c(staffing_rating.x, staff_star_change, status, year_factor),
      c(admin_change_pct_pt)
      ) %>% tbl_summary(by=year_factor)
```

```{r}
select(final.df, 
      staffing_rating_seq, year_factor) %>% tbl_summary(by=year_factor)
```


### Results
The model results suggest there is a relationship between a top 5th percentile change in admin intensity and a jump in staffing ratings, increasing the odds ratio by a factor of 1.64. This statistic requires *very careful interpretation* considering how case / controls were created. Unlike the NYTimes investigators, I did not perform any audits or due diligence at the facility level to separate helpful prune beneficial staffing improvements from "funny business". *Cases include everyone who made staffing ratio improvements*. Therefore, the coefficient on anomalous admin intensity increases is basically a Rorschach test indicating the interpreter's opinions regarding "administrators".

I have included controls:
* Year level fixed effects to address failed independence assumption using panel data. Multiple observations of the same facility are obviously not independent.
* For profit status (year 2)
* Starting avg number of residents. This tends to stay relatively static.
* Starting total nurse staffing hours
* Change of ownership during assessed time period

```{r}
model <- glm(status ~ top_5_percentile_admin_change + for_profit + average_number_of_residents_per_day.x + adjusted_total_nurse_staffing_hours_per_resident_per_day.x + chow.y + year_factor, family=binomial, final.df)
summary(model)
```

To steel-man both sides, an admin-proponent might argue that administration holds direct care staff accountable to higher quality, makes their work more efficient, and also participates in direct care when necessary. Therefore, increased admin intensity does not harm, and may support the patient care function. *Plus, if patients receive less attention, this should be visible in other quality scores going down!*

But... An admin-skeptic might argue that admin time is principally focused on curating data related to public-facing quality scores. Except for claim-based measures, quality scores are largely derived from documentation. If admin duties primarily comprise parsing / cleaning up documentation, admin hours might enhance quality scores, without affecting, or at the expense of direct patient care.

Wherever you stand, I suggest seeking alternative sources in addition to CMS star ratings in making decisions regarding your loved ones!