---
title: "BMIN503/EPID600 Project Template"
author: "Danielle Kellier"
bibliography: references.bib
output: 
  bookdown::html_document2:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
    keep_md: true
---

```{r install-packages, eval=FALSE}

install.packages("renv")
renv::restore()

```


```{r set-options, message=FALSE, cache=FALSE, warning=FALSE}
options(width = 400)
library(tidyverse)
library(gtsummary)
# library(simstudy)
library(randomForest)
library(inTrees)
library(glue)
library(icd)
# library(fabricatr)
library(mgcv)
library(ranger)
library(magrittr)
library(tidytext)
library(gt)
# library(ggforce)

# Settings
refresh = F #When true, reads in raw CSV files and processes. When false, loads in already-processed feather files
nobs <- 6000 #num of observations included in dataset
missing_threshold <- 0.05 # level of missingness acceptable for an individual variable
test_to_train <- c(0.8, 0.2) #ratio of data used from training dataset compared to testing data set
num_randomforest_trees <- 500 # number of randomForest trees

source("load_data.R") #Loads in records from outpatient visits from a folder not included in git repository


```  
***
Use this template to complete your project throughout the course. Your Final Project presentation will be based on the contents of this document. Replace the title/name above and text below with your own, but keep the headers.

1. Recall that you forked the [Final Project Repo](https://github.com/HimesGroup/BMIN503_Final_Project) and have downloaded it as a project to your local computer. Write the overview and introduction for your final project. The overview consists of 2-3 sentences summarizing the project and goals. For the introduction, the first paragraph describes the problem addressed, its significance, and some background to motivate the problem. In the second paragraph, explain why your problem is interdisciplinary, what fields can contribute to its understanding, and incorporate background related to what you learned from meeting with faculty/staff. Start working on the Methods/Results section, which consists of code and its output along with text describing what you are doing (Note: we will not check your code now, but you should have something in place before Assignment 6 is distributed).


# Overview
`Give a brief a description of your project and its goal(s), what data you are using to complete it, and what three faculty/staff in different fields you have spoken to about your project with a brief summary of what you learned from each person. Include a link to your final project GitHub repository.`

For children visiting the emergency department with a chief complaint of headache of migraine, are there features of their initial clinical presentation that can predict the presence of an intracranial abnormality? As a proof of concept, this study involves a retrospective cohort of children who visited the CHOP neurology outpatient clinic between 2016 and 2020 with the complaint of headache or migraine to predict risk of positive findings seen on imaging.

# Introduction 
`Describe the problem addressed, its significance, and some background to motivate the problem.`
`Explain why your problem is interdisciplinary, what fields can contribute to its understanding, and incorporate background related to what you learned from meeting with faculty/staff.`

Headache is consistently ranked as one of the top 10 chief complaints for an emergency room visit within the United States according to the National Hospital Ambulatory Medical Care Survey [@UnitedStatesDepartmentofHealthHumanServices-NationalCenterforHealthStatistics2019]. In fact, for over 1.7 million (5% overall) pediatric emergency visits in 2017, headache was listed as a reason for the visit [@UnitedStatesDepartmentofHealthHumanServices-NationalCenterforHealthStatistics2019]. Headache can be an alarming but largely nonspecific symptom with a wide range of causes including a disabling but ultimately benign primary headache disorder such as migraine, a secondary but nonconcerning case of influenza, or something more grave such as a brain tumor or subarachnoid hemorrhage. Although a neurologist or headache subspecialist may focus on pinpointing a headache's etiology to direct treatment, an emergency room physician's goal is to identify "red flags" or signs of a serious etiology requiring urgent intervention. Still, a symptom's status as a "red flag" can be questionable: changes in vision or sensation can be associated with a stroke or simply be the aura of an incoming migraine. As a result, roughly 18-41% of children with headache end up receiving some form of neuroimaging of which only 4-10% of those imaged will have new abnormal findings seen on imaging [@Cain2018; @Kan2000; @Sheridan2013]. Uneccesary imaging can be costly for both the patient and the healthcare system [@Cain2018; @Irwin2018; @Kan2000; @Young2018]. In addition, exposure to radiation from computed tomography scanning in children can greatly increase the risk of later malignancy [@Brenner2001; @Feng2010]. The high rate of children undergoing unnecessary imaging demonstrates a gap in the knowledge base available to emergency medicine physicians who must deal with a high volume of patients diverse in their clinical presentations and levels of acuity. 

Due to bottlenecks in the data acquisition process, electronic medical record data from emergency room visits has yet to be acquired. As a result, this paper will use already available medical record data from children visiting the outpatient neurology clinic with the complaint of headache or migraine as a proof of concept. This study aims to use machine learning to 1) select features of the clinical presentation that influence the risk of an abnormal finding on imaging and 2)  simplify the predictive model generated into an algorithm that can be disseminated and shared amongst emergency room physicians.

# Methods
Describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why. 


```{r eval = FALSE, echo = FALSE}

# imaging_data %>%
#   unnest_tokens(word, img_impression) %>%
#   count(word, sort = TRUE) %>% 
#   filter(between(row_number(),200,300)) %>%
#   pull(word) %>%
#   paste0('"', ., '"', collapse = ', ')

med_stop_words <- c(
  "the", "of", "and", "with", "i", "images", "have", "reviewed", "agree", 
  "brain", "personally", "mri", "interpretation", "in", "to", "end", "is", 
  "noncontrast", "impression", "as", "contrast", "a", "or", "on", "for", 
  "be", "are", "this", "at", "appears", "further", "has", "possibly",
  "without", "detailed", "there", "clinical", "above", "false", "evidence", 
  "seen", "may", "which", "patient", "report", "noted", "examination", "by", 
  "study", "related", "act", "apos", "identified", "s", "likely", "described", 
  "from", "magnetic", "resonance", "codes", "professional", "notification", 
  "dr", "appearance", "mr", "associated", "clinically", "recommended", "an", 
  "interpretation.act", "up", "correlation", "please", "represent", "requires",
  "new", "follow", "imaging", "but", "correlate", "evaluation", "along", 
  "performed", "can", "could", "was", "change", "if", "however", "discussed", 
  "most", "time", "visualized", "compared", "were", "limits", "that", "into", 
  "these", "involving", "appear", "exam", "than", "also", "definite", "pm", 
  "source", "due", "post", "limitations", "major", "although", "tesla"
)

icd10cm <- get_icd10cm_latest()

num_grams <- 3
impression_flags <- readxl::read_excel("../../Secondary Headache (SNOOPY Review)/Danielle work/imaging_impressions.xlsx") %>%
  mutate(flag = case_when(
    `...7` == "FALSE" ~ "Other",
    `...7` == "TRUE" ~ "Unremarkable",
    is.na(`...7`) ~ "Other",
    TRUE ~ as.character(`...7`))) %>%
  unnest_tokens(ngram, img_impression, token = "ngrams", n = num_grams) %>%
  separate(ngram, paste0("word", 1:num_grams), sep = " ", remove = FALSE) %>%
  mutate(across(starts_with("word"), 
                ~ as.numeric(.x %in% med_stop_words | str_detect(.x, "\\d+")))) %>%
  rowwise() %>%
  filter(!is.na(ngram) & sum(c_across(starts_with("word"))) <= 1) %>%
  group_by(flag) %>%
  filter(n_distinct(record_id) >= 3) %>%
  mutate(
    flag_label = str_wrap(
      glue::glue("{flag} (n={n_distinct(record_id)})"), width = 30)
    ) %>%
  count(flag, flag_label, ngram) %>%
  bind_tf_idf(ngram, flag, n) %>% 
  slice_max(order_by = tf_idf, n = 10, with_ties = FALSE) %>% 
  ungroup() %>%
  mutate(ngram = fct_reorder(ngram, tf_idf)) %>%
  ggplot(aes(ngram, tf_idf, fill = flag_label)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ flag_label, scales = "free", ncol = 2) +
  scale_y_continuous(limits = c(0,NA)) +
  coord_flip() +
  theme_bw()

ggsave("Y:\\Downloads\\three-grams.png", width = 10, height = 10)

icd_flags <- readxl::read_xlsx("Additional ICD Codes.xlsx", sheet = "WIP4") %>%
  select(flag = Flag, icd10 = `ICD-10 codes`, code_meaning = `code meaning`) %>%
  filter(!is.na(icd10)) %>%
  separate_rows(icd10, sep = "(\\s)?,(\\s)?") %>%
  mutate(icd10 = str_remove_all(icd10, "\\.")) %>%
  separate(icd10, c("icd10_1", "icd10_2"), sep = "-", fill = "right", remove = FALSE) %>%
  mutate(across(c(icd10_1, icd10_2), ~match(.x, icd10cm$code))) %>%
  rowwise() %>%
  mutate(icdx = ifelse(
    !is.na(icd10_2), list(icd10cm$code[icd10_1:icd10_2]), list(icd10cm$code[icd10_1])
  )) %>%
  unnest(icdx) %>%
  filter(!is.na(icdx))




dx_visit <- visitDx_data %>%
  filter(redcap_repeat_instrument == "visit_diagnoses") %>%
  mutate(icdx = str_remove_all(icd10, "\\.")) %>%
  left_join(icd_flags %>%
              select(icdx, flag) %>% 
              mutate(icdx = as.character(icdx)), by = "icdx" ) %>%
  select(record_id, icd10, dx_name, icdx, flag)

pmh_data %>%
  filter(redcap_repeat_instrument == "all_problems") %>%
  mutate(icdx = str_remove_all(pl_icd10, "\\.")) %>%
  left_join(icd_flags %>%
              select(icdx, flag) %>% 
              mutate(icdx = as.character(icdx)), by = "icdx" ) %>%
  select(record_id, pl_icd10, pl_dx_nm, icdx, flag) %>% 
  filter(!is.na(flag))


num_grams <- 4 
dx_imaging <- imaging_data %>% 
  filter(redcap_repeat_instrument == "imaging") %>%
  group_by(record_id) %>%
  filter(img_ord_dt == first(img_ord_dt)) %>%
  left_join(dx_visit, by = "record_id") %>%
  unnest_tokens(ngram, img_impression, token = "ngrams", n = num_grams) %>%
  separate(ngram, paste0("word", 1:num_grams), sep = " ", remove = FALSE) %>%
  mutate(across(starts_with("word"), 
                ~ as.numeric(.x %in% med_stop_words | str_detect(.x, "\\d+")))) %>%
  rowwise() %>%
  filter(!is.na(ngram) & sum(c_across(starts_with("word"))) <= 2)

tf_idf_plot_raw <- dx_imaging %>% 
  mutate(flag = ifelse(is.na(flag), "Other", flag)) %>%
  count(flag, ngram) %>%
  bind_tf_idf(ngram, flag, n) %>% 
  group_by(flag) %>%
  filter(n_distinct(record_id) >= 5) %>%
  mutate(
    flag_label = str_wrap(
      glue::glue("{flag} (n={n_distinct(record_id)})"), width = 30)
    ) %>%
  slice_max(order_by = tf_idf, n = 20, with_ties = FALSE) %>% 
  ungroup() %>%
  mutate(ngram = fct_reorder(ngram, tf_idf)) %>%
  ggplot(aes(ngram, tf_idf, fill = flag_label)) +
  geom_col(show.legend = FALSE) +
  scale_y_continuous(limits = c(0,NA)) +
  coord_flip() +
  theme_bw()
  
gg <- tf_idf_plot_raw +
 facet_wrap_paginate(~ flag_label, scales = "free", ncol = 2, nrow = 1, page = 1) 

n <- n_pages(gg)

pdf('Y://Downloads\\tf-idfs_by_flag.pdf', paper= 'A4r', w= 297/25.4, 210/25.4)
for(i in 1:n){
    print(gg + facet_wrap_paginate(~ flag_label, scales = "free", ncol = 2, nrow = 1, page = i))
}
dev.off()

  
  # mutate(headache = is.numeric(str_detect(icd10, paste(c("G43", "G44", "R51", "M54.2"), collapse = '|'))),
  #        psych = is.numeric(str_detect(icd10, paste(c("F41", "F32.9", "F90"), collapse = '|'))),
  #        dizzy = is.numeric(str_detect(icd10, paste(c("R42"), collapse = '|'))),
  #        autism = is.numeric(str_detect(icd10, paste(c("F84.0"), collapse = '|'))),
  #        autonomic = is.numeric(str_detect(icd10, paste(c("G90.9", "R00.0", "I95.1"), collapse = '|'))),
  #        seizure = is.numeric(str_detect(icd10, paste(c("R56.9"), collapse = '|'))),
  #        chiari = is.numeric(str_detect(icd10, paste(c("G93.5"), collapse = '|'))),
  #        concussion = is.numeric(str_detect(icd10, paste(c("F07.81", "S06.0X0S"), collapse = '|'))),
  #        coma = is.numeric(str_detect(icd10, paste(c("R40.4"), collapse = '|')))) 



dx_imaging %>%
  filter(across(headache:seizure, ~ .x == 0)) %>%
  group_by(icd10) %>%
  summarise(dx_name = Mode(dx_name),
            n_patients = n_distinct(record_id)) %>% 
  arrange(desc(n_patients)) %>%
  View()  

dx_imaging %>% group_by(icd10) %>% 
  summarise(dx_name = Mode(dx_name),
            n_patients = n_distinct(record_id)) %>% 
  arrange(desc(n_patients)) %>%
  View()

```

```{r}
# Pull in data from outpatient redcap database

# Select names of columns that end in ".factor"
demo_factor_names <- str_subset(names(demo_data),".factor") %>% 
  str_remove(., ".factor") 

# Select names of factors with only two levels
 #Can keep binary integer columns instead
ha_factor_names_binary <- ha_data %>% 
  select(matches("^p_.*\\.factor$")) %>%
  select_if(~ nlevels(.) == 2) %>%
  names(.)

# Select names of factors with more than two factors.
# Remove integer column counterparts
ha_factor_names_nonbinary <- ha_data %>% 
  select(matches("^p_.*\\.factor$")) %>%
  select_if(~ nlevels(.) != 2) %>%
  names(.) %>%
  str_remove(., ".factor")

# Select names of columns that end in ".factor"
image_factor_names <- str_subset(names(imaging_data),".factor") %>% 
  str_remove(., ".factor")

```

```{r serious-codes}

icd10cm <- get_icd10cm_latest()

icd_flags <- readxl::read_xlsx("Additional ICD Codes.xlsx", sheet = "WIP4") %>%
  select(flag = Flag, icd10 = `ICD-10 codes`, code_meaning = `code meaning`) %>%
  filter(!is.na(icd10)) %>%
  separate_rows(icd10, sep = "(\\s)?,(\\s)?") %>%
  mutate(icd10 = str_remove_all(icd10, "\\.")) %>%
  separate(icd10, c("icd10_1", "icd10_2"), sep = "-", fill = "right", remove = FALSE) %>%
  mutate(across(c(icd10_1, icd10_2), ~match(.x, icd10cm$code))) %>%
  rowwise() %>%
  mutate(icdx = ifelse(
    !is.na(icd10_2), list(icd10cm$code[icd10_1:icd10_2]), list(icd10cm$code[icd10_1])
  )) %>%
  unnest(icdx) %>%
  filter(!is.na(icdx))

```

```{r, warning=FALSE}

# Select subset of demographic data based on number of observations
# Select patients that completed headache questionnaire
# Clean up columns and remove duplicates (keep factor counterparts)
dt_demo <- demo_data %>%
  filter(between(age, 2,17) & pt_ha_quest_yn  == 1) %>%
  select(-all_of(demo_factor_names)) %>%
  rename_with(~str_remove(.x, ".factor")) %>%
  mutate(gender = fct_recode(gender, Other = "Unknown"),
         race = fct_collapse(race,
           Black = c("Black or African-American"),
           White = c("White"),
           other_level = "Other"
         ))

# Select subset of headache questionnaire data based on number of observations
# Clean up columns and remove duplicates (keep factor counterparts nonbinary, keep integer counterparts if binary)
dt_ha_data <- ha_data %>%
  filter(record_id %in% unique(dt_demo$record_id) ) %>%
  filter(patient_ha_complete == 2) %>%
  select(-all_of(c("redcap_repeat_instrument", "redcap_repeat_instance"))) %>%
  select(-starts_with("c_")) %>%
  select(-all_of(ha_factor_names_binary)) %>%
  select(-all_of(ha_factor_names_nonbinary)) %>%
  select(record_id, p_age_first_ha:p_assoc_sx_pul_ear___oth, p_prob_preg_birth:p_fam_hist_med___oth, p_ha_in_lifetime.factor:p_preg_full_term.factor) %>%
  select_if(~is.factor(.) | is.numeric(.)) %>%
  rename_with(~str_remove(.x, ".factor"))

# Merge demographic + HA data
# sample by number of observations
dt_demo_ha <- dt_demo %>%
  select(record_id, age, gender, ethnicity, race) %>%
  inner_join(dt_ha_data, by = "record_id") 

# Find cases of serious diagnostic codes given PRIOR to imaging
prior_flags <- icd_flags %>%
    filter(flag %in% c("CNS Hemorrhage", "Meningoencephalitis", "Hydrocephalus",
                      "Neoplasm - Malignant CNS", "Neoplasm - Benign CNS",
                      "Other intracranial or spinal malformation", "RCVS/PRES", 
                      "Past neurosurgical procedure", "Stroke, TIA") |
           grepl("G93", icdx) |
           grepl("Q05", icdx)
         ) %>%
  mutate(icdx = as.character(icdx)) %>%
  left_join(pmh_data %>% 
               filter(record_id %in% dt_demo_ha$record_id) %>% 
               mutate(icdx = str_remove_all(pl_icd10, "\\.")), 
             by = "icdx") %>%
  filter(!is.na(record_id))

#Exclude from dataset and sample
dt_demo_ha <- dt_demo_ha %>%
  filter(!record_id %in% prior_flags$record_id) %>%
  slice_sample(n = nobs)
  

```


```{r merge-imaging}


# Select subset of imaging data based on number of observations
# Select any imaging done on the first day (trying to exclude prior findings)
# Rough regex searches for key words to flag if impression mentioned a finding
dt_imaging_all <- imaging_data %>%
  filter(record_id %in% unique(dt_demo_ha$record_id)) %>%
  filter(redcap_repeat_instrument == "imaging") %>%
  group_by(record_id) %>%
  filter(img_ord_dt >= visit_dt) %>%
  filter(img_ord_dt == first(img_ord_dt)) %>% 
  mutate(
    impression_lower = str_squish(str_to_lower(img_impression)),
    finding = case_when(
      str_detect(impression_lower, 'discuss') ~ 1,
      str_detect(impression_lower, 'advise') ~ 1,
      str_detect(impression_lower, 'communicate') ~ 1,
      str_detect(impression_lower,
                 regex('112(?!.*false)' )) ~ 1,
      str_detect(impression_lower, 'finding[\\w\\s]{0,3}(?:was|were)[\\s]{0,3}(?:discuss|communica|review|relay|acknowled)') ~ 1,
      str_detect(impression_lower, '\\bemergen') ~ 1,
      str_detect(impression_lower,
                 regex('immediate(?!.*(?:postsurg|post surg|adjacent|subjacent|postoper|prior))' )) ~ 1,
      str_detect(impression_lower, 'phone') ~ 1,
      str_detect(impression_lower, 'email') ~ 1,
      TRUE ~ NA_real_),
    finding = case_when(
      finding == 1 ~ 1,
      is.na(finding) & str_detect(impression_lower,
                                  regex("(?:otherwise|essential).*[\\s,]{0,3}(?:unremarkable|normal)")) ~ 0,
      is.na(finding) & str_detect(impression_lower, "^[\\s\\d\\.]{0,4}(?:unremarkable|normal|stable)") ~ 0,
      is.na(finding) & str_detect(impression_lower, "^[\\s\\d\\.]{0,4}(?:brain mr| brain ct|brain|impression)[^a-z]{0,4}(?:unremarkable|normal)") ~ 0,
      is.na(finding) & str_detect(impression_lower, '\\b(?:no|none)[\\w\\s]{0,30}(?:finding|abnormality|mass|hydrocephalus|infarct|hemo|struct)') ~ 0,
      TRUE ~ NA_real_
    )) 

dt_imaging <- dt_imaging_all %>%
  summarise(
    finding = case_when(
      any(finding == 1) ~ 1,
      all(finding == 0) ~ 0,
      all(is.na(finding)) ~ NA_real_
    ),
    img_ord_dt = first(img_ord_dt),
    img_impression = first(img_impression),
    .groups = 'drop')


```




```{r warning=FALSE}

# Merge in imaging data
# Mark if patient was imaged based on date of imaging 
# (not all kids imaged had conclusive findings)
dt_all <- dt_demo_ha %>%
  left_join(dt_imaging, by = "record_id") %>%
  mutate(imaging = ifelse(!is.na(img_ord_dt), 1, 0))

# Select names of columns with missingness above a threshold
# Exclude columns from imaging dataset as not all kids were imaged
dt_missing_cols_all <- dt_all %>%
  select(-any_of(names(dt_imaging))) %>%
  summarise(across(everything(), ~sum(is.na(.x))/length(.x))) %>%
  pivot_longer(everything(), names_to = "column", values_to = "prop") %>%
  filter(prop >= missing_threshold)

dt_missing_cols_imaged <- dt_all %>%
  filter(imaging == 1) %>%
  select(-any_of(names(dt_imaging))) %>%
  summarise(across(everything(), ~sum(is.na(.x))/length(.x))) %>%
  pivot_longer(everything(), names_to = "column", values_to = "prop") %>%
  filter(prop >= missing_threshold)

# Exclude variables with missingness above a certain variable
dt <- dt_all %>%
  select(-any_of(dt_missing_cols_imaged$column))

dt_exclude <- demo_data %>% 
  filter(between(age, 2,17)) %>% 
  select(record_id, age) %>% 
  anti_join(dt, by = "record_id")

```


## Participants
We conducted a retrospective cohort study consisting of children ages 2-18 seen in the outpatient pediatric neurology clinic between July 2016 and November 2020 with the primary complaint of headache or migraine. This study took place within a tertiary pediatric hospital system which often receives referrals from nearby healthcare systems for specialized care. This can inflate the number of cases with rare and/or serious etiologies compared to another pediatric outpatient setting. Children with a condition predisposing to an intracranial abnormality but no prior documented findings were included in the study. 

## Predictors
We used a combination of demographic features (age, sex, race, ethnicity) and clinical features as predictors. Patients and their families completed health questionnaires during their outpatient visit of interest, describing their headache type and severity, associated symptoms, medication use, and past medical and family history. Given the nature of secondary data collection, we expected to encounter records with missing variables. We excluded potential variables from our predictors if their missingness in the dataset exceeded `r scales::percent(missing_threshold )`. For variables below this threshold, we imputed missing values based on proximity to other cases using the `rfImpute` function within the *randomForest* package [@AndyLiaw2002].

## Outcome
We selected outpatient cases in which imaging was performed after the outpatient visit. We then classified cases based on the documentation associated with the soonest imaging encounter performed after the clinic visit of interest. As some cases had multiple imaging procedures performed during a single encounter, subsequent imaging procedures performed on the same day as the initial imaging procedure were included. Using the `stringr` package [@RStringr], we selected cases in which the radiologist documented urgently contacting the ordering physician and cases in which the radiologist documented imaging as 'unremarkable' or 'normal'. In the former scenario, these cases were labeled as having 'urgent findings' and our primary goal is to correctly classify these cases. In the latter scenario, these cases were as 'unremarkable' and serve as the contrasting set for our predictive model.

<!-- We extracted symptom information from a health questionnaire completed by the patient for the visit of interest. Based on past work, we were particularly interested in self-report of fever and other constitutional symptoms, acute neurological deficits, age, descriptions of sudden onset or "thunderclap" headache, awakening from sleep, occipital location, and responsiveness to positional changes or the Valsalva maneuver [@Do2019]. We also included other variables often associated with headache assessment including age, family history of headache, blurry vision, nausea and vomiting, photophobia and phonophobia, headache frequency at baseline, and current headache duration. 
Given the nature of secondary data collection, we expected to encounter records with missing variables. We excluded any records that lacked a visit diagnosis commonly associated with headache such as minor trauma to extremities. Given the broad relationship between infection and headache, we included cases related to head and neck local infections such as sinusitis and dental infections, along with cases of systemic infection such as influenza and recorded sepsis. Further, we excluded records that lacked documentation of a neurological exam, headache characterization, or overall lacked data for >= XX% variables of interest. For analysis of a single variable, we excluded records with missing data if the variable's missingness in the dataset did not exceed 10% across age and sex strata. In cases where a variable's missingness rested between 10% and 20%, we used data imputation with the median for a continuous value by age and sex strata. Missingness that exceeded 20% was evaluated on a case-by-case basis to assess the value of and ramifications of analysis with investigation of nonrandom patterns. -->

## Statistical Analysis and Model Creation
Data analysis was done using `r R.version.string` [@RCoreTeam2020]. We used the *gtsummary* package to perform descriptive analyses on the demographic characteristics of the dataset [@DanielDSjoberg2020].  We used the *tidyverse* package collection extensively throughout analysis and particularly relied on *ggplot2* for graph creation [@Wickham2019]. For descriptive purposes, data was represented using the median and interquartile ranges for continuous variables and counts and proportions for categorical data.

After descriptive analyses, we divided the dataset into 2 parts for model training (`r scales::percent(test_to_train[[1]])`) and testing (`r scales::percent(test_to_train[[2]])`). We used the *randomForest* R package based on Leo Breiman's machine learning method [@AndyLiaw2002; @Breiman2001] to create a supervised machine learning model capable of predicting presence of urgent imaging findings based clinical presentation. This method is notable for creating an 'ensemble' set of decision trees based on randomly generated subsets of cases and predictors which classifies cases based on general consensus. We then simplified this ensemble model into a single decision tree using the *inTrees* package [@Deng2014] and compared performance across both ensemble and simplified models. 

# Results
Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.

```{r table-1}

#Table 1 to describe patient characteristics
table_1 <- dt %>%
  mutate(
    finding = factor(case_when(
      finding == 0 ~ "Unremarkable",
      finding == 1 ~ "Urgent Findings",
      is.na(finding) & imaging == 1 ~ "Non-Urgent Findings",
      is.na(finding) & imaging == 0 ~ "Not imaged"
    ), levels = c("Urgent Findings", "Non-Urgent Findings", "Unremarkable", "Not imaged"))) %>%
  mutate(across(where(is.factor), fct_drop)) %>%
  select(age, gender, race, ethnicity, finding) %>%
  rename_with(stringr::str_to_title) %>%
  tbl_summary(by = Finding)

table_1


```



We found a total of `r demo_data %>% filter(between(age, 2,17)) %>% nrow(.)` records from children ages 2 to 18 years who visited the outpatient neurology clinic with a complaint of headache or migraine and. After excluding `r nrow(dt_exclude)` children who did not completed the headache questionnaire `r glue("(median age = {quant[3]} yo, IQR: {quant[2]}-{quant[4]})", quant = quantile(dt_exclude$age))`, we had a remaining `r nrow(dt)`children `r glue("(median age = {quant[3]} yo, IQR: {quant[2]}-{quant[4]})", quant = quantile(dt$age))` of which `r glue("{imaged$n} children ({imaged$prop}%)", imaged = dt %>% count(imaging) %>% mutate(prop = round(100*n/sum(n))) %>% filter(imaging == 0))` had undergone imaging (\@ref(table-1)). For the sake of simplification, we selected `r dt %>% filter(finding == 0) %>% nrow(.)` cases in which the radiology note describes unremarkable imaging compared to `r dt %>% filter(finding == 1) %>% nrow(.)` cases in which the radiologists described reaching out to the ordering physician to relay findings in an urgent manner. A remaining `r dt %>% filter(imaging == 1 & is.na(finding)) %>% nrow(.)` cases lacked a description of the radiologist reaching out to the ordering physician and often had findings that did not warrant urgent intervention such as anatomic variants. 



Given the nature of imaging done on an outpatient basis, few cases warranted urgent delivery of findings and we expect to have a greater proportion of urgent cases in the impending emergency medicine dataset from which the predictive model will have more data to examine for trends.


```{r propensity,echo=FALSE}

# # Calculates propensity scores for imaging selection based on demographics and symptoms
# prop_formula <- str_c("imaging ~ s(age) + ", paste0(names(select(dt, -record_id, -age, -finding:-imaging)), collapse = " + "))
# 
# propensity_mod <- gam(as.formula(prop_formula), 
#                       family = binomial(), data = dt)
# # Create curated dataset for machine learning
# # Select kids with imaging and conclusive findings
# # Select kids with no NAs
# # Predict propensity score and inverse weight
# dt_propensity <- dt %>%
#   filter(imaging == 1 & !is.na(finding)) %>%
#   modelr::add_predictions(propensity_mod, 
#                           var = "propensity_score", type = "response") %>%
#   filter(complete.cases(.)) %>%
#   mutate(propensity_weight = (1 /(propensity_score)),
#          finding = factor(finding))


```


```{r echo = FALSE}

# set.seed(12345)
# 
# # Create random assignments for test and training datasets
# dt_assignments <- sample(1:2, size = nrow(dt_propensity), 
#                          replace = T, prob = test_to_train)
# 
# dt_train <- filter(dt_propensity, dt_assignments == 1) 
# 
# dt_test <- filter(dt_propensity, dt_assignments == 2) 
# 
# #Select variables not to include in machine learning 
# trim_vars <- dt_propensity %>%
#   select(record_id, finding:imaging, starts_with("propensity")) %>%
#   names(.) 
# 
# #Convert predictor variables into a matrix
# dt_predictors <- select(dt_train, -all_of(trim_vars)) %>% data.matrix()


```

```{r}

set.seed(12345)

#Select variables not to include in machine learning 
trim_vars <- dt %>%
  select(record_id, img_ord_dt:imaging, starts_with("propensity")) %>%
  names(.) 

#Select predictor variables and impute
dt_findings <- dt %>% 
  filter(imaging == 1 & !is.na(finding)) %>%
  mutate(finding = factor(finding)) %>%
  select(-all_of(trim_vars))

dt_findings_impute <- rfImpute(finding ~ ., data = dt_findings)

# Create random assignments for test and training datasets
dt_assignments <- sample(1:2, size = nrow(dt_findings_impute), 
                         replace = T, prob = test_to_train)

dt_train <- filter(dt_findings_impute, dt_assignments == 1)

dt_test <- filter(dt_findings_impute, dt_assignments == 2)


#Select predictor variables and impute
dt_predictors <- dt_train %>% select(-finding)


```


```{r randomForest-cutoffs}
 
set.seed(1234)

pred_normal_cutoff_seq <- c(seq(0.5,0.95, 0.05), seq(0.96,0.99, 0.01))

cutoff_performance <- pred_normal_cutoff_seq %>%
  map_df(function(curr_cutoff){
    
    outcome_cols <- c(TP = 0, FN = 0, FP = 0, TN = 0)

    curr_rf <- randomForest(
      x = dt_predictors, y = dt_train$finding, ntree = num_randomforest_trees,
      importance = TRUE, cutoff = c(curr_cutoff, 1-curr_cutoff), nodesize = 3
    )
    
    curr_pred <-  predict(curr_rf, dt_test, type = "response")
    
    curr_perform <- tibble(cutoff = curr_cutoff,
                           model = list(curr_rf),
                           true = dt_test$finding, 
                           pred = curr_pred) %>%
      count(cutoff, model, true, pred) %>%
  mutate(outcome = case_when(
    true == 1 & pred == 1 ~ "TP",
    true == 1 & pred == 0 ~ "FN",
    true == 0 & pred == 0 ~ "TN",
    true == 0 & pred == 1 ~ "FP"
  )) %>% 
  pivot_wider(id_cols = c(cutoff, model), names_from = outcome, 
              values_from = n, values_fill = 0) %>%
  add_column(!!!outcome_cols[!names(outcome_cols) %in% names(.)]) %>%
  mutate(Sensitivity = TP/(TP+FN),
         Specificity = TN/(TN+FP)) 
    
    curr_perform
  })

rf_normal_cutoff <- 0.99

cutoff_plot_df <- cutoff_performance %>%
  select(cutoff, Sensitivity, Specificity) %>%
  pivot_longer(-cutoff, names_to = "measure", values_to = "score")
  
cutoff_plot <- cutoff_plot_df %>%
  ggplot(aes(x = cutoff, y = score, group = measure, color = measure)) +
  geom_line(size = 1.5) +
  geom_vline(xintercept = rf_normal_cutoff, colour = "#440154FF", linetype = "dashed", size = 1.2) +
  annotate(
    geom = "segment", x = 0.85, y = 0.1, xend = rf_normal_cutoff, yend = 0.3, 
    colour = "#440154FF", size = 1, arrow = arrow(length = unit(3, "mm"))
  ) +
  annotate(geom = "text", x = 0.83, y = 0.1, label = "Chosen cutoff", hjust = "right", colour = "#440154FF") +
  scale_y_continuous(name = "Proportion", limits = c(0,1), breaks = seq(0,1,0.25)) +
  scale_x_continuous(name = "Cutoff for predicting 'unremarkable'", limits = c(0.5, 1), breaks = c(seq(0.5, 0.9, 0.1), 0.99)) +
  ggtitle("Random Forest Performance\nwith varying cutoffs") +
  cowplot::theme_half_open() +
  theme(legend.title = element_blank())

cutoff_plot

```


After imputation, we chose a cutoff of at least `r scales::percent(rf_normal_cutoff)` agreement between decision trees needed for the predictive model could label a case as 'unremarkable'. However this chosen cutoff had high sensitivity with poor specificity. In clinical practice, emergency room physicians often operate at a low threshold of suspicion for possible grave etiologies. Conversely, that means a physician must be very confident before they exclude a possible serious condition. At default, cutoffs are set at the inverse of the number of possible labels, in this case, a cutoff of 0.50 between the two labels of 'unremarkable' and 'serious'. Increasing the cutoff to higher levels would more closely mimic the natural high suspicion seen in clinical practice, however, the model appears to perform poorly with no apparent threshold that carries both high sensitivity and specificity.

```{r randomForest-creation}

set.seed(12345)

dt_randomForest <- filter(cutoff_performance, cutoff == rf_normal_cutoff)$model[[1]]

# dt_randomForest <- randomForest(
#   x = dt_predictors, y = dt_train$finding, ntree = num_randomforest_trees,
#   importance = TRUE, cutoff = c(rf_normal_cutoff, 1-rf_normal_cutoff), nodesize = 3
#   )

ha_var_labels <- as_tibble(sjlabelled::get_label(ha_data), rownames = "var_name") %>%
  rename(var_mean = value)

rf_importance <- as_tibble(dt_randomForest$importance, rownames = "var_name") %>%
  select(var_name, starts_with("Mean")) %>%
  mutate(MeanDecreaseAccuracy_Rank = row_number(desc(MeanDecreaseAccuracy)),
         MeanDecreaseGini_Rank = row_number(desc(MeanDecreaseGini))) %>%
  filter(MeanDecreaseAccuracy_Rank <= 10 |
           MeanDecreaseGini_Rank <= 10) %>%
  left_join(ha_var_labels, by = "var_name") %>%
  mutate(
    `Variable Name` = case_when(
      !is.na(var_mean) ~ var_mean,
      TRUE ~ str_to_sentence(var_name)),
    `Variable Name` = str_replace(`Variable Name`, "PT", "Physical therapy" ),
    `Variable Name` = str_replace(`Variable Name`, "Ng-", "NG-" ),
    `Variable Name` = gsub("=(\\w)(\\w*)","=\\U\\1\\L\\2",`Variable Name`,perl=TRUE)
  ) %>%
  select(-var_name, -var_mean) %>%
  pivot_longer(cols = c(MeanDecreaseAccuracy, MeanDecreaseGini), names_to = "MeanDecrease", values_to = "score") %>%
  pivot_longer(cols = c(MeanDecreaseAccuracy_Rank, MeanDecreaseGini_Rank), names_to = "rank_label", values_to = "rank") %>%
  mutate(label = case_when(
    MeanDecrease == "MeanDecreaseAccuracy" & rank_label == "MeanDecreaseAccuracy_Rank" ~ "Accuracy",
     MeanDecrease == "MeanDecreaseGini" & rank_label == "MeanDecreaseGini_Rank" ~ "Gini",
    TRUE ~ NA_character_
  )) %>%
  filter(!is.na(label) & rank <= 10) %>%
  select(-MeanDecrease, -rank_label) %>%
  pivot_wider(id_cols = rank, names_from = label, 
              names_glue = "{label}.{str_to_title(.value)}",
              values_from = c(`Variable Name`, score)) %>% 
  arrange(rank) %>% 
  select(Rank = rank, `Accuracy.Variable Name`, Accuracy.Score, `Gini.Variable Name`, Gini.Score) 

rf_importance %>%
  gt() %>%
  tab_spanner_delim(delim = ".") %>%
  fmt_scientific(
    columns = vars(Accuracy.Score),
    decimals = 1
  ) %>%
  fmt_number(
    columns = vars(Gini.Score),
    decimals = 2
  )
  

```



Variables ranked as important to the predictive model appeared to follow some themes. Among the 20 most important variables according to mean decrease in accuracy or Gini index upon removal, patient age, sleep symptoms, history of speech or physical therapy, family history, and headache triggers appeared to be informative. However, given the poor performance of the model, it is hard to assess the true importance of these aspects.

```{r randomForest-prediction}
dt_randomForest_pred <- dt_test %>%
  predict(dt_randomForest, ., type = "response")

treeList_randomForest <- RF2List(dt_randomForest)
ruleExec_randomForest <- extractRules(treeList_randomForest, X = dt_predictors, digits = 2)

ruleMetric <- getRuleMetric(ruleExec_randomForest, X = dt_predictors, target = dt_train$finding) %>%
  pruneRule(., X = dt_predictors, target = dt_train$finding, maxDecay = 0.2)

learner_randomForest <- buildLearner(ruleMetric, X = dt_predictors, target = dt_train$finding, minFreq = 0.05)


confusion_to_gt <- function(x, xtitle) {
  as_tibble(x$table) %>%
    pivot_wider(names_from = `True Outcome`, values_from = n) %>%
    mutate(across(everything(), as.character)) %>%
    add_row(
      Prediction = "",
      Finding = glue("Sensitivity: {scales::percent(x$byClass['Sensitivity'])}"),
      `No Finding` = glue("Specificity: {scales::percent(x$byClass['Specificity'])}")) %>%
    gt(rowname_col = "Prediction") %>%
    tab_spanner(
      label = "True Outcome",
      columns = vars(`Finding`, `No Finding`)) %>%
    tab_stubhead(label = "Prediction") %>%
    tab_header(title = xtitle) 
}

# Confusion matrix for ensemble model
confusionMatrix_ensemble <- caret::confusionMatrix(
    data = factor(dt_randomForest_pred, 
                  levels = 1:0, labels = c("Finding", "No Finding")),
    reference = factor(dt_test$finding, 
                       levels = 1:0, labels = c("Finding", "No Finding")), 
    positive = "Finding", dnn = c("Prediction", "True Outcome"))


# Confusion matrix for simplified model
dt_simple_pred <- applyLearner(learner_randomForest, 
                              X = select(dt_test, -any_of(trim_vars)))

confusionMatrix_simple <- caret::confusionMatrix(
    data = factor(dt_simple_pred, 
                  levels = 1:0, labels = c("Finding", "No Finding")),
    reference = factor(dt_test$finding, 
                       levels = 1:0, labels = c("Finding", "No Finding")), 
    positive = "Finding", dnn = c("Prediction", "True Outcome"))

confusion_to_gt(confusionMatrix_ensemble, "Ensemble Model")

confusion_to_gt(confusionMatrix_simple, "Simplified Model")

```


The ensemble model performs poorly with a higher sensitivity and low specificity of `r scales::percent(confusionMatrix_ensemble$byClass['Sensitivity'])` and `r scales::percent(confusionMatrix_ensemble$byClass['Specificity'])`. As a result of the poorly performing ensemble model, the simplified model shows even worse performance with a sensitivity of `r scales::percent(confusionMatrix_simple$byClass['Sensitivity'])` and specificity of `r scales::percent(confusionMatrix_simple$byClass['Specificity'])`. Loss of accuracy is to be expected when flattening an ensemble model, which benefits from general consensus, to a single decision tree. Given the performance of the ensemble model, this loss of nuance leaves the model unable to identify cases in which there is a serious finding.

```{r pretty-learner}

prettyLearner_randomForest <- presentRules(learner_randomForest, colnames(dt_predictors)) %>%
  as_tibble() %>%
  mutate(
    across(-condition, ~ round(as.numeric(as.character(.x)), 2)),
    value = case_when(
      str_detect(condition, ">") ~ "Yes",
      str_detect(condition, "<") ~ "No",
      TRUE ~ ""),
    Predict = case_when(
      pred == 0 ~ "Unremarkable",
      pred == 1 ~ "Abnormal"
    ),
    var_name = str_remove(condition, "[><=].*")) %>%
  left_join(ha_var_labels, by = "var_name") %>%
  mutate(`Variable Name` = case_when(
    !is.na(var_mean) ~ var_mean,
    TRUE ~ str_to_sentence(var_name)
  )) %>%
  select(Depth = len, `Variable Name`, Value = value, Predict, Frequency = freq, Error = err) 

prettyLearner_randomForest %>%
  mutate(
       `Variable Name` = gsub("=(\\w)(\\w*)","=\\U\\1\\L\\2",`Variable Name`,perl=TRUE)) %>%
  gt() %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_style(
    style = cell_fill(color = "indianred1", alpha = 0.5),
    locations = cells_body(
      columns = vars(Value),
      rows = Value == "No")
  ) %>%
  tab_style(
    style = cell_fill(color = "darkolivegreen2", alpha = 0.5),
    locations = cells_body(
      columns = vars(Value),
      rows = Value == "Yes")
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = list(cells_body(
      columns = vars(Value, Predict)),
      cells_column_labels(
        columns = vars(Value, Predict)))
  )

```



Even though the simplified model has poor performance, it is to be expected when the model is tested with data that it was not intended for. Even so, the simplified model is relatively easy to understand. One can read the individual rules present and understand how the model classifies cases. This allows researchers to look for trends in data classification and to judge if an even simpler algorithm can be generated.


```{r ranger, eval = FALSE, echo = FALSE}

set.seed(12345)


Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

Ranger2List <- function(rf_ranger)
{
  formatRanger <- function(tree){
    rownames(tree) <- 1:nrow(tree)
    tree$`status` <- ifelse(tree$`terminal`==TRUE,-1,1)
    tree$`left daughter` <- tree$`leftChild` + 1
    tree$`right daughter` <- tree$`rightChild` + 1
    tree$`split var` <- tree$`splitvarID`
    tree$`split point` <- tree$`splitval`
    tree$`prediction` <- tree$`prediction`
    tree <- tree[,c("left daughter","right daughter","split var","split point","status")]
    tree <- data.matrix(as.data.frame(tree))
    return(tree)
  }
  treeList <- NULL
  treeList$ntree <- rf_ranger$num.trees
  treeList$list <- vector("list",rf_ranger$num.trees)
  for(i in 1:rf_ranger$num.trees){
    treeList$list[[i]] <- formatRanger( treeInfo(rf_ranger, tree = i) )
  }
  return(treeList)
}

dt_ranger_noweight <- ranger(x = dt_predictors, y = dt_train$finding, 
                          num.trees = num_randomforest_trees, importance = "impurity",
                          class.weights =  c(rf_normal_cutoff, 1-rf_normal_cutoff), min.node.size = 3,
                          seed = 12345, classification = TRUE)

dt_ranger_invweight <- ranger(x = dt_predictors, y = dt_train$finding, 
                          num.trees = num_randomforest_trees, importance = "impurity",
                          class.weights =  c(rf_normal_cutoff, 1-rf_normal_cutoff), min.node.size = 3,
                          case.weights = dt_train$propensity_weight,
                          seed = 12345, classification = TRUE)

dt_ranger_pred <- dt_test %>%
  select(-all_of(trim_vars)) %>%
  predict(dt_ranger_invweight, data = ., type = "response", seed = 12345)

table(Outcome = dt_test$finding, Predicted = dt_ranger_pred$predictions)

# treeList_ranger <- Ranger2List(dt_ranger_invweight)
# 
# debug_treeList_ranger <- list()
# debug_treeList_ranger$ntree <- 1
# debug_treeList_ranger$list[[1]] <- treeList_ranger$list[[1]]
# ruleExec_ranger <- extractRules(treeList_ranger,dt_predictors_noNA,digits=10) 
# rule_metric <- getRuleMetric(ruleExec_ranger[1,,drop=FALSE],dt_predictors_noNA,outcome_measured_noNA)

```


# Discussion

In this report, we created two predictive models to assess the risk of a serious finding on head imaging based on patient demographics and self-reported symptoms and history. The models failed to reach both a high sensitivity and specificity, with the ensemble randomForests model reaching a sensitivity of `r scales::percent(confusionMatrix_ensemble$byClass['Sensitivity'])` and specificity of `r scales::percent(confusionMatrix_ensemble$byClass['Specificity'])` and the simplified decision tree only `r scales::percent(confusionMatrix_simple$byClass['Sensitivity'])` and `r scales::percent(confusionMatrix_simple$byClass['Specificity'])`. To better its clinical context, we chose a suspicion threshold of `r scales::percent(rf_normal_cutoff)` for the randomForests model best performed at a suspicion threshold of `r scales::percent(rf_normal_cutoff)` which was the minimum consensus among trees needed to label a case as 'unremarkable'. However, the model performed poorly at any specified threshold between its default 50% and 99%, with sensitivity positively and specificity negatively associated with the cutoff value.

Although the models failed to reach the high standards needed for adoption into clinical use, there are several limitations that make future directions more promising. Ultimately, the models and associated analysis plan were designed for a dataset of emergency room encounters. Due to time constraints, we were forced to substitute in an already-available dataset of outpatient neurology visits but there were several key differences that likely contributed to the models' performance. For one, children with headache seen in the outpatient context are less likely to have a serious cause of headache due to the fast-progressing nature and severity of associated symptoms for some serious etiologies (i.e., intracranial hemorrhage due to trauma, bacterial meningitis). In addition, outpatient cases only have diagnostic codes given before or during a visit. As the vast majority of imaging was done weeks after a visit, we were unable to select a serious case based on an associated ICD code. As a result, we could only rely on documentation of the radiologist contacting the ordering physician as recorded within the radiology note. These limitations led to the selection of too few cases of serious findings on imaging (only `r dt %>% filter(finding == 1) %>% nrow(.)`). As a serious finding on imaging encompasses a varied range of etiologies and their associated symptom profiles, this small but heterogenous group of serious cases was likely too small for the models to identify distinguishing traits.  In addition, predictors for the models included a health questionnaire characterizing headache type and severity. Although rich, this set of predictors lacked information on physical exam findings noted by the physician which can be very informative. For instance, papilledema, or swelling of the optic disc, is often a sign of high intracranial pressure and can warrant more urgent investigation and intervention. However, it is only visible via an ophthalmoscope used by an experienced clinician. A patient and their family would be unable to note this finding without such a tool. A limitation outside of dataset choice is the possibility of selection bias. In order for a child to be labeled as a serious case, the physician must first choose to image them, which means that possibly there are some children who are undiscovered serious cases which could bias the predictions made.

As a result of limitations by the choice of dataset, we are still optimistic about their future performance. For one, data that poorly fits the given model is likely to lead to poor performance. Even so, the models do work, a sign that the code is at least syntactically correct. The ensemble model is able to properly classify some cases and performs better than at random given the high suspicion threshold chosen. In addition, although the simplified model is unable to detect serious cases, we are able to look into its determined methods and understand the choices it made. These signs give hope that both model will perform better when given its intended dataset. In addition, we plan to add an additional lasso regression model to examine which variables both randomForests and lasso regression models deem as important. Also, given the varied nature of headache etiologies, we expect there to be multiple symptom profiles matching to different groups of causes (i.e;, infectious, trauma, neoplasm). We plan to adopt several unsupervised methods such as K-means and hierarchical clustering to identify clinical profiles. In addition, we plan to address the possible imaging selection bias through inverse weighting. If we calculate the propensity score for children to be selected for imaging, then weighting cases by the inverse of their propensity score should, in theory, reduce selection bias within the dataset.

Although the given prediction models fail to distinguish between cases of unremarkable and urgent findings on imaging based on demographics and associated symptoms, these methods do remain promising. The code runs without errors and we are able to explore the models for general themes and trends. This means we may be able to create an algorithm based on our predictive models should they perform well with their intended data. Even if they are still unable to do so, this effort would inform future attempts on how to explore the high-dimensional space of health data using machine learning methods.

# References
