---
title: "BMIN503/EPID600 Project Template"
author: "Danielle Kellier"
bibliography: references.bib
output: 
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---

```{r install-packages, eval=FALSE}

install.packages("renv")
renv::restore()

```


```{r set-options, message=FALSE, cache=FALSE}
options(width = 400)
library(tidyverse)
library(gtsummary)
library(simstudy)
library(randomForest)
library(inTrees)
library(glue)
library(fabricatr)
library(mgcv)
library(ranger)
library(tidytext)

# Settings
refresh = F #When true, reads in raw CSV files and processes. When false, loads in already-processed feather files
nobs <- 4000 #num of observations included in dataset
missing_threshold <- 0.05 # level of missingness acceptable for an individual variable
test_to_train <- c(0.8, 0.2) #ratio of data used from training dataset compared to testing data set
num_randomforest_trees <- 500 # number of randomForest trees

source("load_data.R") #Loads in records from outpatient visits from a folder not included in git repository

Ranger2List <- function(rf_ranger)
{
  formatRanger <- function(tree){
    rownames(tree) <- 1:nrow(tree)
    tree$`status` <- ifelse(tree$`terminal`==TRUE,-1,1)
    tree$`left daughter` <- tree$`leftChild` + 1
    tree$`right daughter` <- tree$`rightChild` + 1
    tree$`split var` <- tree$`splitvarID`
    tree$`split point` <- tree$`splitval`
    tree$`prediction` <- tree$`prediction`
    tree <- tree[,c("left daughter","right daughter","split var","split point","status")]
    tree <- data.matrix(as.data.frame(tree))
    return(tree)
  }
  treeList <- NULL
  treeList$ntree <- rf_ranger$num.trees
  treeList$list <- vector("list",rf_ranger$num.trees)
  for(i in 1:rf_ranger$num.trees){
    treeList$list[[i]] <- formatRanger( treeInfo(rf_ranger, tree = i) )
  }
  return(treeList)
}

```  
***
Use this template to complete your project throughout the course. Your Final Project presentation will be based on the contents of this document. Replace the title/name above and text below with your own, but keep the headers.

1. Recall that you forked the [Final Project Repo](https://github.com/HimesGroup/BMIN503_Final_Project) and have downloaded it as a project to your local computer. Write the overview and introduction for your final project. The overview consists of 2-3 sentences summarizing the project and goals. For the introduction, the first paragraph describes the problem addressed, its significance, and some background to motivate the problem. In the second paragraph, explain why your problem is interdisciplinary, what fields can contribute to its understanding, and incorporate background related to what you learned from meeting with faculty/staff. Start working on the Methods/Results section, which consists of code and its output along with text describing what you are doing (Note: we will not check your code now, but you should have something in place before Assignment 6 is distributed).


### Overview
`Give a brief a description of your project and its goal(s), what data you are using to complete it, and what three faculty/staff in different fields you have spoken to about your project with a brief summary of what you learned from each person. Include a link to your final project GitHub repository.`

For children visiting the emergency department with a chief complaint of headache of migraine, are there features of their initial clinical presentation that can predict the presence of an intracranial abnormality? This study uses a retrospective cohort of children who visited the CHOP ED between 2012 and 2018 with the complaint of headache or migraine to predict risk of positive findings seen on imaging.

### Introduction 
`Describe the problem addressed, its significance, and some background to motivate the problem.`
`Explain why your problem is interdisciplinary, what fields can contribute to its understanding, and incorporate background related to what you learned from meeting with faculty/staff.`

Headache is a major cause of disability in children and a common chief complaint in the pediatric emergency room. Given the range of etiologies from primary to life-threatening, many children end up receiving imaging for ultimately benign which leads to excessive cost for both the patient and the healthcare system[@Cain2018; @Irwin2018; @Kan2000; @Young2018]. In addition, exposure to radiation from computed tomography scanning in children can greatly increase the risk of later malignancy[@Brenner2001; @Feng2010]. Using a nested case-control design within a larger cohort of ED visits for headache/migraine, we hope to establish whether there are particular features of a child's initial clinical presentation that can inform an emergency physicianâ€™s decision to order imaging for a child with headache.

Headache is consistently ranked as one of the top 10 chief complaints for an emergency room visit within the United States according to the National Hospital Ambulatory Medical Care Survey[@UnitedStatesDepartmentofHealthHumanServices-NationalCenterforHealthStatistics2019]. In fact, for over 1.7 million (5% overall) pediatric emergency visits in 2017, headache was listed as a reason for the visit[@UnitedStatesDepartmentofHealthHumanServices-NationalCenterforHealthStatistics2019]. Headache can be an alarming but highly nonspecific symptom in that it can be triggered by a disabling but ultimately benign primary disorder, a secondary but nonconcerning case of influenza, or act as the harbinger for a brain tumor or subarachnoid hemorrhage. Although a neurologist or headache subspecialist may focus on pinpointing a headache's etiology to direct treatment, an emergency room physician's goal is to look for "red flags" or signs of a serious etiology requiring urgent intervention. Still, a symptom's status as a "red flag" can be questionable: changes in vision or sensation can be associated with a stroke or simply be the aura of an incoming migraine. As a result, roughly 18-41% of children with headache end up receiving some form of neuroimaging of which only 4-10% had new abnormal findings seen on imaging[@Cain2018; @Kan2000; @Sheridan2013]. The high rate of children undergoing unnecessary imaging, especially the additional exposure risks of computed tomography, demonstrate a gap in the knowledge base that is made available to physicians within the emergency room who must deal with a high volume of patients diverse in their clinical presentations and acuity. This study aims to attempts to use machine learning techniques to select features of the clinical presentation that are informative to assessing the risk of an abnormal finding on imaging and to simplify the predictive model generated into an algorithm that can be disseminated and shared amongst emergency room physicians.

### Methods
Describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why. 


```{r simulate-data}
# 
# set.seed(12345)
# 
# image_types <- c("Imaged", "Not imaged")
# image_freq <- c(0.2, 0.8)
# outcome_types <- c("Abnormality", "No abnormality")
# outcome_freq <- c(0.05, 0.95)
# 
# nsymptoms <- 10
# symptom_corr <- genCorMat(nsymptoms)
# nobs <- 6000
# 
# dt <- fabricate(N = nobs,
#                 age = runif(N, min = 3, max = 17),
#                 sex = correlate(given = age, rho = 0.2, draw_binary, prob = 0.55),
#                 outcome_true = correlate(given = age, rho = -0.6, draw_binary, prob = outcome_freq[[1]]),
#                 image = correlate(given = outcome_true, rho = 0.8, draw_binary, prob = image_freq[[1]]),
#                 symptoms_mu = pmin(pmax(rnorm(N, mean = 0.05 + 0.2*image + 0.7*outcome_true - 0.05*age, 
#                                               sd = 0.05 + 0.05*image + 0.05*outcome_true),0),1))
# 
# rmat <- matrix(rnorm(nsymptoms^2),nsymptoms,nsymptoms)
# cov_mat <- rmat%*%t(rmat)
# corr_mat <- cov_mat/sqrt(diag(cov_mat)%*%t(diag(cov_mat)))
# 
# dt_symptoms <- cbind(genData(nobs, id = "ID"), "symptoms_mu" = dt$symptoms_mu) %>%
#   addCorGen(., nvars = nsymptoms, idvar = "ID", param1 = "symptoms_mu", dist = "binary", 
#             corMatrix = corr_mat, cnames = paste("symptom", 1:nsymptoms, sep = "_")) %>% 
#   dplyr::select(-ID, -symptoms_mu)
# 
# dt <- bind_cols(dt, dt_symptoms) %>% 
#   dplyr::select(-symptoms_mu) %>%
#   dplyr::mutate(outcome_measured = ifelse(image == 0, NA, outcome_true),
#                 sex = factor(sex, levels = 0:1, labels = c("Male", "Female")),
#                 image = factor(image, levels = 0:1, labels = rev(image_types)),
#                 across(starts_with("outcome"), ~factor(.x, levels = 0:1, labels = rev(outcome_types))))
# 
# dt_assignments <- sample(1:2, size = nobs, replace = T, prob = c(0.8, 0.2))

```


```{r}
# Pull in data from outpatient redcap database

# Select names of columns that end in ".factor"
demo_factor_names <- str_subset(names(demo_data),".factor") %>% 
  str_remove(., ".factor") 

# Select names of factors with only two levels
 #Can keep binary integer columns instead
ha_factor_names_binary <- ha_data %>% 
  select(matches("^p_.*\\.factor$")) %>%
  select_if(~ nlevels(.) == 2) %>%
  names(.)

# Select names of factors with more than two factors.
# Remove integer column counterparts
ha_factor_names_nonbinary <- ha_data %>% 
  select(matches("^p_.*\\.factor$")) %>%
  select_if(~ nlevels(.) != 2) %>%
  names(.) %>%
  str_remove(., ".factor")

# Select subset of demographic data based on number of observations
# Select patients that completed headache questionnaire
# Clean up columns and remove duplicates (keep factor counterparts)
dt_demo <- demo_data %>%
  filter(record_id <= nobs & pt_ha_quest_yn  == 1) %>%
  select(-all_of(demo_factor_names)) %>%
  rename_with(~str_remove(.x, ".factor"))

# Select subset of headache questionnaire data based on number of observations
# Clean up columns and remove duplicates (keep factor counterparts nonbinary, keep integer counterparts if binary)
dt_ha_data <- ha_data %>%
  filter(record_id <= nobs ) %>%
  select(-all_of(c("redcap_repeat_instrument", "redcap_repeat_instance"))) %>%
  select(-starts_with("c_")) %>%
  select(-all_of(ha_factor_names_binary)) %>%
  select(-all_of(ha_factor_names_nonbinary)) %>%
  select(record_id, p_age_first_ha:p_assoc_sx_pul_ear___oth, p_prob_preg_birth:p_fam_hist_med___oth, p_ha_in_lifetime.factor:p_preg_full_term.factor) %>%
  select_if(~is.factor(.) | is.numeric(.)) %>%
  rename_with(~str_remove(.x, ".factor"))


```


```{r}

# Select names of columns that end in ".factor"
image_factor_names <- str_subset(names(imaging_data),".factor") %>% 
  str_remove(., ".factor")

# Select subset of imaging data based on number of observations
# Select any imaging done on the first day (trying to exclude prior findings)
# Rough regex searches for key words to flag if impression mentioned a finding
dt_imaging <- imaging_data %>% 
  filter(record_id <= nobs & redcap_repeat_instrument == "imaging") %>%
  group_by(record_id) %>%
  filter(img_ord_dt == first(img_ord_dt)) %>% 
  mutate(finding = case_when( 
    str_detect(img_impression,
               regex("(?!(?:no|none)).*[\\w\\s]{0,15}(?:sequelae|infarct|tortuous|ectopi|heterotopi|prominen|anomalous)", 
                     ignore_case = TRUE)) ~ 1,
    str_detect(img_impression,
               regex("(?:chronic|prior).*infarct", 
                     ignore_case = TRUE)) ~ 1,
    str_detect(img_impression,
               regex("otherwise[\\s,]{0,3}(?:unremarkable|normal)", 
                     ignore_case = TRUE)) ~ 1,
    str_detect(img_impression,
               regex("volume loss", 
                     ignore_case = TRUE)) ~ 1,
    str_detect(img_impression,
               regex("(?!(?:otherwise)).*[\\s,]{0,3}(?:unremarkable|normal)",
                     ignore_case = TRUE)) ~ 0,
    str_detect(img_impression,
               regex("(?:nonspecific).*[\\s,]{0,3}finding", 
                     ignore_case = TRUE)) ~ 0,
    str_detect(img_impression, 
               regex("\\bno[\\w\\s]{0,20}(?:finding|abnormality|mass|hydrocephalus)", 
                     ignore_case = TRUE)) ~ 0,
    TRUE ~ NA_real_
  )) %>%
  summarise(
    finding = case_when(
      any(finding == 1) ~ 1,
      all(finding == 0) ~ 0,
      all(is.na(finding)) ~ NA_real_
    ),
    img_ord_dt = first(img_ord_dt),
    img_impression = first(img_impression),
    .groups = 'drop')


```


```{r}

# Merge datasets
# Mark if patient was imaged based on date of imaging 
# (not all kids imaged had conclusive findings)
dt <- dt_demo %>%
  select(record_id, age, gender, ethnicity, race) %>%
  left_join(dt_ha_data, by = "record_id") %>%
    left_join(dt_imaging, by = "record_id") %>%
  mutate(imaging = ifelse(!is.na(img_ord_dt), 1, 0))

# Select names of columns with missingness above a threshold
# Exclude columns from imaging dataset as not all kids were imaged
dt_missing <- dt %>%
  select(-any_of(names(dt_imaging))) %>%
  summarise(across(everything(), ~sum(is.na(.x))/length(.x))) %>%
  pivot_longer(everything(), names_to = "column", values_to = "prop") %>%
  filter(prop >= missing_threshold) %>%
  pull(column)

#Exclude variables with missingness above a certain variable
dt <- dt %>%
  select(-any_of(dt_missing))

```

# Methods

## Participants
We conducted a retrospective cohort study consisting of children ages 2-18 seen in the pediatric emergency department between 2012 and 2018 with the primary complaint of headache or migraine. This study took place within a tertiary pediatric hospital system in a urban setting with XX,XXX visits yearly. Due to its tertiary status, the pediatric emergency department often receives patient transfers from nearby hospitals for admission to specialized care. This can inflate the number of cases with rare and/or serious etiologies compared to another pediatric emergency care setting. We excluded children if they had their first headache-related visit prior to 2012, abnormal neurological findings at baseline, or documentation of prior abnormal findings on imaging. Children with a condition predisposing to an intracranial abnormality but no prior documented findings were included in the study. 

## Red Flag Findings
We extracted both symptom and physical finding information from provider notes documented during the emergency room visit of interest. Based on past work, we were particularly interested in documentation of fever and other constitutional symptoms, acute neurological abnormalities, papilledema and other ophthalmic symptoms and findings, age, descriptions of sudden onset or "thunderclap" headache, awakening from sleep, occipital location, and responsiveness to positional changes or the Valsalva maneuver [@Do2019]. We also included other variables often associated with headache assessment including age, family history of headache, blurry vision, nausea and vomiting, photophobia and phonophobia, abnormal blood pressure or recorded changes in blood pressure, headache frequency at baseline, and current headache duration. 

Given the nature of secondary data collection, we expected to encounter records with missing variables. We excluded any records that lacked a visit diagnosis commonly associated with headache such as minor trauma to extremities. Given the broad relationship between infection and headache, we included cases related to head and neck local infections such as sinusitis and dental infections, along with cases of systemic infection such as influenza and recorded sepsis. Further, we excluded records that lacked documentation of a neurological exam, headache characterization, or overall lacked data for >= XX% variables of interest. For analysis of a single variable, we excluded records with missing data if the variable's missingness in the dataset did not exceed 10% across age and sex strata. In cases where a variable's missingness rested between 10% and 20%, we used data imputation with the median for a continuous value by age and sex strata. Missingness that exceeded 20% was evaluated on a case-by-case basis to assess the value of and ramifications of analysis with investigation of nonrandom patterns.

Data analysis was done using `r R.version.string` [@RCoreTeam2020]. The dataset was divided into 3 parts for descriptive analyses and logistic regression modeling, creation of a predictive model for case classification, and testing of predictive model performance. The exploratory analyses on the first third of the dataset were completed using binomial logistic regression modeling provided by base R and visualization done using the *gtsummary* package and *tidyverse* package collection [@DanielDSjoberg2020; @Wickham2019]. For descriptive purposes, data was represented using the median and interquartile ranges for continuous variables and counts and proportions for categorical data. In order to create a model capable of predicting case severity by clinical presentation, the random forests method was used to design a set of decision trees based on randomly generated subsets of cases and predictors from the middle third of the dataset using the *randomForest* R package based on Leo Breiman's machine learning method [@AndyLiaw2002; @Breiman2001]. The ensemble predictive model was then simplified into a single decision tree using the *inTrees* package and tested against the final third of the dataset [@Deng2014].


### Results
Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.

```{r table-1}

#Table 1 to describe patient characteristics
dt %>%
  mutate(
    finding = factor(case_when(
      finding == 0 ~ "No abnormality",
      finding == 1 ~ "Abnormality",
      is.na(finding) & imaging == 1 ~ "Inconclusive findings",
      is.na(finding) & imaging == 0 ~ "Not imaged"
    ))) %>%
  select(age, gender, race, ethnicity, finding) %>%
  rename_with(stringr::str_to_title) %>%
  tbl_summary(by = Finding)


```

```{r propensity}

# Calculates propensity scores for imaging selection based on demographics and symptoms
prop_formula <- str_c("imaging ~ s(age) + ", paste0(names(select(dt, -record_id, -age, -finding:-imaging)), collapse = " + "))

propensity_mod <- gam(as.formula(prop_formula), 
                      family = binomial(), data = dt)
# Create curated dataset for machine learning
# Select kids with imaging and conclusive findings
# Select kids with no NAs
# Predict propensity score and inverse weight
dt_propensity <- dt %>%
  filter(imaging == 1 & !is.na(finding)) %>%
  modelr::add_predictions(propensity_mod, 
                          var = "propensity_score", type = "response") %>%
  filter(complete.cases(.)) %>%
  mutate(propensity_weight = (1 /(propensity_score)),
         finding = factor(finding))


```


```{r}

set.seed(12345)

# Create random assignments for test and training datasets
dt_assignments <- sample(1:2, size = nrow(dt_propensity), 
                         replace = T, prob = test_to_train)

dt_train <- filter(dt_propensity, dt_assignments == 1) 

dt_test <- filter(dt_propensity, dt_assignments == 2) 

#Select variables not to include in machine learning 
trim_vars <- dt_propensity %>%
  select(record_id, finding:imaging, starts_with("propensity")) %>%
  names(.) 

#Convert predictor variables into a matrix
dt_predictors <- select(dt_train, -all_of(trim_vars)) %>% data.matrix()


```


```{r randomForest-cutoffs}

pred_normal_cutoff_seq <- seq(0.5,0.99, 0.05)
cutoff_performance <- tibble()

for (curr_cutoff in pred_normal_cutoff_seq){
  
  curr_rf <- randomForest(
    x = dt_predictors, y = dt_train$finding, ntree = num_randomforest_trees,
    importance = TRUE, cutoff = c(curr_cutoff, 1-curr_cutoff), nodesize = 3
  )
  
  curr_pred <- dt_test %>%
    select(-all_of(trim_vars)) %>%
    data.matrix() %>%
    predict(curr_rf, ., type = "response")
  
  curr_perform <- tibble(cutoff = curr_cutoff,
            true = dt_test$finding, 
            pred = curr_pred) %>%
    group_by(cutoff, true, pred) %>%
    summarise(n = n(), .groups = 'drop')
  
  cutoff_performance <- bind_rows(cutoff_performance, curr_perform)
}

cutoff_performance %>%
  mutate(outcome = case_when(
    true == 1 & pred == 1 ~ "TP",
    true == 1 & pred == 0 ~ "FN",
    true == 0 & pred == 0 ~ "TN",
    true == 0 & pred == 1 ~ "FP",
  )) %>%
  pivot_wider(id_cols = cutoff, names_from = outcome, 
              values_from = n, values_fill = 0) %>%
  mutate(Sensitivity = TP/(TP+FN),
         Specificity = TN/(TN+FP)) %>%
  select(cutoff, Sensitivity, Specificity) %>%
  pivot_longer(-cutoff, names_to = "measure", values_to = "score") %>%
  ggplot(aes(x = cutoff, y = score, group = measure, color = measure)) +
  geom_line() +
  scale_y_continuous(name = "Proportion", limits = c(0,1), breaks = seq(0,1,0.25)) +
  scale_x_continuous(name = "Cutoff for predicting 'no finding'", limits = c(0.5, 1), breaks = seq(0.5, 1, 0.1)) +
  ggtitle("Random Forest Performance\nwith varying cutoffs") +
  cowplot::theme_half_open() +
  theme(legend.title = element_blank())

```

```{r randomForest}

set.seed(12345)

rf_normal_cutoff <- 0.85
  

dt_randomForest <- randomForest(
  x = dt_predictors, y = dt_train$finding, ntree = num_randomforest_trees,
  importance = TRUE, cutoff = c(rf_normal_cutoff, 1-rf_normal_cutoff), nodesize = 3
  )

as_tibble(dt_randomForest$importance, rownames = "Variable Name") %>%
  arrange(desc(MeanDecreaseAccuracy)) %>%
  select(`Variable Name`, starts_with("Mean")) %>%
  filter(row_number() <= 20)

dt_randomForest_pred <- dt_test %>%
  select(-all_of(trim_vars)) %>%
  data.matrix() %>%
  predict(dt_randomForest, ., type = "response")

table(Outcome = dt_test$finding, Predicted = dt_randomForest_pred)

treeList_randomForest <- RF2List(dt_randomForest)
ruleExec_randomForest <- extractRules(treeList_randomForest, X = dt_predictors, digits = 2)

ruleMetric <- getRuleMetric(ruleExec_randomForest, X = dt_predictors, target = dt_train$finding) %>%
  pruneRule(., X = dt_predictors, target = dt_train$finding, maxDecay = 0.2)

learner_randomForest <- buildLearner(ruleMetric, X = dt_predictors, target = dt_train$finding, minFreq = 0.05)

prettyLearner_randomForest <- presentRules(learner_randomForest, colnames(dt_predictors))

bind_cols(true = dt_test$finding, 
          pred = applyLearner(learner_randomForest, 
                              X = select(dt_test, -any_of(trim_vars)))) %>%
  group_by(true, pred) %>%
  summarise(n = n())


```


```{r ranger, eval = FALSE}

set.seed(12345)

dt_ranger_noweight <- ranger(x = dt_predictors_noNA, y = outcome_measured_noNA, 
                          num.trees = num_randomforest_trees, importance = "impurity",
                          class.weights = c(0.95, 0.05), min.node.size = 3,
                          seed = 12345, classification = TRUE)

dt_ranger_invweight <- ranger(x = dt_predictors_noNA, y = outcome_measured_noNA, 
                          num.trees = num_randomforest_trees, importance = "impurity",
                          class.weights = c(0.95, 0.05), min.node.size = 3,
                          case.weights = propensity_weight_noNA,
                          seed = 12345, classification = TRUE)

dt_ranger_pred <- dt_test %>%
  select(-all_of(trim_vars)) %>%
  data.matrix() %>%
  predict(dt_ranger_invweight, data = ., type = "response", seed = 12345)

table(Outcome = dt_test$outcome_true, Predicted = dt_ranger_pred$predictions)

treeList_ranger <- Ranger2List(dt_ranger_invweight)

debug_treeList_ranger <- list()
debug_treeList_ranger$ntree <- 1
debug_treeList_ranger$list[[1]] <- treeList_ranger$list[[1]]
ruleExec_ranger <- extractRules(treeList_ranger,dt_predictors_noNA,digits=10) 
rule_metric <- getRuleMetric(ruleExec_ranger[1,,drop=FALSE],dt_predictors_noNA,outcome_measured_noNA)

```

# References
