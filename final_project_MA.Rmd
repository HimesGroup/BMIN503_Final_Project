---
title: "BMIN503/EPID600 Car Crashes in Philadelphia in 2008 and 2018"
author: "Nuoying MA"
output: 
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  
***
Use this template to complete your project throughout the course. Your Final Project presentation in class will be based on the contents of this document. Replace the title/name and text below with your own, but keep the headers.

### Overview
In this section, give a brief a description of your project and its goal, what data you are using to complete it, and what three faculty/staff in different fields you have spoken to about your project with a brief summary of what you learned from each person. Include a link to your final project GitHub repository.

The goal is to examine car crash data in Philadelphia in 2008 and 2018, respectively. This project will first obtain and compare the critical factors contributing the fatal car crashes in 2008 and 2018 using machine learning methods. Then, the important factors contributing to the deadly car crashes will be plotted. 


### Introduction 
In the first paragraph, describe the problem addressed, its significance, and some background to motivate the problem.

This project utilizes the data provided by the Philadelphia government for all car crashes in 2008 and 2018. Car crashes can be fatal and detrimental to the families of victims. By finding the most critical factors contributing to fatal car crashes, this project could provide a warning and raise the caution to drivers and pedestrians of those factors, and potentially reduce the fatal rate of accidents. 

In the second paragraph, explain why your problem is interdisciplinary, what fields can contribute to its understanding, and incorporate background related to what you learned from meeting with faculty/staff.

Since the car crash data also contains information on observed dangerous driving behaviors, including alcohol consumption, speeding, this data would be useful to study the effect of human psychology and health status on the severity of accidents. 


### Methods
In the first paragraph, describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why. 

```{r, eval = TRUE}
getwd()
library(ggplot2)
library(dplyr)
flag.2018 <- read.csv("Philadelphia_FLAG_2018.csv", header = TRUE) #load 2018 flag data
crash.2018 <- read.csv("CRASH_2018_Philadelphia.csv", header = TRUE) #load 2018 flag data

crash.2008<-read.csv("CRASH_2008_Philadelphia.csv", header=TRUE)
flag.2008<-read.csv("FLAG_2008_Philadelphia.csv", header=TRUE) #load 2008 flag data, look for most important factors contributing to fatal car crashes.
crash.08 <- crash.2008 %>%
             select(POLICE_AGCY, ARRIVAL_TM, DISPATCH_TM, CRN, CRASH_YEAR,DAY_OF_WEEK, HOUR_OF_DAY,FATAL_COUNT, INJURY_COUNT, PERSON_COUNT, ROAD_CONDITION, ILLUMINATION, WEATHER, COLLISION_TYPE, RELATION_TO_ROAD, TCD_FUNC_CD)

#We would like to find out important factors contributing to fatal/major injury containing crashes.
flagu.18<-flag.2018 %>% 
  select_if(~n_distinct(.) > 1)#We first need to exclude columns with only 1 value (otherwise cause error in logistic regression)
colnames(flagu.18)
ncol(flagu.18) #184

df1<-as.vector(colnames(flag.2008))
df1 #95 variables
df2<-as.vector(colnames(flagu.18))
df2 #184 variables
df3<-df2[df2%in%df1]#df2 include 87 variables in df1
df1[!df1%in%df3]#lack"LOCAL_ROAD","TURNPIKE","DRIVER_16YR","DRIVER_50_64YR","CRASH_YEAR","C         OUNTY","COUNTY_YEAR","MUNICIPALITY" 
df2[!df2%in%df1] #df2 has 97 more variables than df1, among them some are worthy to be examined, includes "CRASH_MONTH", "DAY_OF_WEEK","TIME_OF_DAY","HOUR_OF_DAY","WEATHER", "COLLISION_TYPE" , "TCD_TYPE", "LATITUDE", "LONGITUDE",                                           
flag.2008$VEHICLE_TOWED<-NULL#this is a variable happens after crashes
flag.2008$COUNTY_YEAR<-NULL

#Look at the distribution of crashes among different situations, 2008 situations
ggplot(data = crash.08, aes(x = factor(DAY_OF_WEEK)))+
  geom_bar(color="darkblue", fill= "darkblue")+
  labs(x = "day of week when crash happens 2008")+
  ggsave("dayofweek08.tiff")
#More crashes happens on Saturday.

crash.08 %>% 
    mutate(HOUR_OF_DAY = ifelse(HOUR_OF_DAY %in% c(99), NA,factor(HOUR_OF_DAY))) %>% 
    ggplot(aes(x = HOUR_OF_DAY))+
    geom_bar()+
    labs(x = "hour of the day when crash happens 2008")+
    ggsave("hourofday08.tiff")
# Peak hour of crashes: from 8-10AM in the morning, and from 5-7PM in the afternoon. Much more likely to happen in the afternoon.

crash.08 %>% 
    ggplot(aes(x = factor(ROAD_CONDITION)))+
    geom_bar()+
    labs(x = "Road condition 2008")+
    ggsave("Roadcondition08.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 

crash.08 %>% 
    ggplot(aes(x = factor(ILLUMINATION)))+
    geom_bar()+
    labs(x = "illumination 2008")+
    ggsave("illumination08.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 

crash.08 %>% 
    ggplot(aes(x = factor(WEATHER)))+
    geom_bar()+
    labs(x = "weather 2008")+
    ggsave("weather08.tiff")# Mostly no adverse condition, second: rain

crash.08 %>% 
    ggplot(aes(x = factor(COLLISION_TYPE)))+
    geom_bar()+
    labs(x = "Collison type 2008")+
    ggsave("Collisontype08.tiff") #Mostly 
crash.08 %>% 
  group_by(COLLISION_TYPE)%>% 
  count(COLLISION_TYPE)

crash.2008 %>% 
    ggplot(aes(x = factor(RELATION_TO_ROAD)))+
    geom_bar()+
    labs(x = "Relation to road 2008")+
    ggsave("Relation to road 08.tiff") #Mostly on roadway.

crash.2008 %>% 
    ggplot(aes(x = factor(TCD_FUNC_CD)))+
    geom_bar()+
    labs(x = "TCD function 2008")+
    ggsave("TCD08.tiff") #About half: with no controls, another half: device functioning properly.
crash.2008 %>% 
  group_by(TCD_FUNC_CD)%>% 
  count(TCD_FUNC_CD)#About 4766/10676=0.45%: with no controls, another 5222/10676=0.49%: device functioning properly.
nrow(crash.2008) #10676 accidents.

#time difference from police dispatched to police arrived //IS it related to police agency?
crash.2008%>% 
    mutate(arrival=DISPATCH_TM-ARRIVAL_TM)%>% 
    ggplot(aes(x = arrival))+
    geom_histogram()+
    labs(x = "Time taken for police to arrive at scene")+
    ggsave("Time08.tiff")

#Look at the distribution of crashes among different situations, 2018 situations
library(ggplot2) 
fig1<-ggplot(data = flag.2018, aes(x = factor(DAY_OF_WEEK)))+
  geom_bar(color="darkblue", fill= "darkblue")+
  labs(x = "day of week when crash happens 2018")
fig1#summarise by day of week
#no significant difference in number of car crashes among different day of week.
ggsave("dayofweek18.tiff",plot=fig1)

library(dplyr)
flag.2018 %>% 
    mutate(HOUR_OF_DAY = ifelse(HOUR_OF_DAY %in% c(99), NA,factor(HOUR_OF_DAY))) %>% 
    ggplot(aes(x = HOUR_OF_DAY))+
    geom_bar()+
    labs(x = "hour of the day when crash happens 2018")+
    ggsave("hourofday18.tiff")
# Peak hour of crashes: from 8-9AM in the morning, and from 4-8PM in the afternoon.

flag.2018 %>% 
    ggplot(aes(x = ROAD_CONDITION))+
    geom_bar()+
    labs(x = "Road condition 2018")+
    ggsave("Roadcondition18.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 

flag.2018 %>% 
    ggplot(aes(x = ILLUMINATION))+
    geom_bar()+
    labs(x = "illumination 2018")+
    ggsave("illumination18.tiff") #Mostly happens on daylight, and dark with street lights are the second common. 

flag.2018 %>% 
    ggplot(aes(x = factor(COLLISION_TYPE)))+
    geom_bar()+
    labs(x = "Collison type 2018")+
    ggsave("Collisontype18.tiff") #First likely at an angle, second likely: rear-end
flag.2018 %>% 
  group_by(COLLISION_TYPE)%>% 
  count(COLLISION_TYPE)
  

crash.2018 %>% 
    ggplot(aes(x = factor(RELATION_TO_ROAD)))+
    geom_bar()+
    labs(x = "Relation to road 2018")+
    ggsave("Relation to road 18.tiff") #Mostly on roadway.
flag.2018 %>% 
    ggplot(aes(x = factor(TCD_FUNC_CD)))+
    geom_bar()+
    labs(x = "TCD function 2018")+
    ggsave("TCD18.tiff")
flag.2018 %>% 
  group_by(TCD_FUNC_CD)%>% 
  count(TCD_FUNC_CD)#About 4878/11003=0.44%: with no controls, another 5998/11003=0.55%: device functioning properly.
nrow(flag.2018) #11003 accidents.

```
 
```{r, eval = TRUE}
colnames(flag.2008)
# To see how many car accidents include "MAJOR_INJURY", "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL","FATAL_OR_MAJ_INJ","INJURY" 
nrow(flag.2008) # 10676 car accidents in total
damage.08 <- flag.2008 %>%
             select(CRN,MAJOR_INJURY,MODERATE_INJURY, MINOR_INJURY, FATAL, INJURY,FATAL_OR_MAJ_INJ) 
fig8<-ggplot(data = damage.08,aes(x = factor(INJURY)))+
    geom_bar()+
    labs(x = "Injury") # plot shows accidents cateogrised into no injury vs injury.
ggsave("injury08.tiff",plot=fig8)
damage.08%>%
  filter(INJURY==1)%>%
  count(INJURY) # 8234 injuries
injury.08<-damage.08%>%
  filter(INJURY==1)
damage.08%>%
  filter(MODERATE_INJURY==1)%>%
  count(MODERATE_INJURY) # 1231 moderate injuries
damage.08%>%
  filter(MAJOR_INJURY==1)%>%
  count(MAJOR_INJURY) # 254 major injuries
damage.08%>%
  filter(MINOR_INJURY==1)%>%
  count(MINOR_INJURY) # 4352 minor injuries
damage.08%>%
  filter(FATAL==1)%>%
  count(FATAL) # 4352 minor injuries
injury.08%>%
  filter(FATAL_OR_MAJ_INJ==1)%>%
  count(FATAL_OR_MAJ_INJ)#280 injury containing accidents involve major injuries or fatal events.
ggplot(data=injury.08, aes(factor(FATAL_OR_MAJ_INJ==1)))+
    geom_bar()+
    labs(x = "Fatal or major injury accidents")+ # plot shows accidents involving injury cateogrised into containing fatal or not.
    ggsave("fatalormajor08.tiff")
class(flag.2008$FATAL)
#This excel sheet contains 4 separate column, driver age=16,17,18,19,20, I would like to combine them to be 1 column, and remove the original columns
flag.2008$Driveryoung<-flag.2008$DRIVER_16YR+flag.2008$DRIVER_17YR+flag.2008$DRIVER_18YR+flag.2008$DRIVER_19YR+flag.2008$DRIVER_20YR
flag.2008$Driveryoung<-ifelse(flag.2008$Driveryoung==0,0,1)
flag.2008$DRIVER_16YR<-NULL
flag.2008$DRIVER_17YR<-NULL
flag.2008$DRIVER_18YR<-NULL
flag.2008$DRIVER_19YR<-NULL
flag.2008$DRIVER_20YR<-NULL
colnames(flag.2008)
test <- flag.2008 %>% 
  select(CRN,Driveryoung)
  class(test$Driveryoung)# test the values of Driveryoung
#We would like to focus on fatal or major injuries crashes, so exclue other conditions, like "MAJOR_INJURY"       "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL", "FATAL","INJURY" 
flag.2008$MAJOR_INJURY<-NULL
flag.2008$MODERATE_INJURY<-NULL
flag.2008$MINOR_INJURY<-NULL
flag.2008$PROPERTY_DAMAGE_ONLY<-NULL
flag.2008$INJURY_OR_FATAL<-NULL
flag.2008$FATAL<-NULL
flag.2008$INJURY<-NULL
colnames(flag.2008)

#Remove highly correlated data
install.packages('lattice')
install.packages('caret')
library('caret')
flag.2008.c = cor(flag.2008)
hc = findCorrelation(flag.2008.c, cutoff=0.9) # putt any value as a "cutoff" 
hc = sort(hc)
reduced_Data = df1[,-c(hc)]
print (reduced_Data)

findCorrelation(flag.2008, cutoff = 0.9, verbose = FALSE, names = FALSE,
  exact = FALSE)


#Run random forest or multiple linear regression the flag dataset, to find out the important factors contributing to fatal/major injury accidents.
#Convert 0,1 to yes and no
flag.2008.r<-flag.2008
flag.2008.r$FATAL_OR_MAJ_INJ<-factor(flag.2008$FATAL_OR_MAJ_INJ, levels=c(0,1), labels = c("fatal/major injuries","no"))
class(flag.2008.r$FATAL_OR_MAJ_INJ)

library(randomForest) 
flag08.rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flag.2008.r, ntree = 100, importance = TRUE) 
flag08.rf 
flag08.rf$importance
#OOB=3.04%, CRN has the largest MeanDecreaseGini,need to remove CRN
flag.2008.r$CRN<-NULL
flag08.rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flag.2008.r, ntree = 100, importance = TRUE) 
flag08.rf 
flag08.rf$importance
rf.pred.08 <- predict(flag08.rf , flag.2008.r, type = "prob")
head(rf.pred.08)
rf.pred.no.08<- rf.pred.08[ ,2]
#OOB=3.05%, most important factors are MOTORCYCLE, PEDESTRIAN, VEHICLE_TOWED, UNBELTED, ILLUMINATION_DARK, DRIVER_50_64YR, WET_ROAD, AGGRESSIVE_DRIVING, MC_DRINKING_DRIVER, STATE_ROAD, Driveryoung, SPEEDING, SV_RUN_OFF_RD, LOCAL_ROAD

varImpPlot(flag08.rf, type=2)#Plot of import variables vs MeanDecreaseGini scores

imp08 <- flag08.rf $importance
rf08.im<-as.data.frame(head(sort(imp08[ , 4], decreasing = TRUE), n = 83))
rf08.im#Dataframe of import variables vs MeanDecreaseGini scores

#Create a logistic regression model, find variables are significant at p < 0.05.
library(dplyr) #We first need to exclude columns with only 1 value (otherwise cause error in logistic regression)
flagu.08<-flag.2008 %>% 
  select_if(~n_distinct(.) > 1)
colnames(flagu.08)
ncol(flagu.08) #79
ncol(flag.2008)#84, there are 5 variables with the same values.

flag08.glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = flagu.08, family = binomial())
summary(flag08.glm) # statistical relationship, CRN is also significant, should exclue that.
flagu.08$CRN<-NULL
flag08.glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = flagu.08, family = binomial())
summary(flag08.glm) 
lr08<-data.frame(summary(flag08.glm)$coef[summary(flag08.glm)$coef[,4]<=0.05, 4])#most important factors are MOTORCYCLE, PEDESTRIAN, BICYCLE, CROSS_MEDIAN, HIT_TREE_SHRUB,ILLUMINATION_DARK,RUNNING_RED_LT,HIT_POLE,COMM_VEHICLE, VEHICLE_TOWED, UNBELTED, SPEEDING, MC_DRINKING_DRIVER,SHLDR_RELATED, OVERTURNED
glm.pred.08 <- predict(flag08.glm, flagu.08, type = "response") 



class(test$Driveryoung)
crash.08 <- crash.2008 %>%
             select(CRN, CRASH_YEAR,DAY_OF_WEEK, HOUR_OF_DAY, FATAL_COUNT, INJURY_COUNT, PERSON_COUNT)
```

Create 10-fold cross validation classification vectors for each model. Obtain AUC values and make an ROC plot that shows ROC curves corresponding to predictive accuracy using the training data as well as the 10-fold cross-validations.

```{r, eval = TRUE} 
#K-fold cross validation for random forest 2008
N = nrow(flag.2008.r) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.rf.08 <- vector(mode = "numeric", length = N)
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flag.2008.r, s != i)
	test <- filter(flag.2008.r, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #RF train/test
	rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = train, ntree = 100)
	rf.pred.curr <- predict(rf, newdata = test, type = "prob") 
	pred.outputs.rf.08[1:length(s[s == i]) + offset] <- rf.pred.curr[ , 2]
  
  offset <- offset + length(s[s == i])
}

library(pROC) 
auc(roc(flag.2008.r$FATAL_OR_MAJ_INJ, rf.pred.no.08))#AUC=0.8665
auc(roc(obs.outputs, pred.outputs.rf.08))#AUC=0.673

plot.roc(flag.2008.r$FATAL_OR_MAJ_INJ, rf.pred.no.08, col = "black") #random forest
plot.roc(obs.outputs, pred.outputs.rf.08, ci = TRUE, col = "blue", add = TRUE)
legend("bottomright", legend = c("2008 RF Training", "2008 RF Cross-Validation"), col = c("black", "blue"), lwd = 1)

#K-fold cross validation for logistic regression 2008

N = nrow(flagu.08) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.glm.08 <- vector(mode = "numeric", length = N) 
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flagu.08, s != i)
	test <- filter(flagu.08, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #GLM train/test
	glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = train, family = binomial(logit))
  glm.pred.curr <- predict(glm, test, type = "response")
  pred.outputs.glm.08[1:length(s[s == i]) + offset] <- glm.pred.curr
  
  offset <- offset + length(s[s == i])
}

library(pROC) 
auc(roc(flagu.08$FATAL_OR_MAJ_INJ, glm.pred.08))#AUC=0.8
auc(roc(obs.outputs, pred.outputs.glm.08))#AUC=0.7548

plot.roc(flagu.08$FATAL_OR_MAJ_INJ, glm.pred.08,col = c("black"))
plot.roc(obs.outputs, pred.outputs.glm.08, col = "red", add = TRUE)
legend("bottomright", legend = c("2008 logistic regression Training", "2008 logistic regression Cross-Validation"), col = c("black", "red"), lwd = 1)
# Turns out the logistic regression is a better model of predicting fatal or major injury containing crashes in 2008. The common factors related to fatal or major injury containing crashes predicted by both rf and logistic regression are....


```


```{r, eval = TRUE} 
colnames(flag.2018)
# To see how many car accidents include "MAJOR_INJURY", "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL","FATAL_OR_MAJ_INJ","INJURY" 
nrow(flag.2008) # 10676 car accidents in total
damage.08 <- flag.2008 %>%
             select(CRN,MAJOR_INJURY,MODERATE_INJURY, MINOR_INJURY, FATAL, INJURY,FATAL_OR_MAJ_INJ) 
fig8<-ggplot(data = damage.08,aes(x = factor(INJURY)))+
    geom_bar()+
    labs(x = "Injury") # plot shows accidents cateogrised into no injury vs injury.
ggsave("injury08.tiff",plot=fig8)
damage.08%>%
  filter(INJURY==1)%>%
  count(INJURY) # 8234 injuries
injury.08<-damage.08%>%
  filter(INJURY==1)
damage.08%>%
  filter(MODERATE_INJURY==1)%>%
  count(MODERATE_INJURY) # 1231 moderate injuries
damage.08%>%
  filter(MAJOR_INJURY==1)%>%
  count(MAJOR_INJURY) # 254 major injuries
damage.08%>%
  filter(MINOR_INJURY==1)%>%
  count(MINOR_INJURY) # 4352 minor injuries
damage.08%>%
  filter(FATAL==1)%>%
  count(FATAL) # 4352 minor injuries
injury.08%>%
  filter(FATAL_OR_MAJ_INJ==1)%>%
  count(FATAL_OR_MAJ_INJ)#280 injury containing accidents involve major injuries or fatal events.
ggplot(data=injury.08, aes(factor(FATAL_OR_MAJ_INJ==1)))+
    geom_bar()+
    labs(x = "Fatal or major injury accidents")+ # plot shows accidents involving injury cateogrised into containing fatal or not.
    ggsave("fatalormajor08.tiff")
class(flag.2008$FATAL)
#This excel sheet contains 4 separate column, driver age=16,17,18,19,20, I would like to combine them to be 1 column, and remove the original columns
flag.2008$Driveryoung<-flag.2008$DRIVER_16YR+flag.2008$DRIVER_17YR+flag.2008$DRIVER_18YR+flag.2008$DRIVER_19YR+flag.2008$DRIVER_20YR
flag.2008$Driveryoung<-ifelse(flag.2008$Driveryoung==0,0,1)
flag.2008$DRIVER_16YR<-NULL
flag.2008$DRIVER_17YR<-NULL
flag.2008$DRIVER_18YR<-NULL
flag.2008$DRIVER_19YR<-NULL
flag.2008$DRIVER_20YR<-NULL
colnames(flag.2008)
test <- flag.2008 %>% 
  select(CRN,Driveryoung)
  class(test$Driveryoung)# test the values of Driveryoung
#We would like to focus on fatal or major injuries crashes, so exclue other conditions, like "MAJOR_INJURY"       "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL", "FATAL","INJURY" 
flag.2008$MAJOR_INJURY<-NULL
flag.2008$MODERATE_INJURY<-NULL
flag.2008$MINOR_INJURY<-NULL
flag.2008$PROPERTY_DAMAGE_ONLY<-NULL
flag.2008$INJURY_OR_FATAL<-NULL
flag.2008$FATAL<-NULL
flag.2008$INJURY<-NULL
colnames(flag.2008)


#Run random forest or multiple linear regression the flag dataset, to find out the important factors contributing to fatal/major injury accidents.
#Convert 0,1 to yes and no
flag.2008.r<-flag.2008
flag.2008.r$FATAL_OR_MAJ_INJ<-factor(flag.2008$FATAL_OR_MAJ_INJ, levels=c(0,1), labels = c("fatal/major injuries","no"))
class(flag.2008.r$FATAL_OR_MAJ_INJ)

library(randomForest) 
flag08.rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flag.2008.r, ntree = 100, importance = TRUE) 
flag08.rf 
flag08.rf$importance
#OOB=3.04%, CRN has the largest MeanDecreaseGini,need to remove CRN
flag.2008.r$CRN<-NULL
flag08.rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flag.2008.r, ntree = 100, importance = TRUE) 
flag08.rf 
flag08.rf$importance
rf.pred.08 <- predict(flag08.rf , flag.2008.r, type = "prob")
head(rf.pred.08)
rf.pred.no.08<- rf.pred.08[ ,2]
#OOB=3.05%, most important factors are MOTORCYCLE, PEDESTRIAN, VEHICLE_TOWED, UNBELTED, ILLUMINATION_DARK, DRIVER_50_64YR, WET_ROAD, AGGRESSIVE_DRIVING, MC_DRINKING_DRIVER, STATE_ROAD, Driveryoung, SPEEDING, SV_RUN_OFF_RD, LOCAL_ROAD

varImpPlot(flag08.rf, type=2)#Plot of import variables vs MeanDecreaseGini scores

imp08 <- flag08.rf $importance
rf08.im<-as.data.frame(head(sort(imp08[ , 4], decreasing = TRUE), n = 83))
rf08.im#Dataframe of import variables vs MeanDecreaseGini scores

#Create a logistic regression model, find variables are significant at p < 0.05.
library(dplyr) #We first need to exclude columns with only 1 value (otherwise cause error in logistic regression)
flagu.08<-flag.2008 %>% 
  select_if(~n_distinct(.) > 1)
colnames(flagu.08)
ncol(flagu.08) #79
ncol(flag.2008)#84, there are 5 variables with the same values.

flag08.glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = flagu.08, family = binomial())
summary(flag08.glm) # statistical relationship, CRN is also significant, should exclue that.
flagu.08$CRN<-NULL
flag08.glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = flagu.08, family = binomial())
summary(flag08.glm) 
lr08<-data.frame(summary(flag08.glm)$coef[summary(flag08.glm)$coef[,4]<=0.05, 4])#most important factors are MOTORCYCLE, PEDESTRIAN, BICYCLE, CROSS_MEDIAN, HIT_TREE_SHRUB,ILLUMINATION_DARK,RUNNING_RED_LT,HIT_POLE,COMM_VEHICLE, VEHICLE_TOWED, UNBELTED, SPEEDING, MC_DRINKING_DRIVER,SHLDR_RELATED, OVERTURNED
glm.pred.08 <- predict(flag08.glm, flagu.08, type = "response") 



class(test$Driveryoung)
crash.08 <- crash.2008 %>%
             select(CRN, CRASH_YEAR,DAY_OF_WEEK, HOUR_OF_DAY, FATAL_COUNT, INJURY_COUNT, PERSON_COUNT)
```

Create 10-fold cross validation classification vectors for each model. Obtain AUC values and make an ROC plot that shows ROC curves corresponding to predictive accuracy using the training data as well as the 10-fold cross-validations.

```{r, eval = TRUE} 
#K-fold cross validation for random forest 2008
N = nrow(flag.2008.r) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.rf.08 <- vector(mode = "numeric", length = N)
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flag.2008.r, s != i)
	test <- filter(flag.2008.r, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #RF train/test
	rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = train, ntree = 100)
	rf.pred.curr <- predict(rf, newdata = test, type = "prob") 
	pred.outputs.rf.08[1:length(s[s == i]) + offset] <- rf.pred.curr[ , 2]
  
  offset <- offset + length(s[s == i])
}

library(pROC) 
auc(roc(flag.2008.r$FATAL_OR_MAJ_INJ, rf.pred.no.08))#AUC=0.8665
auc(roc(obs.outputs, pred.outputs.rf.08))#AUC=0.673

plot.roc(flag.2008.r$FATAL_OR_MAJ_INJ, rf.pred.no.08, col = "black") #random forest
plot.roc(obs.outputs, pred.outputs.rf.08, ci = TRUE, col = "blue", add = TRUE)
legend("bottomright", legend = c("2008 RF Training", "2008 RF Cross-Validation"), col = c("black", "blue"), lwd = 1)

#K-fold cross validation for logistic regression 2008

N = nrow(flagu.08) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.glm.08 <- vector(mode = "numeric", length = N) 
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flagu.08, s != i)
	test <- filter(flagu.08, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #GLM train/test
	glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = train, family = binomial(logit))
  glm.pred.curr <- predict(glm, test, type = "response")
  pred.outputs.glm.08[1:length(s[s == i]) + offset] <- glm.pred.curr
  
  offset <- offset + length(s[s == i])
}

library(pROC) 
auc(roc(flagu.08$FATAL_OR_MAJ_INJ, glm.pred.08))#AUC=0.8
auc(roc(obs.outputs, pred.outputs.glm.08))#AUC=0.7548

plot.roc(flagu.08$FATAL_OR_MAJ_INJ, glm.pred.08,col = c("black"))
plot.roc(obs.outputs, pred.outputs.glm.08, col = "red", add = TRUE)
legend("bottomright", legend = c("2008 logistic regression Training", "2008 logistic regression Cross-Validation"), col = c("black", "red"), lwd = 1)
# Turns out the logistic regression is a better model of predicting fatal or major injury containing crashes in 2008. The common factors related to fatal or major injury containing crashes predicted by both rf and logistic regression are....


### Results
Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.
