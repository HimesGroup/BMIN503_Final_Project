---
title: "BMIN503/EPID600 Car Crashes in Philadelphia in 2008 and 2018"
author: "Nuoying MA"
output: 
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  
***
Use this template to complete your project throughout the course. Your Final Project presentation in class will be based on the contents of this document. Replace the title/name and text below with your own, but keep the headers.

### Overview
In this section, give a brief a description of your project and its goal, what data you are using to complete it, and what three faculty/staff in different fields you have spoken to about your project with a brief summary of what you learned from each person. Include a link to your final project GitHub repository.

The goal is to examine car crash data in Philadelphia in 2008 and 2018, respectively. This project will first obtain and compare the critical factors contributing the fatal car crashes in 2008 and 2018 using machine learning methods. Then, the important factors contributing to the deadly car crashes will be plotted. 


### Introduction 
In the first paragraph, describe the problem addressed, its significance, and some background to motivate the problem.

This project utilizes the data provided by the Philadelphia government for all car crashes in 2008 and 2018. Car crashes can be fatal and detrimental to the families of victims. By finding the most critical factors contributing to fatal car crashes, this project could provide a warning and raise the caution to drivers and pedestrians of those factors, and potentially reduce the fatal rate of accidents. 

In the second paragraph, explain why your problem is interdisciplinary, what fields can contribute to its understanding, and incorporate background related to what you learned from meeting with faculty/staff.

Since the car crash data also contains information on observed dangerous driving behaviors, including alcohol consumption, speeding, this data would be useful to study the effect of human psychology and health status on the severity of accidents. 


### Methods & Results
In the first paragraph, describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why. 

```{r, eval = TRUE}
getwd()
library(ggplot2)
library(dplyr)
flag.2018 <- read.csv("Philadelphia_FLAG_2018.csv", header = TRUE) #load 2018 flag data
crash.2018 <- read.csv("CRASH_2018_Philadelphia.csv", header = TRUE) #load 2018 flag data

crash.2008<-read.csv("CRASH_2008_Philadelphia.csv", header=TRUE)
flag.2008<-read.csv("FLAG_2008_Philadelphia.csv", header=TRUE) #load 2008 flag data, look for most important factors contributing to fatal car crashes.
crash.08 <- crash.2008 %>%
             select(POLICE_AGCY, ARRIVAL_TM, DISPATCH_TM, CRN, CRASH_YEAR,DAY_OF_WEEK, HOUR_OF_DAY,FATAL_COUNT, INJURY_COUNT, PERSON_COUNT, ROAD_CONDITION, ILLUMINATION, WEATHER, COLLISION_TYPE, RELATION_TO_ROAD, TCD_FUNC_CD)

#We would like to find out important factors contributing to fatal/major injury containing crashes.
flagu.18<-flag.2018 %>% 
  select_if(~n_distinct(.) > 1)#We first need to exclude columns with only 1 value (otherwise cause error in logistic regression)
colnames(flagu.18)
ncol(flagu.18) #184

df1<-as.vector(colnames(flag.2008))
df1 #95 variables
df2<-as.vector(colnames(flagu.18))
df2 #184 variables
df3<-df2[df2%in%df1]#df2 include 87 variables in df1
df1[!df1%in%df3]#lack"LOCAL_ROAD","TURNPIKE","DRIVER_16YR","DRIVER_50_64YR","CRASH_YEAR","C         OUNTY","COUNTY_YEAR","MUNICIPALITY" 
df2[!df2%in%df1] #df2 has 97 more variables than df1, among them some are worthy to be examined, includes "CRASH_MONTH", "DAY_OF_WEEK","TIME_OF_DAY","HOUR_OF_DAY","WEATHER", "COLLISION_TYPE" , "TCD_TYPE", "LATITUDE", "LONGITUDE","TCD_FUNC_CD", "WZ_CLOSE_DETOUR", "WZ_FLAGGER", "WZ_LAW_OFFCR_IND", "WZ_LN_CLOSURE", "WZ_MOVING", "WZ_OTHER", "WZ_SHLDER_MDN", "DRIVER_5NO_64YR",  "LIMIT_70MPH", "ANGLE_CRASH", "HORSE_BUGGY", "ATV", "CORE_NETWORK", "OPIOID_RELATED", "LANE_DEPARTURE", "BACKUP_PRIOR", "BACKUP_NONRECURRING" ,"BACKUP_CONGESTION"          
flag.2008$VEHICLE_TOWED<-NULL#this is a variable happens after crashes
flag.2008$COUNTY_YEAR<-NULL

#Look at the distribution of crashes among different situations, 2008 situations
ggplot(data = crash.08, aes(x = factor(DAY_OF_WEEK)))+
  geom_bar(color="darkblue", fill= "darkblue")+
  labs(x = "day of week when crash happens 2008")+
  ggsave("dayofweek08.tiff")
#More crashes happens on Saturday.

crash.08 %>% 
    mutate(HOUR_OF_DAY = ifelse(HOUR_OF_DAY %in% c(99), NA,factor(HOUR_OF_DAY))) %>% 
    ggplot(aes(x = HOUR_OF_DAY))+
    geom_bar()+
    labs(x = "hour of the day when crash happens 2008")+
    ggsave("hourofday08.tiff")
# Peak hour of crashes: from 8-10AM in the morning, and from 5-7PM in the afternoon. Much more likely to happen in the afternoon.

crash.08 %>% 
    ggplot(aes(x = factor(ROAD_CONDITION)))+
    geom_bar()+
    labs(x = "Road condition 2008")+
    ggsave("Roadcondition08.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 

crash.08 %>% 
    ggplot(aes(x = factor(ILLUMINATION)))+
    geom_bar()+
    labs(x = "illumination 2008")+
    ggsave("illumination08.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 

crash.08 %>% 
    ggplot(aes(x = factor(WEATHER)))+
    geom_bar()+
    labs(x = "weather 2008")+
    ggsave("weather08.tiff")# Mostly no adverse condition, second: rain

crash.08 %>% 
    ggplot(aes(x = factor(COLLISION_TYPE)))+
    geom_bar()+
    labs(x = "Collison type 2008")+
    ggsave("Collisontype08.tiff") #Mostly 
crash.08 %>% 
  group_by(COLLISION_TYPE)%>% 
  count(COLLISION_TYPE)

crash.2008 %>% 
    ggplot(aes(x = factor(RELATION_TO_ROAD)))+
    geom_bar()+
    labs(x = "Relation to road 2008")+
    ggsave("Relation to road 08.tiff") #Mostly on roadway.

crash.2008 %>% 
    ggplot(aes(x = factor(TCD_FUNC_CD)))+
    geom_bar()+
    labs(x = "TCD function 2008")+
    ggsave("TCD08.tiff") #About half: with no controls, another half: device functioning properly.
crash.2008 %>% 
  group_by(TCD_FUNC_CD)%>% 
  count(TCD_FUNC_CD)#About 4766/10676=0.45%: with no controls, another 5222/10676=0.49%: device functioning properly.
nrow(crash.2008) #10676 accidents.

#time difference from police dispatched to police arrived //IS it related to police agency?
crash.2008%>% 
    mutate(arrival=DISPATCH_TM-ARRIVAL_TM)%>% 
    ggplot(aes(x = arrival))+
    geom_histogram()+
    labs(x = "Time taken for police to arrive at scene")+
    ggsave("Time08.tiff")

#Look at the distribution of crashes among different situations, 2018 situations
library(ggplot2) 
fig1<-ggplot(data = flag.2018, aes(x = factor(DAY_OF_WEEK)))+
  geom_bar(color="darkblue", fill= "darkblue")+
  labs(x = "day of week when crash happens 2018")
fig1#summarise by day of week
#no significant difference in number of car crashes among different day of week.
ggsave("dayofweek18.tiff",plot=fig1)

library(dplyr)
flag.2018 %>% 
    mutate(HOUR_OF_DAY = ifelse(HOUR_OF_DAY %in% c(99), NA,factor(HOUR_OF_DAY))) %>% 
    ggplot(aes(x = HOUR_OF_DAY))+
    geom_bar()+
    labs(x = "hour of the day when crash happens 2018")+
    ggsave("hourofday18.tiff")
# Peak hour of crashes: from 8-9AM in the morning, and from 4-8PM in the afternoon.

flag.2018 %>% 
    ggplot(aes(x = ROAD_CONDITION))+
    geom_bar()+
    labs(x = "Road condition 2018")+
    ggsave("Roadcondition18.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 
nrow(flag.2018)
flag.2018%>%
  group_by(ROAD_CONDITION)%>%
  count(ROAD_CONDITION)
#There are 207 sunny days in Philadelphia. 158 non-sunny days.

flag.2018 %>% 
    ggplot(aes(x = ILLUMINATION))+
    geom_bar()+
    labs(x = "illumination 2018")+
    ggsave("illumination18.tiff") #Mostly happens on daylight, and dark with street lights are the second common. 

flag.2018 %>% 
    ggplot(aes(x = factor(COLLISION_TYPE)))+
    geom_bar()+
    labs(x = "Collison type 2018")+
    ggsave("Collisontype18.tiff") #First likely at an angle, second likely: rear-end
flag.2018 %>% 
  group_by(COLLISION_TYPE)%>% 
  count(COLLISION_TYPE)
 
  

crash.2018 %>% 
    ggplot(aes(x = factor(RELATION_TO_ROAD)))+
    geom_bar()+
    labs(x = "Relation to road 2018")+
    ggsave("Relation to road 18.tiff") #Mostly on roadway.
flag.2018 %>% 
    ggplot(aes(x = factor(TCD_FUNC_CD)))+
    geom_bar()+
    labs(x = "TCD function 2018")+
    ggsave("TCD18.tiff")
flag.2018 %>% 
  group_by(TCD_FUNC_CD)%>% 
  count(TCD_FUNC_CD)#About 4878/11003=0.44%: device functioning properly, another 5998/11003=0.55%:with no controls 
nrow(flag.2018) #11003 accidents.

```
 
```{r, eval = TRUE}
colnames(flag.2008)
# To see how many car accidents include "MAJOR_INJURY", "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL","FATAL_OR_MAJ_INJ","INJURY" 
nrow(flag.2008) # 10676 car accidents in total
damage.08 <- flag.2008 %>%
             select(CRN,MAJOR_INJURY,MODERATE_INJURY, MINOR_INJURY, FATAL, INJURY,FATAL_OR_MAJ_INJ) 
fig8<-ggplot(data = damage.08,aes(x = factor(INJURY)))+
    geom_bar()+
    labs(x = "Injury 2008") # plot shows accidents cateogrised into no injury vs injury.
ggsave("injury08.tiff",plot=fig8)
damage.08%>%
  filter(INJURY==1)%>%
  count(INJURY) # 8234 injuries
injury.08<-damage.08%>%
  filter(INJURY==1)
damage.08%>%
  filter(MODERATE_INJURY==1)%>%
  count(MODERATE_INJURY) # 1231 moderate injuries
damage.08%>%
  filter(MAJOR_INJURY==1)%>%
  count(MAJOR_INJURY) # 254 major injuries
damage.08%>%
  filter(MINOR_INJURY==1)%>%
  count(MINOR_INJURY) # 4352 minor injuries
damage.08%>%
  filter(FATAL==1)%>%
  count(FATAL) # 4352 minor injuries
injury.08%>%
  filter(FATAL_OR_MAJ_INJ==1)%>%
  count(FATAL_OR_MAJ_INJ)#280 injury containing accidents involve major injuries or fatal events.
ggplot(data=injury.08, aes(factor(FATAL_OR_MAJ_INJ==1)))+
    geom_bar()+
    labs(x = "Fatal or major injury accidents")+ # plot shows accidents involving injury cateogrised into containing fatal or not.
    ggsave("fatalormajor08.tiff")
class(flag.2008$FATAL)
#This excel sheet contains 4 separate column, driver age=16,17,18,19,20, I would like to combine them to be 1 column, and remove the original columns
flag.2008$Driveryoung<-flag.2008$DRIVER_16YR+flag.2008$DRIVER_17YR+flag.2008$DRIVER_18YR+flag.2008$DRIVER_19YR+flag.2008$DRIVER_20YR
flag.2008$Driveryoung<-ifelse(flag.2008$Driveryoung==0,0,1)
flag.2008$DRIVER_16YR<-NULL
flag.2008$DRIVER_17YR<-NULL
flag.2008$DRIVER_18YR<-NULL
flag.2008$DRIVER_19YR<-NULL
flag.2008$DRIVER_20YR<-NULL
colnames(flag.2008)
test <- flag.2008 %>% 
  select(CRN,Driveryoung)
  class(test$Driveryoung)# test the values of Driveryoung
#We would like to focus on fatal or major injuries crashes, so exclue other conditions, like "MAJOR_INJURY"       "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL", "FATAL","INJURY" 
flag.2008$MAJOR_INJURY<-NULL
flag.2008$MODERATE_INJURY<-NULL
flag.2008$MINOR_INJURY<-NULL
flag.2008$PROPERTY_DAMAGE_ONLY<-NULL
flag.2008$INJURY_OR_FATAL<-NULL
flag.2008$FATAL<-NULL
flag.2008$INJURY<-NULL
colnames(flag.2008)

#Remove highly correlated data
install.packages('lattice')
install.packages('caret')
library('caret')
#remove columns with only 1 value.
flag.2008$CRASH_YEAR<-NULL
flag.2008$COUNTY<-NULL
flag.2008$MUNICIPALITY<-NULL
flag.2008$LIMIT_65MPH<-NULL


flag.2008.c = cor(flag.2008,use="complete.obs")
hc = findCorrelation(flag.2008.c, cutoff=0.9,verbose = FALSE, names = FALSE,
  exact = FALSE) 
hc = sort(hc)
reduced_Data08 = flag.2008[,-c(hc)]
rd08<-reduced_Data08
df5<-colnames(flag.2008)
df6<-colnames(rd08)
df7<-df6[df6%in%df5]
df5[!df5%in%df7]#exclude"STATE_ROAD","NON_INTERSECTION","TRAIN_TROLLEY","DRINKING_DRIVER","DRUGGED_DRIVER"

#Run random forest or multiple linear regression the flag dataset, to find out the important factors contributing to fatal/major injury accidents.
#Convert 0,1 to yes and no
flag.2008.r<-rd08
flag.2008.r$FATAL_OR_MAJ_INJ<-factor(flag.2008$FATAL_OR_MAJ_INJ, levels=c(0,1), labels = c("no","fatal/major injuries"))
class(flag.2008.r$FATAL_OR_MAJ_INJ)

library(randomForest) 
#Need to remove CRN
flag.2008.r$CRN<-NULL
#Tune random forest by changing mtry
#mtry: Number of random variables collected at each split. In normal equal square number columns.
f18r.res<-tuneRF(flag.2008.r, flag.2008.r$FATAL_OR_MAJ_INJ,mtry=6, ntreeTry=50, stepFactor=1.5,improve=0,
trace=TRUE, plot=TRUE, doBest=FALSE)
#When mtry=13, the OOB is the lowest, so we use mtry=13 for random forest.
flag08.rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flag.2008.r, ntree = 100, importance = TRUE,mtry=13)  #OOB estimate of  error rate: 3.15%
flag08.rf

flag08.rf$importance
rf.pred.08 <- predict(flag08.rf , flag.2008.r, type = "prob")
head(rf.pred.08)
rf.pred.yes.08<- rf.pred.08[ ,2]


varImpPlot(flag08.rf, type=2)#Plot of import variables vs MeanDecreaseGini scores
imp08 <- flag08.rf $importance
rf08.im<-as.data.frame(head(sort(imp08[ , 4], decreasing = TRUE), n = 83))
rf08.im#Dataframe of import variables vs MeanDecreaseGini scores


#Create a logistic regression model, find variables are significant at p < 0.05.
library(dplyr) #We first need to exclude columns with only 1 value (otherwise cause error in logistic regression)
flagu.08<-rd08 %>% 
  select_if(~n_distinct(.) > 1)
colnames(flagu.08)
flagu.08$FATAL_OR_MAJ_INJ<-factor(flagu.08$FATAL_OR_MAJ_INJ, levels=c(0,1), labels = c("No","fatal/major injuries"))
flagu.08$CRN<-NULL
flag08.glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = flagu.08, family = binomial())
summary(flag08.glm) 
lr08<-data.frame(summary(flag08.glm)$coef[summary(flag08.glm)$coef[,4]<=0.05, 4])#most important factors are SHLDR_RELATED	HIT_TREE_SHRUB	HIT_POLE	OVERTURNED	MOTORCYCLE	BICYCLE	RUNNING_RED_LT	CROSS_MEDIAN	SPEEDING	UNBELTED	PEDESTRIAN	COMM_VEHICLE	ILLUMINATION_DARK	MC_DRINKING_DRIVER
glm.pred.08 <- predict(flag08.glm, flagu.08, type = "response") 
lr08
```

Create 10-fold cross validation classification vectors for each model. Obtain AUC values and make an ROC plot that shows ROC curves corresponding to predictive accuracy using the training data as well as the 10-fold cross-validations.

```{r, eval = TRUE} 
#K-fold cross validation for random forest 2008
N = nrow(flag.2008.r) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.rf.08 <- vector(mode = "numeric", length = N)
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flag.2008.r, s != i)
	test <- filter(flag.2008.r, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #RF train/test
	rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = train, ntree = 100,mtry=13)
	rf.pred.curr <- predict(rf, newdata = test, type = "prob") 
	pred.outputs.rf.08[1:length(s[s == i]) + offset] <- rf.pred.curr[ , 2]
  
  offset <- offset + length(s[s == i])
}

library(pROC) 
auc(roc(flag.2008.r$FATAL_OR_MAJ_INJ, rf.pred.yes.08))#AUC=0.8799
auc(roc(obs.outputs, pred.outputs.rf.08))#AUC=0.6649

plot.roc(flag.2008.r$FATAL_OR_MAJ_INJ, rf.pred.no.08, col = "black") #random forest
plot.roc(obs.outputs, pred.outputs.rf.08, ci = TRUE, col = "blue", add = TRUE)
legend("bottomright", legend = c("2008 RF Training", "2008 RF Cross-Validation"), col = c("black", "blue"), lwd = 1)

#K-fold cross validation for logistic regression 2008

N = nrow(flagu.08) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.glm.08 <- vector(mode = "numeric", length = N) 
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flagu.08, s != i)
	test <- filter(flagu.08, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #GLM train/test
	glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = train, family = binomial(logit))
  glm.pred.curr <- predict(glm, test, type = "response")
  pred.outputs.glm.08[1:length(s[s == i]) + offset] <- glm.pred.curr
  
  offset <- offset + length(s[s == i])
}

library(pROC) 
auc(roc(flagu.08$FATAL_OR_MAJ_INJ, glm.pred.08))#AUC=0.7983
auc(roc(obs.outputs, pred.outputs.glm.08))#AUC=0.7525

plot.roc(flagu.08$FATAL_OR_MAJ_INJ, glm.pred.08,col = c("black"))
plot.roc(obs.outputs, pred.outputs.glm.08, col = "red", add = TRUE)
legend("bottomright", legend = c("2008 logistic regression Training", "2008 logistic regression Cross-Validation"), col = c("black", "red"), lwd = 1)
# Turns out the logistic regression is a better model of predicting fatal or major injury containing crashes in 2008. The common factors related to fatal or major injury containing crashes predicted by both rf and logistic regression are....


```

Fatal/major injury containing crashes for 2018 data
```{r, eval = TRUE} 
colnames(flag.2018)
# To see how many car accidents include "MAJOR_INJURY", "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL","FATAL_OR_MAJ_INJ","INJURY" 
nrow(flag.2018) # 11003 car accidents in total
damage.18 <- flag.2018 %>%
             select(CRN,MAJOR_INJURY,MODERATE_INJURY, MINOR_INJURY, FATAL, INJURY,FATAL_OR_MAJ_INJ) 
ggplot(data = damage.18,aes(x = factor(INJURY)))+
    geom_bar()+
    labs(x = "Injury 2018")+ # plot shows accidents cateogrised into no injury vs injury.
    ggsave("injury18.tiff",plot=fig8)
damage.18%>%
  filter(INJURY=="Yes")%>%
  count(INJURY) # 7631 injuries
injury.18<-damage.18%>%
  filter(INJURY=="Yes")
damage.18%>%
  filter(MODERATE_INJURY=="Yes")%>%
  count(MODERATE_INJURY) # 1844 moderate injuries
damage.18%>%
  filter(MAJOR_INJURY=="Yes")%>%
  count(MAJOR_INJURY) # 242 major injuries
damage.18%>%
  filter(MINOR_INJURY=="Yes")%>%
  count(MINOR_INJURY) # 3160 minor injuries
damage.18%>%
  filter(FATAL=="Yes")%>%
  count(FATAL) #100 fatal injuries
injury.18%>%
  filter(FATAL_OR_MAJ_INJ=="Yes")%>%
  count(FATAL_OR_MAJ_INJ)#267 injury containing accidents involve major injuries or fatal events.
ggplot(data=injury.18, aes(factor(FATAL_OR_MAJ_INJ=="Yes")))+
    geom_bar()+
    labs(x = "Fatal or major injury accidents 2018")+ # plot shows accidents involving injury cateogrised into containing fatal or not.
    ggsave("fatalormajor18.tiff")
class(flag.2018$FATAL_OR_MAJ_INJ)#factor
```
Now, we clear the data for flag.2018 or flagu.18 (flag.2018, excluding columns with single values)
This excel sheet contains 4 separate column with regards to young drivers, driver age=16,17,18,19,20, I would like to combine them to be 1 column, and remove the original columns
```{r, eval = TRUE} 
install.packages('varhandle')
library('varhandle')#first convert factors into numerical values
flagu.18$DRIVER_YES6YR <- unfactor(flagu.18$DRIVER_YES6YR)
flagu.18$DRIVER_YES6YR<-ifelse(flagu.18$DRIVER_YES6YR==c("No"),0,1)
flagu.18$DRIVER_17YR<-unfactor(flagu.18$DRIVER_17YR)
flagu.18$DRIVER_17YR<-ifelse(flagu.18$DRIVER_17YR==c("No"),0,1)
flagu.18$DRIVER_18YR<-unfactor(flagu.18$DRIVER_18YR)
flagu.18$DRIVER_18YR<-ifelse(flagu.18$DRIVER_18YR==c("No"),0,1)
flagu.18$DRIVER_19YR<-unfactor(flagu.18$DRIVER_19YR)
flagu.18$DRIVER_19YR<-ifelse(flagu.18$DRIVER_19YR==c("No"),0,1)
flagu.18$DRIVER_20YR<-unfactor(flagu.18$DRIVER_20YR)
flagu.18$DRIVER_20YR<-ifelse(flagu.18$DRIVER_20YR==c("No"),0,1)
flagu.18$Driveryoung<-flagu.18$DRIVER_YES6YR+flagu.18$DRIVER_17YR+flagu.18$DRIVER_18YR+flagu.18$DRIVER_19YR+flagu.18$DRIVER_20YR
flagu.18$Driveryoung<-ifelse(flagu.18$Driveryoung==0,0,1)
flagu.18$Driveryoung<-factor(flagu.18$Driveryoung, levels=c(0,1), labels = c("No","Yes"))
flagu.18$DRIVER_YES6YR<-NULL
flagu.18$DRIVER_17YR<-NULL
flagu.18$DRIVER_18YR<-NULL
flagu.18$DRIVER_19YR<-NULL
flagu.18$DRIVER_20YR<-NULL
colnames(flagse.18)

#We would like to focus on fatal or major injuries crashes, so exclue other injury conditions and other counts of injuries by using select function, and exclude highly correlated variables and storing into flagse.18 dataframe.
flagse.18<-flagu.18%>%
  select("Driveryoung","INTERSTATE","LOCAL_ROAD_ONLY","WET_ROAD","SNOW_SLUSH_ROAD","ICY_ROAD","SUDDEN_DEER","SHLDR_RELATED","REAR_END","HO_OPPDIR_SDSWP","HIT_FIXED_OBJECT","SV_RUN_OFF_RD","WORK_ZONE","FATAL_OR_MAJ_INJ","INTERSECTION","SIGNALIZED_INT","STOP_CONTROLLED_INT","UNSIGNALIZED_INT","SCHOOL_BUS","SCHOOL_ZONE","HIT_DEER","HIT_TREE_SHRUB","HIT_EMBANKMENT","HIT_POLE","HIT_GDRAIL","HIT_GDRAIL_END","HIT_BARRIER","HIT_BRIDGE","OVERTURNED","MOTORCYCLE","BICYCLE","HVY_TRUCK_RELATED","VEHICLE_FAILURE","PHANTOM_VEHICLE","ALCOHOL_RELATED","UNDERAGE_DRNK_DRV","UNLICENSED","DISTRACTED","CELL_PHONE","NO_CLEARANCE","RUNNING_RED_LT","TAILGATING","CROSS_MEDIAN","CURVED_ROAD","CURVE_DVR_ERROR","LIMIT_65MPH","SPEEDING","SPEEDING_RELATED","AGGRESSIVE_DRIVING","FATIGUE_ASLEEP","DRIVER_65_74YR","DRIVER_75PLUS","UNBELTED","PEDESTRIAN","COMM_VEHICLE","PSP_REPORTED","NHTSA_AGG_DRIVING","DEER_RELATED","ILLUMINATION_DARK","RUNNING_STOP_SIGN","TRAIN","TROLLEY","HIT_PARKED_VEHICLE","FIRE_IN_VEHICLE","VEHICLE_TOWED","HAZARDOUS_TRUCK","MC_DRINKING_DRIVER","DRUG_RELATED","ILLEGAL_DRUG_RELATED","SCHOOL_BUS_UNIT","IMPAIRED_DRIVER","CRASH_MONTH", "DAY_OF_WEEK","HOUR_OF_DAY","WEATHER", "COLLISION_TYPE" , "TCD_TYPE","TCD_FUNC_CD", "DRIVER_5NO_64YR","LIMIT_70MPH", "ANGLE_CRASH", "HORSE_BUGGY", "ATV", "CORE_NETWORK", "OPIOID_RELATED", "LANE_DEPARTURE", "BACKUP_PRIOR", "BACKUP_NONRECURRING" ,"BACKUP_CONGESTION")

```

```{r, eval = TRUE}
#Use random forest to map out import factors related to fatal/major injuries 2018 using flagse.18 dataframe
library(randomForest) 
#Tune random forest by changing mtry
#mtry: Number of random variables collected at each split. In normal equal square number columns.
f18.res<-tuneRF(flagse.18, flagse.18$FATAL_OR_MAJ_INJ,mtry=6, ntreeTry=100, stepFactor=1.5,trace=TRUE, doBest=FALSE)
#When mtry=13, the OOB is the lowest, so we use mtry=13 for random forest.
flag18.rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flagse.18, ntree = 200, importance = TRUE, mtry=13) 
flag18.rf #OOB=2.93% for ntree=200
flag18.rf$importance
varImpPlot(flag18.rf, type=2)#Plot of import variables vs MeanDecreaseGini scores

imp18 <- flag18.rf $importance
rf18.im<-as.data.frame(head(sort(imp18[ , 4], decreasing = TRUE), n = 99))
rf18.im#Dataframe of import variables vs MeanDecreaseGini scores
#Most important factors are HOUR_OF_DAY, CRASH_MONTH, DAY_OF_WEEK, COLLISION_TYPE, MOTORCYCLE,WEATHER,LOCAL_ROAD_ONLY, DRIVER_5NO_64YR, ILLUMINATION_DARK, TCD_TYPE, AGGRESSIVE_DRIVING, UNBELTED, PEDESTRIAN, CORE_NETWORK, OPIOID_RELATED, OPIOID_RELATED, WET_ROAD, ALCOHOL_RELATED, DRIVER_65_74YR,TCD_FUNC_CD,INTERSECTION,SPEEDING_RELATED,SPEEDING, HIT_PARKED_VEHICLE, COMM_VEHICLE, CURVED_ROAD, UNSIGNALIZED_INT, SIGNALIZED_INT, IMPAIRED_DRIVER, HIT_TREE_SHRUB, HORSE_BUGGY, SV_RUN_OFF_RD
rf.pred.18 <- predict(flag18.rf, flagse.18, type = "prob")


#Try to use very important factors to run Random forest again.
flag18rf<-flagse.18%>% 
  select(HOUR_OF_DAY, CRASH_MONTH, DAY_OF_WEEK, COLLISION_TYPE, MOTORCYCLE,WEATHER,LOCAL_ROAD_ONLY, DRIVER_5NO_64YR, ILLUMINATION_DARK, TCD_TYPE, AGGRESSIVE_DRIVING, UNBELTED, PEDESTRIAN, CORE_NETWORK, OPIOID_RELATED, OPIOID_RELATED, WET_ROAD, ALCOHOL_RELATED, DRIVER_65_74YR,TCD_FUNC_CD,INTERSECTION,SPEEDING_RELATED,SPEEDING, HIT_PARKED_VEHICLE, COMM_VEHICLE, CURVED_ROAD, UNSIGNALIZED_INT, SIGNALIZED_INT, IMPAIRED_DRIVER, HIT_TREE_SHRUB, HORSE_BUGGY, SV_RUN_OFF_RD, FATAL_OR_MAJ_INJ)
flag18.rf.2 <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flag18rf, ntree = 200, importance = TRUE) 
flag18.rf.2 #OOB=3.04% for ntree=200 using those important variables only, but class.error rate increases.


#Create a logistic regression model, find variables are significant at p < 0.05.
library(dplyr) 
flag18lr<-flagse.18 %>% 
  select_if(~n_distinct(.) > 1)
flag18lr$CRASH_MONTH <- as.factor(as.character(flag18lr$CRASH_MONTH))
flag18lr$DAY_OF_WEEK <- as.factor(as.character(flag18lr$DAY_OF_WEEK))
flag18lr$HOUR_OF_DAY <- as.factor(as.character(flag18lr$HOUR_OF_DAY))


flag18.glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = flag18lr, family = binomial(logit))
summary(flag18.glm) 
lr18<-data.frame(summary(flag18.glm)$coef[summary(flag18.glm)$coef[,4]<=0.05, 4])#most important factors are INTERSTATEYes,LOCAL_ROAD_ONLYes,REAR_ENDYes, WORK_ZONEYes,STOP_CONTROLLED_INTYes, HIT_TREE_SHRUBYes,HIT_GDRAILYes, OVERTURNEDYes, MOTORCYCLEYes, BICYCLEYes, ALCOHOL_RELATEDYes, CROSS_MEDIANYes, SPEEDINGYes, DRIVER_65_74YRYes, UNBELTEDYes, PEDESTRIANYes, COMM_VEHICLEYes, ILLUMINATION_DARKYes, RUNNING_STOP_SIGNYes, VEHICLE_TOWEDYes, DRUG_RELATEDYes,CRASH_MONTH, COLLISION_TYPESideswipe (same dir.), HORSE_BUGGYNo, ATVYes, OPIOID_RELATEDYes 
lr18
# Use only significant factors to do the glm again, and create ROC plot.
flag18lr2<-flag18lr%>%
  select(INTERSTATE, LOCAL_ROAD_ONLY,REAR_END, STOP_CONTROLLED_INT, HIT_TREE_SHRUB, HIT_GDRAIL, HIT_BRIDGE,OVERTURNED, MOTORCYCLE, BICYCLE, ALCOHOL_RELATED, CROSS_MEDIAN, SPEEDING,DRIVER_65_74YR, UNBELTED, PEDESTRIAN, COMM_VEHICLE, ILLUMINATION_DARK, RUNNING_STOP_SIGN, DRUG_RELATED, CRASH_MONTH, DAY_OF_WEEK, HOUR_OF_DAY, COLLISION_TYPE, HORSE_BUGGY,ATV, OPIOID_RELATED, FATAL_OR_MAJ_INJ)
flag18.glm2 <- glm(FATAL_OR_MAJ_INJ ~ ., data = flag18lr2, family = binomial(logit))
summary(flag18.glm2) 
lr18.2<-data.frame(summary(flag18.glm2)$coef[summary(flag18.glm2)$coef[,4]<=0.05, 4])
lr18.2#still all those factors are significant.
glm.pred.18 <- predict(flag18.glm2, flag18lr2, type = "response")
glm.pred.18 
```


Create 10-fold cross validation classification vectors for each model. Obtain AUC values and make an ROC plot that shows ROC curves corresponding to predictive accuracy using the training data as well as the 10-fold cross-validations.

```{r, eval = TRUE}
#K-fold cross validation for random forest and logistic regression 2018
N = nrow(flagse.18) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.rf.18 <- vector(mode = "numeric", length = N)
pred.outputs.glm.18 <- vector(mode = "numeric", length = N) 
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flagse.18, s != i)
	test <- filter(flagse.18, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #RF train/test
	rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = train, ntree = 200, mtry=13)
	rf.pred.curr <- predict(rf, newdata = test, type = "prob") 
	pred.outputs.rf.18[1:length(s[s == i]) + offset] <- rf.pred.curr[ , 2]
  
  offset <- offset + length(s[s == i])
}



#GLM train/test
N = nrow(flag18lr2) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.glm.18 <- vector(mode = "numeric", length = N) 
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flag18lr2, s != i)
	test <- filter(flag18lr2, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
  
  #logistic regression
  glm <- glm(FATAL_OR_MAJ_INJ~ ., data = train, family = binomial(logit))
  glm.pred.curr <- predict(glm, test, type = "response")
  pred.outputs.glm.18[1:length(s[s == i]) + offset] <- glm.pred.curr
  
  offset <- offset + length(s[s == i])
}
	
library(pROC) 
#For random forest
auc(roc(flagse.18$FATAL_OR_MAJ_INJ, rf.pred.yes.18))#AUC=0.9991
auc(roc(obs.outputs, pred.outputs.rf.18))#AUC=0.7751

plot.roc(flagse.18$FATAL_OR_MAJ_INJ, rf.pred.yes.18, col = "black") #random forest
plot.roc(obs.outputs, pred.outputs.rf.18, ci = TRUE, col = "blue", add = TRUE)
legend("bottomright", legend = c("2008 RF Training", "2008 RF Cross-Validation"), col = c("black", "blue"), lwd = 1)

#For logistic regression
auc(roc(flag18lr2$FATAL_OR_MAJ_INJ, glm.pred.18))#AUC=0.8415
auc(roc(obs.outputs, pred.outputs.glm.18))#AUC=0.8034
plot.roc(flag18lr2$FATAL_OR_MAJ_INJ, glm.pred.18,col = c("black"))
plot.roc(obs.outputs, pred.outputs.glm.18, col = "red", add = TRUE)
legend("bottomright", legend = c("2008 logistic regression Training", "2008 logistic regression Cross-Validation"), col = c("black", "red"), lwd = 1)
# Turns out the logistic regression is a better model of predicting fatal or major injury containing crashes in 2008. The common factors related to fatal or major injury containing crashes predicted by both rf and logistic regression are....
```

###Conclusion
Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.
