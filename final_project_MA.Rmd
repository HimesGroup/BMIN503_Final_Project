---
title: "BMIN503/EPID600 Car Crashes in Philadelphia in 2008 and 2018"
author: "Nuoying MA"
output: 
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  
***
Use this template to complete your project throughout the course. Your Final Project presentation in class will be based on the contents of this document. Replace the title/name and text below with your own, but keep the headers.

### Overview
In this section, give a brief a description of your project and its goal, what data you are using to complete it, and what three faculty/staff in different fields you have spoken to about your project with a brief summary of what you learned from each person. Include a link to your final project GitHub repository. 

The goal is to examine car crash data in Philadelphia in 2008 and 2018, respectively. This project will first obtain important objective factors related to crashes. This project will then obtain and compare the critical factors contributing the fatal or major injury resulting car crashes in 2008 and 2018 using machine learning methods (random forest and logistic regression). Then, the important objective factors contributing to the car crashes will be plotted. 
My Repo: [Nuoying final project][https://github.com/Nuoying/BMIN503_Final_Project] 


### Introduction 
In the first paragraph, describe the problem addressed, its significance, and some background to motivate the problem.

This project utilizes the data provided by the Philadelphia government for all crashes in 2008 and 2018. Car crashes can be fatal and detrimental to the families of victims. By finding the most critical factors contributing to fatal or major injury resulting crashes, this project could provide a warning and raise the caution to drivers and pedestrians of those factors, and potentially reduce the fatal rate of accidents. 

Since the car crash data also contains information on observed dangerous driving behaviors, including alcohol consumption, speeding, this data would be useful to study the effect of human psychology and health status on the severity of accidents. 


### Methods & Results
In the first paragraph, describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why. 

I extract the Philadelphia crashes dataset for 2008 and 2018 from Pennsylvania Crash Information Tool (PCIT), the publicly available crash datasets. The website also allows users to visualise the locations of crashes. Since the dataset I obtained did not contains FIPS code, I did not create leaflet maps. From all the possible datasets, I focused on Philadelphia crashes, and compare 2008 data with 2018 data.


```{r, eval = TRUE} 
#install all needed package here.
#install.packages('lattice')
#install.packages('caret')
#install.packages('varhandle')
```

```{r, eval = TRUE}
getwd()
library(ggplot2)
library(dplyr)
flag.2018 <- read.csv("Philadelphia_FLAG_2018.csv", header = TRUE) #load 2018 flag data
crash.2018 <- read.csv("CRASH_2018_Philadelphia.csv", header = TRUE) #load 2018 flag data

crash.2008<-read.csv("CRASH_2008_Philadelphia.csv", header=TRUE)
flag.2008<-read.csv("FLAG_2008_Philadelphia.csv", header=TRUE) #load 2008 flag data, look for most important factors contributing to fatal car crashes.
crash.08 <- crash.2008 %>%
             select(POLICE_AGCY, ARRIVAL_TM, DISPATCH_TM, CRN, CRASH_YEAR,DAY_OF_WEEK, HOUR_OF_DAY,FATAL_COUNT, INJURY_COUNT, PERSON_COUNT, ROAD_CONDITION, ILLUMINATION, WEATHER, COLLISION_TYPE, RELATION_TO_ROAD, TCD_FUNC_CD)

#We would like to find out important factors contributing to fatal/major injury containing crashes.
flagu.18<-flag.2018 %>% 
  select_if(~n_distinct(.) > 1)#We first need to exclude columns with only 1 value (otherwise cause error in logistic regression)
colnames(flagu.18)
ncol(flagu.18) #184

df1<-as.vector(colnames(flag.2008))
#95 variables
df2<-as.vector(colnames(flagu.18))
#184 variables
df3<-df2[df2%in%df1]#df2 include 87 variables in df1
df1[!df1%in%df3]#lack"LOCAL_ROAD","TURNPIKE","DRIVER_16YR","DRIVER_50_64YR","CRASH_YEAR","C         OUNTY","COUNTY_YEAR","MUNICIPALITY" 
df2[!df2%in%df1] #df2 has 97 more variables than df1, among them some are worthy to be examined, includes "CRASH_MONTH", "DAY_OF_WEEK","TIME_OF_DAY","HOUR_OF_DAY","WEATHER", "COLLISION_TYPE" , "TCD_TYPE", "LATITUDE", "LONGITUDE","TCD_FUNC_CD", "WZ_CLOSE_DETOUR", "WZ_FLAGGER", "WZ_LAW_OFFCR_IND", "WZ_LN_CLOSURE", "WZ_MOVING", "WZ_OTHER", "WZ_SHLDER_MDN", "DRIVER_5NO_64YR",  "LIMIT_70MPH", "ANGLE_CRASH", "HORSE_BUGGY", "ATV", "CORE_NETWORK", "OPIOID_RELATED", "LANE_DEPARTURE", "BACKUP_PRIOR", "BACKUP_NONRECURRING" ,"BACKUP_CONGESTION"          
flag.2008$VEHICLE_TOWED<-NULL#this is a variable happens after crashes
flag.2008$COUNTY_YEAR<-NULL

#Look at the distribution of crashes among different situations, 2008 situations
ggplot(data = crash.08, aes(x = factor(DAY_OF_WEEK)))+
  geom_bar(color="darkblue", fill= "darkblue")+
  labs(x = "day of week when crash happens 2008")+
  ggsave("dayofweek08.tiff")
#More crashes happens on Saturday.

crash.08 %>% 
    mutate(HOUR_OF_DAY = ifelse(HOUR_OF_DAY %in% c(99), NA,factor(HOUR_OF_DAY))) %>% 
    ggplot(aes(x = HOUR_OF_DAY))+
    geom_bar()+
    labs(x = "hour of the day when crash happens 2008")+
    ggsave("hourofday08.tiff")
# Peak hour of crashes: from 8-10AM in the morning, and from 5-7PM in the afternoon. Much more likely to happen in the afternoon.

crash.08 %>% 
    ggplot(aes(x = factor(ROAD_CONDITION)))+
    geom_bar()+
    labs(x = "Road condition 2008")+
    ggsave("Roadcondition08.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 

crash.08 %>% 
    ggplot(aes(x = factor(ILLUMINATION)))+
    geom_bar()+
    labs(x = "illumination 2008")+
    ggsave("illumination08.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 

crash.08 %>% 
    ggplot(aes(x = factor(WEATHER)))+
    geom_bar()+
    labs(x = "weather 2008")+
    ggsave("weather08.tiff")# Mostly no adverse condition, second: rain

crash.08 %>% 
    ggplot(aes(x = factor(COLLISION_TYPE)))+
    geom_bar()+
    labs(x = "Collison type 2008")+
    ggsave("Collisontype08.tiff") #Mostly 
crash.08 %>% 
  group_by(COLLISION_TYPE)%>% 
  count(COLLISION_TYPE)

crash.2008 %>% 
    ggplot(aes(x = factor(RELATION_TO_ROAD)))+
    geom_bar()+
    labs(x = "Relation to road 2008")+
    ggsave("Relation to road 08.tiff") #Mostly on roadway.

crash.2008 %>% 
    ggplot(aes(x = factor(TCD_FUNC_CD)))+
    geom_bar()+
    labs(x = "TCD function 2008")+
    ggsave("TCD08.tiff") #About half: with no controls, another half: device functioning properly.
crash.2008 %>% 
  group_by(TCD_FUNC_CD)%>% 
  count(TCD_FUNC_CD)#About 4766/10676=0.45%: with no controls, another 5222/10676=0.49%: device functioning properly.
nrow(crash.2008) #10676 accidents.

#time difference from police dispatched to police arrived. Mostly 0 minutes, probably not true, not a useful data to include.
crash.2008%>% 
    mutate(arrival=DISPATCH_TM-ARRIVAL_TM)%>% 
    ggplot(aes(x = arrival))+
    geom_histogram()+
    labs(x = "Time taken for police to arrive at scene")+
    ggsave("Time08.tiff")

#Look at the distribution of crashes among different situations, 2018 situations
library(ggplot2) 
fig1<-ggplot(data = flag.2018, aes(x = factor(DAY_OF_WEEK)))+
  geom_bar(color="darkblue", fill= "darkblue")+
  labs(x = "day of week when crash happens 2018")
fig1#summarise by day of week
#no significant difference in number of car crashes among different day of week.
ggsave("dayofweek18.tiff",plot=fig1)

library(dplyr)
flag.2018 %>% 
    mutate(HOUR_OF_DAY = ifelse(HOUR_OF_DAY %in% c(99), NA,factor(HOUR_OF_DAY))) %>% 
    ggplot(aes(x = HOUR_OF_DAY))+
    geom_bar()+
    labs(x = "hour of the day when crash happens 2018")+
    ggsave("hourofday18.tiff")
# Peak hour of crashes: from 8-9AM in the morning, and from 4-8PM in the afternoon.

flag.2018 %>% 
    ggplot(aes(x = ROAD_CONDITION))+
    geom_bar()+
    labs(x = "Road condition 2018")+
    ggsave("Roadcondition18.tiff") #Mostly happens on dry roads, and wet roads are the second contributing factors. 
nrow(flag.2018)
flag.2018%>%
  group_by(ROAD_CONDITION)%>%
  count(ROAD_CONDITION)
#There are 207 sunny days in Philadelphia. 158 non-sunny days.

flag.2018 %>% 
    ggplot(aes(x = ILLUMINATION))+
    geom_bar()+
    labs(x = "illumination 2018")+
    ggsave("illumination18.tiff") #Mostly happens on daylight, and dark with street lights are the second common. 

flag.2018 %>% 
    ggplot(aes(x = factor(COLLISION_TYPE)))+
    geom_bar()+
    labs(x = "Collison type 2018")+
    ggsave("Collisontype18.tiff") #First likely at an angle, second likely: rear-end
flag.2018 %>% 
  group_by(COLLISION_TYPE)%>% 
  count(COLLISION_TYPE)
 
  

crash.2018 %>% 
    ggplot(aes(x = factor(RELATION_TO_ROAD)))+
    geom_bar()+
    labs(x = "Relation to road 2018")+
    ggsave("Relation to road 18.tiff") #Mostly on roadway.
flag.2018 %>% 
    ggplot(aes(x = factor(TCD_FUNC_CD)))+
    geom_bar()+
    labs(x = "TCD function 2018")+
    ggsave("TCD18.tiff")
flag.2018 %>% 
  group_by(TCD_FUNC_CD)%>% 
  count(TCD_FUNC_CD)#About 4878/11003=0.44%: device functioning properly, another 5998/11003=0.55%:with no controls 
nrow(flag.2018) #11003 accidents.

```
###Dicussion of part I
For the first part of looking at objective variables, I found the following results:
There are 10676 crashes in 2008, and 11003 crashes in 2018. The increase of crashes is probably due to the increase number of vehicles in Philadelphia.

For the day of the week, Saturdays have more crashes in both 2008 and 2018. The crashes are least likely to happen on Mondays for 2008, Thurdays for 2018.

For the hour of the day, afternoons have more crashes in both years from the histograms. In addition, for 2008, the peak hours for crashes in the moring are 8-10 AM, and the peak hours for crashes in the afternoons are 5-7PM. For 2018, the peak hours for crashes are 8-9AM, 4-8PM. The peak hours are highly correlated to rush hours of traffic in Philadelphia, before and after work.

For  road conditions, most crashes happened on dry roads, but about 20% of crashes happened on wet roads.

For illumination conditions, for both years, about 60% crashes happened under daylight, and about 40% crashes happened in dark street lights, which suggests that illumination condition is a potential issue to be addressed in the future.  

For Collison types, the three most common types and percentages are: 32% crashes happened at an angle, 23% crashes happened at rear-end, 14% crashes hit pedestrians.

For traffic control device (TCD), about 45% of crashes happened where there were no traffic control device, and this could be a potential problem to be addressed in the future.

###Part II
The second part of the project will focus on finding out important variables contributing to fatal or major injury containing crashes. There are 77% crashes resulted in injuries in 2008, the percentage decreased to 69% in 2018, suggesting the improvement. However,the percentage of crashes involving fatal or major injuries did not change much (2.6% in 2008, and 2.4% in 2018).

Machine learning will be applied to both 2008 and 2018 datasets to find out the important variables contributing to fatal or major injury resulting crashes. 

```{r, eval = TRUE}
colnames(flag.2008)
# To see how many car accidents include "MAJOR_INJURY", "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL","FATAL_OR_MAJ_INJ","INJURY" 
nrow(flag.2008) # 10676 car accidents in total
damage.08 <- flag.2008 %>%
             select(CRN,MAJOR_INJURY,MODERATE_INJURY, MINOR_INJURY, FATAL, INJURY,FATAL_OR_MAJ_INJ) 
fig8<-ggplot(data = damage.08,aes(x = factor(INJURY)))+
    geom_bar()+
    labs(x = "Injury 2008") # plot shows accidents cateogrised into no injury vs injury.
ggsave("injury08.tiff",plot=fig8)
damage.08%>%
  filter(INJURY==1)%>%
  count(INJURY) # 8234 injuries
injury.08<-damage.08%>%
  filter(INJURY==1)
damage.08%>%
  filter(MODERATE_INJURY==1)%>%
  count(MODERATE_INJURY) # 1231 moderate injuries
damage.08%>%
  filter(MAJOR_INJURY==1)%>%
  count(MAJOR_INJURY) # 254 major injuries
damage.08%>%
  filter(MINOR_INJURY==1)%>%
  count(MINOR_INJURY) # 4352 minor injuries
damage.08%>%
  filter(FATAL==1)%>%
  count(FATAL) # 4352 minor injuries
injury.08%>%
  filter(FATAL_OR_MAJ_INJ==1)%>%
  count(FATAL_OR_MAJ_INJ)#280 injury containing accidents involve major injuries or fatal events.
ggplot(data=injury.08, aes(factor(FATAL_OR_MAJ_INJ==1)))+
    geom_bar()+
    labs(x = "Fatal or major injury accidents")+ # plot shows accidents involving injury cateogrised into containing fatal or not.
    ggsave("fatalormajor08.tiff")
class(flag.2008$FATAL)
#This excel sheet contains 4 separate column, driver age=16,17,18,19,20, I would like to combine them to be 1 column, and remove the original columns
flag.2008$Driveryoung<-flag.2008$DRIVER_16YR+flag.2008$DRIVER_17YR+flag.2008$DRIVER_18YR+flag.2008$DRIVER_19YR+flag.2008$DRIVER_20YR
flag.2008$Driveryoung<-ifelse(flag.2008$Driveryoung==0,0,1)
flag.2008$DRIVER_16YR<-NULL
flag.2008$DRIVER_17YR<-NULL
flag.2008$DRIVER_18YR<-NULL
flag.2008$DRIVER_19YR<-NULL
flag.2008$DRIVER_20YR<-NULL
colnames(flag.2008)
test <- flag.2008 %>% 
  select(CRN,Driveryoung)
  class(test$Driveryoung)# test the values of Driveryoung
#We would like to focus on fatal or major injuries crashes, so exclue other conditions, like "MAJOR_INJURY"       "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL", "FATAL","INJURY" 
flag.2008$MAJOR_INJURY<-NULL
flag.2008$MODERATE_INJURY<-NULL
flag.2008$MINOR_INJURY<-NULL
flag.2008$PROPERTY_DAMAGE_ONLY<-NULL
flag.2008$INJURY_OR_FATAL<-NULL
flag.2008$FATAL<-NULL
flag.2008$INJURY<-NULL
colnames(flag.2008)

#Remove highly correlated data

library('caret')
#remove columns with only 1 value.
flag.2008$CRASH_YEAR<-NULL
flag.2008$COUNTY<-NULL
flag.2008$MUNICIPALITY<-NULL
flag.2008$LIMIT_65MPH<-NULL


flag.2008.c = cor(flag.2008,use="complete.obs")
hc = findCorrelation(flag.2008.c, cutoff=0.9,verbose = FALSE, names = FALSE,
  exact = FALSE) 
hc = sort(hc)
reduced_Data08 = flag.2008[,-c(hc)]
rd08<-reduced_Data08
df5<-colnames(flag.2008)
df6<-colnames(rd08)
df7<-df6[df6%in%df5]
df5[!df5%in%df7]#exclude"STATE_ROAD","NON_INTERSECTION","TRAIN_TROLLEY","DRINKING_DRIVER","DRUGGED_DRIVER"

#Run random forest or multiple linear regression the flag dataset, to find out the important factors contributing to fatal/major injury accidents.
#Convert 0,1 to yes and no
flag.2008.r<-rd08
flag.2008.r$FATAL_OR_MAJ_INJ<-factor(flag.2008$FATAL_OR_MAJ_INJ, levels=c(0,1), labels = c("no","fatal/major injuries"))
class(flag.2008.r$FATAL_OR_MAJ_INJ)

library(randomForest) 
#Need to remove CRN
flag.2008.r$CRN<-NULL
#Tune random forest by changing mtry
#mtry: Number of random variables collected at each split. In normal equal square number columns.
#f18r.res<-tuneRF(flag.2008.r, flag.2008.r$FATAL_OR_MAJ_INJ,mtry=6, ntreeTry=50, stepFactor=1.5,improve=0,trace=TRUE, plot=TRUE, doBest=FALSE) Did not run this because OOB=0 for both 13 and 19, and the code report error, but I can see the progress, so I know mtry=13 has the lowest OOB, and should be used.
#When mtry=13, the OOB is the lowest, so we use mtry=13 for random forest.
flag08.rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flag.2008.r, ntree = 100, importance = TRUE,mtry=13)  #OOB estimate of  error rate: 3.15%
flag08.rf

flag08.rf$importance
rf.pred.08 <- predict(flag08.rf , flag.2008.r, type = "prob")
head(rf.pred.08)
rf.pred.yes.08<- rf.pred.08[ ,2]


varImpPlot(flag08.rf, type=2)#Plot of import variables vs MeanDecreaseGini scores
imp08 <- flag08.rf $importance
rf08.im<-as.data.frame(head(sort(imp08[ , 4], decreasing = TRUE), n = 83))
rf08.im#Dataframe of import variables vs MeanDecreaseGini scores


#Create a logistic regression model, find variables are significant at p < 0.05.
library(dplyr) #We first need to exclude columns with only 1 value (otherwise cause error in logistic regression)
flagu.08<-rd08 %>% 
  select_if(~n_distinct(.) > 1)
colnames(flagu.08)
flagu.08$FATAL_OR_MAJ_INJ<-factor(flagu.08$FATAL_OR_MAJ_INJ, levels=c(0,1), labels = c("No","fatal/major injuries"))
flagu.08$CRN<-NULL
flag08.glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = flagu.08, family = binomial())
summary(flag08.glm) 
lr08<-data.frame(summary(flag08.glm)$coef[summary(flag08.glm)$coef[,4]<=0.05, 4])
glm.pred.08 <- predict(flag08.glm, flagu.08, type = "response") 
lr08
```
The important variables predicted by random forest and logistic regression for 2008 are summarised in the last part of the code.

Create 10-fold cross validation classification vectors for each model. Obtain AUC values and make an ROC plot that shows ROC curves corresponding to predictive accuracy using the training data as well as the 10-fold cross-validations.

```{r, eval = TRUE} 
#K-fold cross validation for random forest 2008
N = nrow(flag.2008.r) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.rf.08 <- vector(mode = "numeric", length = N)
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flag.2008.r, s != i)
	test <- filter(flag.2008.r, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #RF train/test
	rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = train, ntree = 100,mtry=13)
	rf.pred.curr <- predict(rf, newdata = test, type = "prob") 
	pred.outputs.rf.08[1:length(s[s == i]) + offset] <- rf.pred.curr[ , 2]
  
  offset <- offset + length(s[s == i])
}

library(pROC) 
auc(roc(flag.2008.r$FATAL_OR_MAJ_INJ, rf.pred.yes.08))#AUC=0.8799
auc(roc(obs.outputs, pred.outputs.rf.08))#AUC=0.6649

plot.roc(flag.2008.r$FATAL_OR_MAJ_INJ, rf.pred.yes.08, col = "black") #random forest
plot.roc(obs.outputs, pred.outputs.rf.08, ci = TRUE, col = "blue", add = TRUE)
legend("bottomright", legend = c("2008 RF Training", "2008 RF Cross-Validation"), col = c("black", "blue"), lwd = 1)

#K-fold cross validation for logistic regression 2008

N = nrow(flagu.08) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.glm.08 <- vector(mode = "numeric", length = N) 
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flagu.08, s != i)
	test <- filter(flagu.08, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #GLM train/test
	glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = train, family = binomial(logit))
  glm.pred.curr <- predict(glm, test, type = "response")
  pred.outputs.glm.08[1:length(s[s == i]) + offset] <- glm.pred.curr
  
  offset <- offset + length(s[s == i])
}

library(pROC) 
auc(roc(flagu.08$FATAL_OR_MAJ_INJ, glm.pred.08))#AUC=0.7983
auc(roc(obs.outputs, pred.outputs.glm.08))#AUC=0.7525

plot.roc(flagu.08$FATAL_OR_MAJ_INJ, glm.pred.08,col = c("black"))
plot.roc(obs.outputs, pred.outputs.glm.08, col = "red", add = TRUE)
legend("bottomright", legend = c("2008 logistic regression Training", "2008 logistic regression Cross-Validation"), col = c("black", "red"), lwd = 1)
# Turns out the logistic regression is a better model of predicting fatal or major injury containing crashes in 2008. The common factors related to fatal or major injury containing crashes predicted by both rf and logistic regression are shown in the last part of the code.


```
Based on the ROC plots, logistic regression is a better model for predicting fatal/major injury resulting crashes in 2008.

Fatal/major injury containing crashes for 2018 data
```{r, eval = TRUE} 
colnames(flag.2018)
# To see how many car accidents include "MAJOR_INJURY", "MODERATE_INJURY", "MINOR_INJURY", "PROPERTY_DAMAGE_ONLY","INJURY_OR_FATAL","FATAL_OR_MAJ_INJ","INJURY" 
nrow(flag.2018) # 11003 car accidents in total
damage.18 <- flag.2018 %>%
             select(CRN,MAJOR_INJURY,MODERATE_INJURY, MINOR_INJURY, FATAL, INJURY,FATAL_OR_MAJ_INJ) 
ggplot(data = damage.18,aes(x = factor(INJURY)))+
    geom_bar()+
    labs(x = "Injury 2018")+ # plot shows accidents cateogrised into no injury vs injury.
    ggsave("injury18.tiff",plot=fig8)
damage.18%>%
  filter(INJURY=="Yes")%>%
  count(INJURY) # 7631 injuries
injury.18<-damage.18%>%
  filter(INJURY=="Yes")
damage.18%>%
  filter(MODERATE_INJURY=="Yes")%>%
  count(MODERATE_INJURY) # 1844 moderate injuries
damage.18%>%
  filter(MAJOR_INJURY=="Yes")%>%
  count(MAJOR_INJURY) # 242 major injuries
damage.18%>%
  filter(MINOR_INJURY=="Yes")%>%
  count(MINOR_INJURY) # 3160 minor injuries
damage.18%>%
  filter(FATAL=="Yes")%>%
  count(FATAL) #100 fatal injuries
injury.18%>%
  filter(FATAL_OR_MAJ_INJ=="Yes")%>%
  count(FATAL_OR_MAJ_INJ)#267 injury containing accidents involve major injuries or fatal events.
ggplot(data=injury.18, aes(factor(FATAL_OR_MAJ_INJ=="Yes")))+
    geom_bar()+
    labs(x = "Fatal or major injury accidents 2018")+ # plot shows accidents involving injury cateogrised into containing fatal or not.
    ggsave("fatalormajor18.tiff")
class(flag.2018$FATAL_OR_MAJ_INJ)#factor
```
Now, we clear the data for flag.2018 or flagu.18 (flag.2018, excluding columns with single values)
This excel sheet contains 4 separate column with regards to young drivers, driver age=16,17,18,19,20, I would like to combine them to be 1 column, and remove the original columns
```{r, eval = TRUE} 

library('varhandle')#first convert factors into numerical values
flagu.18$DRIVER_YES6YR <- unfactor(flagu.18$DRIVER_YES6YR)
flagu.18$DRIVER_YES6YR<-ifelse(flagu.18$DRIVER_YES6YR==c("No"),0,1)
flagu.18$DRIVER_17YR<-unfactor(flagu.18$DRIVER_17YR)
flagu.18$DRIVER_17YR<-ifelse(flagu.18$DRIVER_17YR==c("No"),0,1)
flagu.18$DRIVER_18YR<-unfactor(flagu.18$DRIVER_18YR)
flagu.18$DRIVER_18YR<-ifelse(flagu.18$DRIVER_18YR==c("No"),0,1)
flagu.18$DRIVER_19YR<-unfactor(flagu.18$DRIVER_19YR)
flagu.18$DRIVER_19YR<-ifelse(flagu.18$DRIVER_19YR==c("No"),0,1)
flagu.18$DRIVER_20YR<-unfactor(flagu.18$DRIVER_20YR)
flagu.18$DRIVER_20YR<-ifelse(flagu.18$DRIVER_20YR==c("No"),0,1)
flagu.18$Driveryoung<-flagu.18$DRIVER_YES6YR+flagu.18$DRIVER_17YR+flagu.18$DRIVER_18YR+flagu.18$DRIVER_19YR+flagu.18$DRIVER_20YR
flagu.18$Driveryoung<-ifelse(flagu.18$Driveryoung==0,0,1)
flagu.18$Driveryoung<-factor(flagu.18$Driveryoung, levels=c(0,1), labels = c("No","Yes"))
flagu.18$DRIVER_YES6YR<-NULL
flagu.18$DRIVER_17YR<-NULL
flagu.18$DRIVER_18YR<-NULL
flagu.18$DRIVER_19YR<-NULL
flagu.18$DRIVER_20YR<-NULL


#We would like to focus on fatal or major injuries crashes, so exclue other injury conditions and other counts of injuries by using select function, and exclude highly correlated variables and storing into flagse.18 dataframe.
flagse.18<-flagu.18%>%
  select("Driveryoung","INTERSTATE","LOCAL_ROAD_ONLY","WET_ROAD","SNOW_SLUSH_ROAD","ICY_ROAD","SUDDEN_DEER","SHLDR_RELATED","REAR_END","HO_OPPDIR_SDSWP","HIT_FIXED_OBJECT","SV_RUN_OFF_RD","WORK_ZONE","FATAL_OR_MAJ_INJ","INTERSECTION","SIGNALIZED_INT","STOP_CONTROLLED_INT","UNSIGNALIZED_INT","SCHOOL_BUS","SCHOOL_ZONE","HIT_DEER","HIT_TREE_SHRUB","HIT_EMBANKMENT","HIT_POLE","HIT_GDRAIL","HIT_GDRAIL_END","HIT_BARRIER","HIT_BRIDGE","OVERTURNED","MOTORCYCLE","BICYCLE","HVY_TRUCK_RELATED","VEHICLE_FAILURE","PHANTOM_VEHICLE","ALCOHOL_RELATED","UNDERAGE_DRNK_DRV","UNLICENSED","DISTRACTED","CELL_PHONE","NO_CLEARANCE","RUNNING_RED_LT","TAILGATING","CROSS_MEDIAN","CURVED_ROAD","CURVE_DVR_ERROR","LIMIT_65MPH","SPEEDING","SPEEDING_RELATED","AGGRESSIVE_DRIVING","FATIGUE_ASLEEP","DRIVER_65_74YR","DRIVER_75PLUS","UNBELTED","PEDESTRIAN","COMM_VEHICLE","PSP_REPORTED","NHTSA_AGG_DRIVING","DEER_RELATED","ILLUMINATION_DARK","RUNNING_STOP_SIGN","TRAIN","TROLLEY","HIT_PARKED_VEHICLE","FIRE_IN_VEHICLE","VEHICLE_TOWED","HAZARDOUS_TRUCK","MC_DRINKING_DRIVER","DRUG_RELATED","ILLEGAL_DRUG_RELATED","SCHOOL_BUS_UNIT","IMPAIRED_DRIVER","CRASH_MONTH", "DAY_OF_WEEK","HOUR_OF_DAY","WEATHER", "COLLISION_TYPE" , "TCD_TYPE","TCD_FUNC_CD", "DRIVER_5NO_64YR","LIMIT_70MPH", "ANGLE_CRASH", "HORSE_BUGGY", "ATV", "CORE_NETWORK", "OPIOID_RELATED", "LANE_DEPARTURE", "BACKUP_PRIOR", "BACKUP_NONRECURRING" ,"BACKUP_CONGESTION")

```

Then, random forest and logistic regression models were used to study 2018 crash data. 
```{r, eval = TRUE}
#Use random forest to map out import factors related to fatal/major injuries 2018 using flagse.18 dataframe
library(randomForest) 
#Tune random forest by changing mtry
#mtry: Number of random variables collected at each split. In normal equal square number columns.
#f18.res<-tuneRF(flagse.18, flagse.18$FATAL_OR_MAJ_INJ,mtry=6, ntreeTry=100, stepFactor=1.5,trace=TRUE, doBest=FALSE) Do not run this because OOB=0 for both 13 and 19, and the code report error, but I can see the progress, so I know mtry=13 has the lowest OOB, and should be used.
#When mtry=13, the OOB is the lowest, so we use mtry=13 for random forest.
flag18.rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flagse.18, ntree = 200, importance = TRUE, mtry=13) 
flag18.rf #OOB=2.93% for ntree=200
flag18.rf$importance
varImpPlot(flag18.rf, type=2)#Plot of import variables vs MeanDecreaseGini scores

imp18 <- flag18.rf $importance
rf18.im<-as.data.frame(head(sort(imp18[ , 4], decreasing = TRUE), n = 99))
rf18.im#Dataframe of import variables vs MeanDecreaseGini scores

rf.pred.18 <- predict(flag18.rf, flagse.18, type = "prob")
rf.pred.yes.18<- rf.pred.18[ ,2]

#Try to use very important factors to run Random forest again.
flag18rf<-flagse.18%>% 
  select(HOUR_OF_DAY, CRASH_MONTH, DAY_OF_WEEK, COLLISION_TYPE, MOTORCYCLE,WEATHER,LOCAL_ROAD_ONLY, DRIVER_5NO_64YR, ILLUMINATION_DARK, TCD_TYPE, AGGRESSIVE_DRIVING, UNBELTED, PEDESTRIAN, CORE_NETWORK, OPIOID_RELATED, OPIOID_RELATED, WET_ROAD, ALCOHOL_RELATED, DRIVER_65_74YR,TCD_FUNC_CD,INTERSECTION,SPEEDING_RELATED,SPEEDING, HIT_PARKED_VEHICLE, COMM_VEHICLE, CURVED_ROAD, UNSIGNALIZED_INT, SIGNALIZED_INT, IMPAIRED_DRIVER, HIT_TREE_SHRUB, HORSE_BUGGY, SV_RUN_OFF_RD, FATAL_OR_MAJ_INJ)
flag18.rf.2 <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = flag18rf, ntree = 200, importance = TRUE) 
flag18.rf.2 #OOB=3.04% for ntree=200 using those important variables only, but class.error rate increases.


#Create a logistic regression model, find variables are significant at p < 0.05.
library(dplyr) 
flag18lr<-flagse.18 %>% 
  select_if(~n_distinct(.) > 1)
flag18lr$CRASH_MONTH <- as.factor(as.character(flag18lr$CRASH_MONTH))
flag18lr$DAY_OF_WEEK <- as.factor(as.character(flag18lr$DAY_OF_WEEK))
flag18lr$HOUR_OF_DAY <- as.factor(as.character(flag18lr$HOUR_OF_DAY))


flag18.glm <- glm(FATAL_OR_MAJ_INJ ~ ., data = flag18lr, family = binomial(logit))
summary(flag18.glm) 
lr18<-data.frame(summary(flag18.glm)$coef[summary(flag18.glm)$coef[,4]<=0.05, 4])#most important factors are INTERSTATEYes,LOCAL_ROAD_ONLYes,REAR_ENDYes, WORK_ZONEYes,STOP_CONTROLLED_INTYes, HIT_TREE_SHRUBYes,HIT_GDRAILYes, OVERTURNEDYes, MOTORCYCLEYes, BICYCLEYes, ALCOHOL_RELATEDYes, CROSS_MEDIANYes, SPEEDINGYes, DRIVER_65_74YRYes, UNBELTEDYes, PEDESTRIANYes, COMM_VEHICLEYes, ILLUMINATION_DARKYes, RUNNING_STOP_SIGNYes, VEHICLE_TOWEDYes, DRUG_RELATEDYes,CRASH_MONTH, COLLISION_TYPESideswipe (same dir.), HORSE_BUGGYNo, ATVYes, OPIOID_RELATEDYes 
lr18
# Use only significant factors to do the glm again, and create ROC plot.
flag18lr2<-flag18lr%>%
  select(INTERSTATE, LOCAL_ROAD_ONLY,REAR_END, STOP_CONTROLLED_INT, HIT_TREE_SHRUB, HIT_GDRAIL, HIT_BRIDGE,OVERTURNED, MOTORCYCLE, BICYCLE, ALCOHOL_RELATED, CROSS_MEDIAN, SPEEDING,DRIVER_65_74YR, UNBELTED, PEDESTRIAN, COMM_VEHICLE, ILLUMINATION_DARK, RUNNING_STOP_SIGN, DRUG_RELATED, CRASH_MONTH, DAY_OF_WEEK, HOUR_OF_DAY, COLLISION_TYPE, HORSE_BUGGY,ATV, OPIOID_RELATED, FATAL_OR_MAJ_INJ)
flag18.glm2 <- glm(FATAL_OR_MAJ_INJ ~ ., data = flag18lr2, family = binomial(logit))
summary(flag18.glm2) 
lr18.2<-data.frame(summary(flag18.glm2)$coef[summary(flag18.glm2)$coef[,4]<=0.05, 4])
lr18.2#still all those factors are significant.
glm.pred.18 <- predict(flag18.glm2, flag18lr2, type = "response")
 
```


Create 10-fold cross validation classification vectors for each model. Obtain AUC values and make an ROC plot that shows ROC curves corresponding to predictive accuracy using the training data as well as the 10-fold cross-validations.

```{r, eval = TRUE}
#K-fold cross validation for random forest and logistic regression 2018
N = nrow(flagse.18) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.rf.18 <- vector(mode = "numeric", length = N)
pred.outputs.glm.18 <- vector(mode = "numeric", length = N) 
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flagse.18, s != i)
	test <- filter(flagse.18, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
    
  #RF train/test
	rf <- randomForest(FATAL_OR_MAJ_INJ ~ ., data = train, ntree = 200, mtry=13)
	rf.pred.curr <- predict(rf, newdata = test, type = "prob") 
	pred.outputs.rf.18[1:length(s[s == i]) + offset] <- rf.pred.curr[ , 2]
  
  offset <- offset + length(s[s == i])
}



#GLM train/test
N = nrow(flag18lr2) 
K = 10 
set.seed(1234) 
s = sample(1:K, size = N, replace = T) 
pred.outputs.glm.18 <- vector(mode = "numeric", length = N) 
obs.outputs <- vector(mode = "numeric", length = N)
offset <- 0
for(i in 1:K){
	train <- filter(flag18lr2, s != i)
	test <- filter(flag18lr2, s == i) #hold sample
  obs.outputs[1:length(s[s == i]) + offset] <- test$FATAL_OR_MAJ_INJ
  
  #logistic regression
  glm <- glm(FATAL_OR_MAJ_INJ~ ., data = train, family = binomial(logit))
  glm.pred.curr <- predict(glm, test, type = "response")
  pred.outputs.glm.18[1:length(s[s == i]) + offset] <- glm.pred.curr
  
  offset <- offset + length(s[s == i])
}
	
library(pROC) 
#For random forest
auc(roc(flagse.18$FATAL_OR_MAJ_INJ, rf.pred.yes.18))#AUC=0.9997
auc(roc(obs.outputs, pred.outputs.rf.18))#AUC=0.7751

plot.roc(flagse.18$FATAL_OR_MAJ_INJ, rf.pred.yes.18, col = "black") #random forest
plot.roc(obs.outputs, pred.outputs.rf.18, ci = TRUE, col = "blue", add = TRUE)
legend("bottomright", legend = c("2008 RF Training", "2008 RF Cross-Validation"), col = c("black", "blue"), lwd = 1)

#For logistic regression
auc(roc(flag18lr2$FATAL_OR_MAJ_INJ, glm.pred.18))#AUC=0.8415
auc(roc(obs.outputs, pred.outputs.glm.18))#AUC=0.8034
plot.roc(flag18lr2$FATAL_OR_MAJ_INJ, glm.pred.18,col = c("black"))
plot.roc(obs.outputs, pred.outputs.glm.18, col = "red", add = TRUE)
legend("bottomright", legend = c("2008 logistic regression Training", "2008 logistic regression Cross-Validation"), col = c("black", "red"), lwd = 1)
# Turns out the logistic regression is a better model of predicting fatal or major injury containing crashes in 2008. The common factors related to fatal or major injury containing crashes predicted by both rf and logistic regression are summarised in the last part of the code.
```
Find out important factors related to fatal/major injury accidents according to random forest and logistic regression for 2008 dataset.
```{r, eval = TRUE}
#important variables predicted by randomforest in 2008
d1.08<-c("MOTORCYCLE", "LOCAL_ROAD_ONLY", "UNBELTED", "DRIVER_50_64YR", "WET_ROAD", "ILLUMINATION_DARK", "PEDESTRIAN", "LOCAL_ROAD", "AGGRESSIVE_DRIVING", "SV_RUN_OFF_RD", "INTERSECTION", "IMPAIRED_DRIVER", "Driveryoung", "HIT_PARKED_VEHICLE", "HO_OPPDIR_SDSWP", "RUNNING_RED_LT", "SIGNALIZED_INT", "HIT_FIXED_OBJECT", "UNSIGNALIZED_INT", "MC_DRINKING_DRIVER", "SPEEDING_RELATED", "SPEEDING", "REAR_END", "HIT_TREE_SHRUB", "ALCOHOL_RELATED", "CURVED_ROAD", "COMM_VEHICLE", "CROSS_MEDIAN", "NHTSA_AGG_DRIVING", "DRIVER_65_74YR")
#important variables predicted by logistic regression in 2008
d2.08<-c("SHLDR_RELATED", "HIT_TREE_SHRUB", "HIT_POLE", "OVERTURNED", "MOTORCYCLE", "BICYCLE", "RUNNING_RED_LT", "CROSS_MEDIAN", "SPEEDING", "UNBELTED", "PEDESTRIAN", "COMM_VEHICLE", "ILLUMINATION_DARK", "MC_DRINKING_DRIVER")
d3.08<-d2.08[d2.08%in%d1.08]
d3.08 #10 common important variables for 2008 predicted by both models
```
There are 10 common important variables for fatal/major injury accidents predicted by both models for 2008 dataset. Among all the 10 variables, 4 of them are subjective variables:
"RUNNING_RED_LT" 	 	At least one Driver Ran a Red Light 
"SPEEDING" 		 	At least one vehicle was Speeding
"UNBELTED" 			Anyone in crash unbelted?
"MC_DRINKING_DRIVER”    At least one Motorcycle driver has suspected Alcohol Use
"ILLUMINATION_DARK”      Illumination Indicates that the Crash Scene Lighting was Dark
"HIT_TREE_SHRUB" 	 	At Least one Unit Hit a Tree or Shrub 
"MOTORCYCLE" 	 	The crash involved at least one Motorcycle
"PEDESTRIAN" 	 	The crash involved at least one Pedestrian
"CROSS_MEDIAN" 	 	At least one unit Crossed a Median
"COMM_VEHICLE" 	     	Crash has at least one involved Commercial Vehicle
Among all the 10 variables, 4 of them are subjective variables: ran a red light; speeding; unbelted, alchol use. In addition, one very important objective condition is the illumination condition is dark.

```{r, eval = TRUE}
#important variables predicted by randomforest in 2008
d1.18<-c("HOUR_OF_DAY", "CRASH_MONTH", "DAY_OF_WEEK", "COLLISION_TYPE", "WEATHER", "MOTORCYCLE", "LOCAL_ROAD_ONLY", "DRIVER_5NO_64YR", "ILLUMINATION_DARK", "TCD_TYPE", "AGGRESSIVE_DRIVING", "UNBELTED", "CORE_NETWORK", "WET_ROAD", "DRIVER_65_74YR", "PEDESTRIAN", "OPIOID_RELATED", "ALCOHOL_RELATED", "TCD_FUNC_CD", "INTERSECTION", "SPEEDING_RELATED", "COMM_VEHICLE", "UNSIGNALIZED_INT", "HORSE_BUGGY", "SPEEDING", "HIT_PARKED_VEHICLE", "CURVED_ROAD", "SIGNALIZED_INT", "Driveryoung"
)
#important variables predicted by logistic regression in 2008
d2.18<-c("INTERSTATE", "LOCAL_ROAD_ONLY","REAR_END", "STOP_CONTROLLED_INT", "HIT_TREE_SHRUB", "HIT_GDRAIL", "HIT_BRIDGE","OVERTURNED", "MOTORCYCLE", "BICYCLE", "ALCOHOL_RELATED", "CROSS_MEDIAN", "SPEEDING","DRIVER_65_74YR", "UNBELTED", "PEDESTRIAN", "COMM_VEHICLE", "ILLUMINATION_DARK", "RUNNING_STOP_SIGN", "DRUG_RELATED", "CRASH_MONTH", "DAY_OF_WEEK", "HOUR_OF_DAY", "COLLISION_TYPE", "HORSE_BUGGY","ATV", "OPIOID_RELATED")
d3.18<-d2.18[d2.18%in%d1.18]
d3.18 #10 common important variables for 2008 predicted by both models
```
There are 15 common important variables for fatal/major injury accidents predicted by both models for 2018 dataset. 
"OPIOID_RELATED" 	At least one Driver or Pedestrian was tested positive for opioids
"ALCOHOL_RELATED”	At Least one Driver or Pedestrian with reported or suspected Alcohol Use
 "UNBELTED" 		Anyone in crash unbelted?
 "SPEEDING" 		At least one vehicle was Speeding
"ILLUMINATION_DARK” 	Illumination Indicates that the Crash Scene Lighting was Dark
"DRIVER_65_74YR”	At Least one Driver 65-74 Years of Age
"MOTORCYCLE" 		The crash involved at least one Motorcycle
"PEDESTRIAN" 		The crash involved at least one Pedestrian
"COMM_VEHICLE"    	Crash has at least one involved Commercial Vehicle
"LOCAL_ROAD_ONLY”	The crash involved only Local Roadway
"CRASH_MONTH"  	Month when the crash occurred
"DAY_OF_WEEK”	Day of the Week code when crash occurred
 "HOUR_OF_DAY” 	The hour of Day when the crash occurred
"COLLISION_TYPE" 	Collision category that defines the crash
"HORSE_BUGGY" 	At least one Horse and Buggy Unit involved
Among all the 15 variables, 6 of them also appear as 2008 common importare variables.There are two more subjective variables predicted in 2018: opioids, and alcohol use in drivers or pedestrians. In addition, at Least one Driver 65-74 Years of Age is also an common importare variable, warning for drivers over 65 years old. Moreover, illumination condition remains to be a important common variable, suggesting this issue should really be addressed in the future by the Philadelphia government. 


###Conclusion
In conclusion, the subjective important variables related to fatal or major injury containing crashes are ALCOHOL_RELATED, SPEEDING, UNBELTED, OPIOID_RELATED, RUNNING_RED_LT, MC_DRINKING_DRIVER. In addition, ILLUMINATION_DARK is an important factor contributing to to fatal or major injury containing crashes, and this issue should be addressed by the government.

### Acknowledgements
	I would like to thank the following individuals for their guidance and support throughout the generation of this report. 

* Blanca Himes, PhD
* Emma Zheng, MS
* Erin Schnellinger, MS
* Elisia Tichy, PhD


