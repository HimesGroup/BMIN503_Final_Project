---
title: "Using Logistic Regression to Surface Gamesmenship By SNF Aministrators"
author: "David Roberts"
output: 
  html_document:
    theme: default
    highlight: tango
---

## Introduction
In this part, I investigate claims in a [2021 NyTimes article](https://www.nytimes.com/2021/03/13/business/nursing-homes-ratings-medicare-covid.html) that some SNF's have padded their public star ratings by exploiting a nuance in staffing ratio calculations. Studies illustrate that quality ratings are influential in consumer choice, with each [additional star correlating to higher profit margins](https://www.ahcancal.org/News-and-Communications/Fact-Sheets/FactSheets/2022%20State%20of%20the%20SNF%20Industry%20Report.pdf). Therefore, nursing home administrators have a strong incentive to maximize their scores, which has predictably resulted in shenanigans. One Florida state inspector plead guilty to accepting [$500k in bribes](https://www.justice.gov/opa/pr/former-florida-state-health-care-administration-official-sentenced-more-four-years-prison) for information regarding when an inspection would occur. Tellingly, ~800 of 15k nursing homes logged their highest staffing ratios on the day of their ["surpise" inspection](https://www.nytimes.com/2021/03/13/business/nursing-homes-ratings-medicare-covid.html). *I do not wish to imply that nursing homes are, in general, acting unethically.* Rather, as with any public ratings system and "value-based care" more generally, administrators respond financial incentives to maximize quality ratings. Keep in mind, the Nytimes has incentives to sensationalize, focusing on fraudsters to the detriment of other facilities.

The exploited rule in this case, is that nursing staff "with administrative duties" may be counted alongside direct care nursing staff in calculating staffing ratios. Per the CMS data payroll submission manual, an "LPN with administrative duties" is defined as follows:

> ...nurses whose principal duties are spent conducting administrative functions. For example, the LPN Charge Nurse 
is conducting educational/in-service, or other duties which are not considered to be direct care giving.

From the perspective of payroll, such a nurse may be engaged in both patient care and administrative tasks, depending on what they spend the most time on. The category is unfortunately ambiguous.

## Methods
To assess whether nursing homes use "admin duties" to pad their quality scores, I merged publicly available, daily  [payroll-based staffing journals](https://data.cms.gov/quality-of-care/payroll-based-journal-daily-nurse-staffing) submitted to CMS with the facility level dataset created in `report_pt2.Rmd`. I construct an "administrative intensity" metric as the percentage of total nursing hours in a given time period which are reported "with admin duties". Finally, using a case / control design, I use logistic regression to model if a large year-over-year change in admin intensity is predictive of year-over-year staffing rating increases.

*Case Definition*

I define a case as a nursing home with a year-over-year (YOY) change in staffing star rating from $\lt4$ to $\geq4$. This reflects my assumption that shenanigan-prone facilities might play games with CMS's overall star rating methodology. Specifically, in the time period studied (2017-2021), a facility with $\geq4$ staffing rating received a +1 star increase to their overall star rating. As an example, the facility described in the [Nytimes article](https://www.nytimes.com/2021/03/13/business/nursing-homes-ratings-medicare-covid.html), Sun Terrace in FL (CCN = 105319), increased their staffing star rating from $2\rightarrow5$ between 2018 and 2019. This yielded an extra star in their overall star rating. This increase coincided with a 18 pct point increase in admin intensity, from 6% to 24%. *Note, this case definition is very broad, and undoubtedly includes a large number of facilities which are not playing games.* My hypothesis is that *anomalous increases in admin intensity* are predictive of cases.

*Control Definition*
Controls are all YOY observations which:
  1. Do not fit the case definition
  2. Have complete year over year observations for both daily payroll based admin intensity and publically reported staffing ratios. Note, when a facility misses or incorrectly reports staffing data, their staffing rating drops to 1. I have omitted these observations because it implies that CMS rejected the data I would use to calculate administrative intensity. *Note, I think facilities without full YOY observations are less likely to play staffing games as they are likely under increased scrutiny by regulators.*
  
### Data Exploration and Cleaning

For details on loading payroll based journal, see `transform/nhc_pbj_nurse_staffing.py`

```{r, message=F, warnings=F}
library(here)
library(dplyr)
load(here("data", "interim", "snf_provider_info.Rda"))
pbj <- read.csv(here("data", "interim", "pbj_facility_level.csv"))
```

Create YOY staffing ratios and quality star changes using existing dataset and add the case definition.

```{r}
staffing_next_yr <- select(q4_snf_data, c("federal_provider_number", "staffing_rating", "collection_yr", "adjusted_rn_staffing_hours_per_resident_per_day", "adjusted_total_nurse_staffing_hours_per_resident_per_day", "ownership_type", "average_number_of_residents_per_day", "chow", "continuing_care_retirement_community","average_number_of_residents_per_day",
"overall_rating", "health_inspection_rating", "qm_rating"))
staffing_next_yr$collection_yr = staffing_next_yr$collection_yr - 1

# left join assigns x and y to shared column names
# x suffix = y1
# y suffix = y2
snf_level_data <- q4_snf_data %>%
  left_join(staffing_next_yr, by=c("federal_provider_number", "collection_yr"))
staffing_yoy <- select(snf_level_data,
    c("federal_provider_number", "staffing_rating.x", "collection_yr",
       "staffing_rating.y","adjusted_rn_staffing_hours_per_resident_per_day.x", "adjusted_total_nurse_staffing_hours_per_resident_per_day.x",
      "adjusted_rn_staffing_hours_per_resident_per_day.y", "adjusted_total_nurse_staffing_hours_per_resident_per_day.y",
      "ownership_type.x", "ownership_type.y", "average_number_of_residents_per_day.x",
      "average_number_of_residents_per_day.y",
      "chow.y", "continuing_care_retirement_community.y",
      "average_number_of_residents_per_day.x", "average_number_of_residents_per_day.y",
      "overall_rating.x", "overall_rating.y", 
      "health_inspection_rating.x", "health_inspection_rating.y", 
      "qm_rating.x", "qm_rating.y")) %>%
  
  # remove cases where no staffing rating is reported...
  filter(is.na(staffing_rating.y) == FALSE & is.na(staffing_rating.x) == FALSE) %>%
  
  # add YOY columsn
  mutate(overall_star_change=overall_rating.y - overall_rating.x) %>%
  mutate(inspect_star_change=health_inspection_rating.y - health_inspection_rating.x) %>%
  mutate(had_bad_inspection=ifelse(inspect_star_change < 0, TRUE, FALSE)) %>%
  mutate(qm_star_change=qm_rating.y - qm_rating.x) %>%
  mutate(staff_star_change=staffing_rating.y - staffing_rating.x) %>%
  mutate(rn_hr_prpd_change=adjusted_rn_staffing_hours_per_resident_per_day.y - adjusted_rn_staffing_hours_per_resident_per_day.x) %>%
  mutate(total_hr_prpd_change=adjusted_total_nurse_staffing_hours_per_resident_per_day.y - adjusted_total_nurse_staffing_hours_per_resident_per_day.x) %>%
  
  # add case control definition
  mutate(status=as.factor(
           ifelse(staff_star_change > 0 & staffing_rating.x < 4 & staffing_rating.y >=4,
          "Case", "Control")))
```

Assess missingness... 

```{r}
staffing_yoy %>% count(status, staff_star_change)
```

Roughly 3k observations missing at least one staffing ratio observation

```{r}
filter(staffing_yoy, is.na(rn_hr_prpd_change)) %>% count(status, staff_star_change)
```

```{r}
# drop them
staffing_yoy.complete <- filter(staffing_yoy, !(is.na(rn_hr_prpd_change) | is.na(total_hr_prpd_change)))
```

Create corresponding PBJ (Payroll Based Journal) dataframe. Filter to Q4 observations. I learned in part 1 that this minimizes missing data.

```{r}
pbj.q4 <- pbj %>%
  mutate(year = as.integer(substr(CY_Qtr, 1, 4))) %>%
  mutate(quarter = as.integer(substr(CY_Qtr, 6, 6))) %>%
  filter(quarter==4) # align with staffing metrics in nursing home compare dataset.

pbj.q4.yoy <- pbj.q4 %>%
  mutate(year = year + 1) %>%
  left_join(pbj.q4, by=c("PROVNUM", "year")) %>%
  filter(!is.na(quarter.y)) %>%
  
  # calculate YOY admin intensity change
  mutate(admin_change_pct_pt = (AdminIntensity.y - AdminIntensity.x) * 100)
```

Merge staffing YOY and payroll based journal datasets
55939 obs in pbj.q4.merged
55473 obs in staffing_yoy 
54807 obs in merged dataframe

```{r}
library(stringr)
staffing_yoy.complete$year = staffing_yoy.complete$collection_yr + 1
pbj.q4.yoy <- rename(pbj.q4.yoy, federal_provider_number = PROVNUM )
final.df <- staffing_yoy.complete %>%
  inner_join(pbj.q4.yoy, by=c("year", "federal_provider_number"))
final.df <- subset(final.df, select=-c(
  collection_yr,
  CY_Qtr.x,
  CY_Qtr.y,
  quarter.x,
  quarter.y
))

# Create final data frame for logistic regression model
final.df$year_factor <- as.factor(final.df$year)
final.df$status <- factor(final.df$status, levels=c("Control", "Case")) # easier to interpret
final.df <- final.df %>%
  mutate(for_profit = ifelse(str_detect(ownership_type.y, 'For profit'), TRUE, FALSE)) %>%
  mutate(top_5_percentile_admin_change = ifelse(admin_change_pct_pt > quantile(admin_change_pct_pt, .95), TRUE, FALSE)) %>%
  mutate(top_1_percentile_admin_change = ifelse(admin_change_pct_pt > quantile(admin_change_pct_pt, .99), TRUE, FALSE)) %>%
  mutate(staffing_rating_seq=paste(staffing_rating.x, staffing_rating.y))
```

Interesting, cases are four times as common in YOY ending in 2020 than 2018. This is likely due to discharge patterns adapting to the COVID-19 pandemic. As discussed in in part 1, fewer residents with higher acuity, as less acute cases were diverted home to mitigate infection risk.

```{r}
library(gtsummary)
select(final.df, 
      c(staffing_rating.x, staff_star_change, status, year_factor),
      c(admin_change_pct_pt)
      ) %>% tbl_summary(by=year_factor)
```


The table below groups by the most common "sequences", i.e. going from $3\rightarrow4$ stars YOY, for each year. Any sequence starting $\lt4$ and ending $\geq4$ is classified as a case. Large jumps which do not result from missing data, i.e. $1\rightarrow4$ or $2\rightarrow5$ are very rare.

```{r}
select(final.df, 
      staffing_rating_seq, year_factor) %>% tbl_summary(by=year_factor)
```
Admin intensity change is centered on zero in all years. The distributions looks fairly symmetric, with major outliers on both negative and positive sides. 

```{r}
library(ggplot2)
plt <- ggplot(final.df, aes(x=year_factor, y=admin_change_pct_pt)) + geom_violin() +
  labs(x="Ending Year (YOY)", y="Pct Point Change in Admin Intensity", title="Distribution of Change in Admin Intensity, By Year") + theme_classic()
plt
```

Across all years, the distribution of YOY admin intensity change has a positive skew: 1.57

*I will define admin intensity "spikes" as follows:*

* Top 5th percentile is a +4.5 pct point increase (in dotted purple below)
* Top 1st percentile is a +8 pct point increase (in solid red below)

```{r, warning=F}
library(moments) 
plt <- ggplot(final.df, aes(x=admin_change_pct_pt)) + geom_histogram(bins=100) + xlim(-15, 15) + geom_vline(xintercept = quantile(final.df$admin_change_pct_pt, .95), linetype ="dashed", color="purple", width=2) + geom_vline(xintercept = quantile(final.df$admin_change_pct_pt, .99), linetype = "solid", color="red", width=2) + labs(title="Histogram of Pct Point Change in Admin Intensity (YOY)", x="Pct Point Change in Admin Intensity")
#median(final.df$admin_change_pct_pt)
plt
# skewness(final.df$admin_change_pct_pt) # 1.57
```

Reviewing admin intensity by change in staffing star rating YOY is a bit confusing. Recall, by definition, cases must have a positive YOY change in staffing star rating. The vast majority of admin intensity spikes occur with no staffing change, which is expected considering the base rates of each sequence. The violin plot is a bit deceptive in that the tracing does not include major outliers under the curve. For instance, I've highlighted the infamous Sun Terrace admin spike as a yellow circle (from Nytimes article).

```{r, warning=F}
#plt <- ggplot(final.df, aes(x=admin_change_pct_pt, y=staff_star_change, color=status)) + geom_point() + xlim(-25, 25)
plt <- ggplot(final.df, aes(x=admin_change_pct_pt, y=staff_star_change, color=status)) + geom_violin() + geom_point(data=filter(final.df, federal_provider_number == "105319" & year == 2019), aes(x=admin_change_pct_pt, y=staff_star_change), colour="black", fill="yellow", shape=21, size=5) + xlim(-25, 25) + labs(x = "Admin Itensity Change (YOY)", y="Staffing Star Rating Change (YOY)", title="Admin Intensity Change by Staffing Star Rating Change") + geom_vline(xintercept = quantile(final.df$admin_change_pct_pt, .95), linetype ="dashed", color="purple", width=2) + geom_vline(xintercept = quantile(final.df$admin_change_pct_pt, .99), linetype = "solid", color="red", width=2)
geom_vline(xintercept = quantile(final.df$admin_change_pct_pt, .99), linetype = "solid", color="red", width=2)
plt
```

```{r}
#plt <- ggplot(final.df, aes(x=as.factor(staffing_rating.x), y=admin_change_pct_pt)) + geom_violin()
#plt
```

## Results
The logistic regression results suggest there is a relationship between a top 5th percentile change in admin intensity and a jump in staffing ratings, increasing the odds ratio by a factor of 1.64, independent of the year and various controls. This relationship strengthens using the top 1st percentile change, increasing the odds ratio by a factor of 2.34.

An explanation of included variables is below:

| *Coefficient Name* | *Type* | *Description* |
| ---------- | -- | ----------- |
| top_5_percentile_admin_change | Var of Interest | SNF has a top 5 percentile increase in administrative intensity in the YOY time period |
| for_profit | Control | SNF is for profit in  |
| avg_res_pd | Control | Average number of residents in first period of YOY time period |
| adj_total_nurse_hrs_prpd | Control | Casemix adjusted staffing ratio for all nursing roles |
| chow | Control | Whether the facility had a change of ownership in during YOY time period |
| year_factorYYYY | Control | Year level fixed effects to account for violated independence assumptions |

Note, the intercept in this case represents:

1. Not top percentile admin change
2. Not "for profit"
4. Without a change of ownership in the YOY time period
3. YOY period ending in 2018

```{r}
options(max.print=50)
final.df <- rename(final.df, adj_total_nurse_hrs_prpd = adjusted_total_nurse_staffing_hours_per_resident_per_day.x, avg_res_pd = average_number_of_residents_per_day.x)
top5pct.fit <- glm(status ~ top_5_percentile_admin_change + for_profit + avg_res_pd + adj_total_nurse_hrs_prpd + chow.y + year_factor, family=binomial, final.df)
summary(top5pct.fit)
exp(cbind(OR = coef(top5pct.fit), CI = confint(top5pct.fit)))
```

```{r}
top1pct.fit <- glm(status ~ top_1_percentile_admin_change + for_profit + avg_res_pd + adj_total_nurse_hrs_prpd + chow.y + year_factor, family=binomial, final.df)
summary(top1pct.fit)
exp(cbind(OR = coef(top1pct.fit), CI = confint(top1pct.fit)))
```

These statistics requires *very careful interpretation* considering how case / controls were created. Unlike the NYTimes investigators, I did not perform due diligence at the facility level to separate helpful beneficial staffing improvements from "funny business". *Cases include everyone who made staffing ratio improvements*, and I assume the _vast majority_ occurred for the right reasons.

I like to think of the coefficient on admin intensity "spikes" (top_1_percentile_admin_changeTRUE) as a Rorschach test surfacing the interpreter's bias regarding "administration".

To steel-man both sides, an admin-proponent might argue that administration holds direct care staff accountable to higher quality, makes their work more efficient, and also participates in direct care when necessary. Therefore, increased admin intensity does not harm, and may support the patient care function. *Plus, if patients receive less attention, this should be visible in other quality scores going down!*

But... An admin-skeptic might argue that admin time is principally focused on curating data related to public-facing quality scores. Except for claim-based measures, quality scores are largely derived from documentation. If admin duties primarily comprise parsing / cleaning up documentation, admin hours might enhance quality scores, without affecting, or at the expense of direct patient care. 

The truth probably lies in between, and discerning which case is occurring would require observing people work. Anecdotally, one SNF expert informed me that facilities find ways to congregate residents around the nursing station, such that nurses with admin duties can multi-task.

Whatever your posture, I suggest seeking alternative sources in addition to CMS star ratings!
