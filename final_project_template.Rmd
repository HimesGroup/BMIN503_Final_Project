---
title: "BMIN503/EPID600 Project Template - Final Assignment"
author: "Aniket Patel"
output:
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  


***
### Overview
Indego is a public bike sharing system in Philadelphia, PA that has docking stations throughout the city. Users can sign up for various types of passes such as annual, monthly, daily, hourly, etc. Registered users are those who have either monthly or yearly passes, and casual users are those without monthly or yearly passes. All users are able to rent a bike at any location and return their bikes at any location in the city. Predicting the demand for bikes throughout the city is difficult. Improper distribution and availability of bikes across the city, due to poor predictions, leaves some stations with not enough bikes and others with too many. A full station will not allow users to return their bike, forcing them to look for another station that has available slots. On the other hand, an empty station will not have any bicycles to rent and users will have to move to another station or simply use another method of transportation. These inconveniences can be frustrating for customers and are often the reason many switch to other transportation methods. This, consequently, negatively affects business performance. 

The goal of my project is to develop a model using random forests to accurately predict bike demand by the hour throughout Philadelphia. A major part of this project will be exploratory, as Indego is unaware of how different variables themselves affect bike demand. The data used to develop my model was obtained by Dr. Michelle Johnson, who is my research PI. Although we work in a rehab robotics lab, she encouraged me to pursue this project because she has personally experienced problems with Indego and the lack of bikes. Moreover, her friend is on the executive leadership team of Indego and was able to provide data not publicly available. 

In addition to Dr. Johnson, I also spoke with Mr. Jefferey Babin and Dr. Kacey Cullen. Dr. Cullen utilizes deep learning in his lab, and Mr. Babin has industry experience with data analytics. All three gave me insight and helped me with different approaches as I tackled the given problem. Mr. Babin explained to me the business problem of not having accurate predictions. This conversation justified the need for this type of project. Dr. Cullen explained that using deep learning was not approach for this project. And Dr. Johnson helped me get started by obtaining the data set and showing me how to go about selecting different features and what should be shown in the exploratory analysis. 

Link to GitHub: https://github.com/acp812/BMIN503_Final_Project


### Introduction 
A bicycle sharing system is a service that allows multiple users to share the use of bicycles distributed in various docking stations along a city. Users can borrow a bike at a station and return it in a different station as previously mentioned. The data generated by these systems makes them attractive for researchers of many fields because the duration of travel, departure location, arrival location, and time elapsed are explicitly recorded. Simultaneously meeting the demand for bikes throughout a city while not having an abundant supply is a problem. There is currently no established prediction model used by Philadelphia’s Indego to forecast demand. Indego, as I learned from a conversation with Dr. Johnson, solely uses population metrics to determine how many new bikes they should roll out each new quarter. There are several other predictors that can be utilized for more accurate predictions, such as weather, time of day, number of registered or casual users, etc. Utilizing this data and these variables to create a model to predict bike demand will address Indego’s problem of not meeting demand. Indego has not looked at how the different variables their system records affect the number of bikes being used; therefore, a major part of this project will be exploratory, aiming to describe how different variables relate to number of bikes being used. This is significant because knowing this information will help Indego retain and gain more customers by meeting the demand. Also, a major business expense for Indego is bike replacement, which happens due to rust or theft, and maintenance. If Indego only put out as many bikes as they need, then a lot of these problems pertaining to theft, bike degradation, etc. can be avoided. Overall, knowing the information this project aims to provide will lead to better business outcomes for Indego. 

Bike sharing systems can be considered as a sensor network, making it a powerful tool for studying mobility in a city. In this sense, urban study groups and professionals can use this information to better plan different areas of the city. The model developed in this project will only look at overall bike demand in Philadelphia, and not be location-specific. Moving forward, however, the process and model derived can be extrapolated to be location-specific. This will allow areas that have a great lack of bikes obtain more of them with quantitative justification. Several neighborhoods in Philadelphia do not have a docking station readily accessible to them, and therefore they rely on other modes of transportation. This invariably leads to decreased health outcomes of those neighborhoods, as Dr. Johnson explained to me when describing the motivation to do this project. This project, therefore, can also be used to address public health issues. This project can be indeed taken to be interdisciplinary and the possible downstream studies are endless. 


### Methods
First, I wanted to clean the data and see if there is anything missing. The data given to me by Dr. Johnson, who received it from her contact in Indego, was already split into two groups. I looked at both datasets and they contained the same information but one had the number of registered, casual and total users while the other did not. The Indego team, from what I was told, wanted me to make a model using all the data and predict what the number of registered, casual and total users would be for the incomplete data set. The incomplete data set was therefore my ‘test’ set, and the complete data set was my ‘train’ set. In order to test the accuracy of my model, I used the complete data set and split it. I used one portion as a pseudo-test data set and the other as pseudo-train data set. The model was confirmed to be accurate, and was used for the actual train and test data sets.

A major portion of this project is exploratory. As this was my first experience with R, I wanted to get a good understanding of making various graphs. I started with histograms of the different variables in the data sets, and looked their frequencies. I looked at how temperature was related to apparent temperature and wind speed. I filtered the outliers to make everything easier. I then made a chart that contained different graphs based on the type of day and weather. All of this is explained in the code. I also made several boxplots to show different distributions of factored data. It is important to note that registered and casual users show different behaviors and therefore were analyzed separately. This approach of analyzing registered and casual users separately was suggested by Dr. Johnson’s Indego contact. 

From the exploratory analysis, I was able to discern features that I could use in the model. I also looked more quantitatively at different demand drivers. Again, this is explained in the code. Using ‘rpart’ I was able to get different decision tree nodes and categorize the data accordingly. I did this for a few variables. I eventually made the random forest model and outputted the final test predictions to be given to Indego for evaluation. I finished on Dec. 12, 2018 and will submit this assignment to Indego later this week for their assessment of how accurate my model was. As stated earlier, I quickly validated my model and the validation showed the model to be accurate. 


### Results
All tables and plots are shown in the code. Comments regarding them will be seen under the tables, graphs, etc. 

Conclusions and takeaways are stated in the code as comments but I will quickly summarize a few points here. Casual users, or those without monthly or yearly passes, rent bikes more on the weekends. Registered users rent bikes more during the working days of the week. Majority of the bike users are registered users and the total user data shows that demand by hour of day is very similar to the registered user’s demand curve. This suggests, that if bikes are to be taken in or off the streets, for repair or maintenance checks, it is best to do these activities on weekends when you are not affecting registered users, who are your primary source of revenue. Angering the registered users will make them switch to other means of transportation and should be avoided. There are several other conclusions that can be drawn, but for the sake of brevity I will discuss the overall takeaway. The demand of bikes is greatly influenced by time of day and as much by other factors one may think, such as temperature. The model obtained takes into account several variables but it was shown that time of day that by far the greatest effect on demand. 

The problem this project attempted to address was predicting bike demand. From the exploratory analysis, we are able to see how different variables affect the demand. Using these variables, we were able to construct a random forest model that can be used to predict future demand. Knowing this, Indego will be able to realize better business outcomes and hopefully influence more people to use their system, thereby creating a healthier city. Future directions include developing an even more robust model by taking into account other variables from data sets that were not available to me but exist. Looking at specific locations and assessing demand can help bring an accurate number of bikes to areas that need them. This will improve intra-city bike dispersion and lead to a happier user group.



```{r, eval=TRUE}

####DATA IMPORT AND CLEANING####

library(ggplot2)
library(ggdendro)
library(lubridate)
library(plyr)
library(dplyr)
library(randomForest)
library(scales)
library(rpart)
library(rattle) #good library for visualization
library(rpart.plot)
library(RColorBrewer)

#import data from computer file
#setwd("C:/Users/Aniket Patel/Desktop/BMIN-503 Data Science/Final Project")

#created two sets of train and test to help be be better organized when creating different graphs, tables, etc. 
test <- read.csv(file="C:/Users/Aniket Patel/Desktop/BMIN-503 Data Science/Final Project/test2a.csv", header=TRUE, sep=",")
train <- read.csv(file="C:/Users/Aniket Patel/Desktop/BMIN-503 Data Science/Final Project/train2a.csv", header=TRUE, sep=",")

test2 <- read.csv(file="C:/Users/Aniket Patel/Desktop/BMIN-503 Data Science/Final Project/test2a.csv", header=TRUE, sep=",")
train2 <- read.csv(file="C:/Users/Aniket Patel/Desktop/BMIN-503 Data Science/Final Project/train2a.csv", header=TRUE, sep=",")

#add appropriate columns to 'test2' so it matches 'train2' and combine both columns
test2$registered=0
test2$casual=0
test2$count=0
data2=rbind(train2,test2)  #combine both data sets

#preview what 'data2' looks like
str(data2)
summary(data2)

#see if there are any missing values...there are none
table(is.na(data2))
table(is.na(test))
table(is.na(train))

#change labels to make it easier to understand
weatherlabels <- c("nice weather", "cloudy/misty", "light weather",
                   "heavy weather")
seasons <- c("spring", "summer", "fall", "winter")
daysoftheweek <- c("Monday","Tuesday","Wednesday","Thursday","Friday",
                   "Saturday","Sunday")

train <- mutate(train, 
                datetime = ymd_hms(datetime),
                season = factor(season, levels=1:4, labels=seasons),
                holiday = as.logical(holiday),
                workingday = factor(workingday, levels=c(1,0), 
                                    labels=c("Workday","Holiday/Weekend")),
                weather = factor(weather,levels=1:4, labels=weatherlabels),
                dayofweek = factor(weekdays(datetime), levels=daysoftheweek),
                timeofday = hour(datetime))

test <- mutate(test, 
                datetime = ymd_hms(datetime),
                season = factor(season, levels=1:4, labels=seasons),
                holiday = as.logical(holiday),
                workingday = factor(workingday, levels=c(1,0), 
                                    labels=c("Workday","Holiday/Weekend")),
                weather = factor(weather,levels=1:4, labels=weatherlabels),
                dayofweek = factor(weekdays(datetime), levels=daysoftheweek),
                timeofday = hour(datetime))

#there is one data point with heavy weather, while the majority are light weather points. To make everything easier, we will replace the heavy weather point with a light weather one. To do this we are assumming the bike rentals during heavy weather will be negligible, which is a safe assumption. 

print(which(train$weather == "heavy weather"))
print(which(test$weather == "heavy weather"))
print(train[which(train$weather == "heavy weather")+(-5):6,c("datetime","weather", "temp", "count")]) #did this to look at weather of days around heavy weather day; interesting to see the count is still 164 (relatively high) despite the heavy weather; nevertheless, my faculty advisor suggested I interpolate heavy weather as light weather for graphing purposes

train$weather[train$weather == "heavy weather"] = "light weather"
test$weather[test$weather == "heavy weather"] = "light weather"

#there is a variable 'atemp' which is apparent temperature. I want to explore the relationship of this variable with others

## atemp vs. temperature, humidity, windspeed
gg <- ggplot(train, aes(temp, atemp, color=humidity, size=windspeed)) +
    geom_point() +
    scale_colour_gradient(low="red",high="blue") + 
    labs(title="Effects on Apparent Temperature") +
    labs(x="tempurature (°C)") +
    labs(y="'Apparent Temperature (°C)") #temperature sensors were European and therefore recorded in Celcius
print(gg)


#one can see outliers on the bottom right hand corner; it is easy to see that all these correspond to one 'atemp' value and all occured on the same day; moving forward we will assume these values of atemp to be temp (atemp=temp) which will help keep everything clean 

#this shows the group of outliers 
select(filter(train, atemp < 15, temp > 24), datetime, weather, temp, atemp, humidity, windspeed) #24 points in total, all on Aug. 17, 2017
select(filter(test, atemp < 15, temp > 24), datetime, weather, temp, atemp, humidity, windspeed) #24 points in total, all on Aug. 17, 2017

# replace atemp with temp for the outliers identified above
train <- mutate(train, atemp=ifelse((atemp < 15) & (temp > 24), temp, atemp))
test <- mutate(test, atemp=ifelse((atemp < 15) & (temp > 24), temp, atemp))


#check to see if outliers were removed...they were
gg <- ggplot(train, aes(temp, atemp, color=humidity, size=windspeed)) +
    geom_point() +
    scale_colour_gradient(low="red",high="blue") + 
    labs(title="Effects on Apparent Temperature") +
    labs(x="tempurature (°C)") +
    labs(y="'Apparent Temperature (°C)") #temperature sensors were European and therefore recorded in Celcius
print(gg)


```



```{r, eval=TRUE}
####EXPLORATORY ANALYSIS####

#look at histograms to see distributions of different variables
hist(data2$season, xlab = "Season", main = "Histogram of Season")
hist(data2$weather, xlab = "Weather", main = "Histogram of Weather")
hist(data2$humidity, xlab = "Humidity", main = "Histogram of Humidity")
hist(data2$holiday, xlab = "Holiday", main = "Histogram of Holiday")
hist(data2$workingday, xlab = "Working Day", main = "Histogram of Working Day")
hist(data2$temp, xlab = "Temperature", main = "Histogram of Temperature")
hist(data2$atemp, xlab = "Apparent Temp.", main = "Histogram of Apparent Temp.")
hist(data2$windspeed, xlab = "Wind Speed", main = "Histogram of Wind Speed")

#All seasons have relatively equal frequences.
#Weather 1 (clear) has the highest contribution in this data set. 
#Temperature, apparent temperature, humidity and wind speed are normally distributed.


#get a more quantitative feel for distribution
prop.table(table(data2$weather)) #Weather 1 has 66% total distribution 
prop.table(table(data2$holiday)) #98% of data recorded was on a non-holiday
prop.table(table(data2$workingday)) #68% of total distribution is of working days 

#convert the discrete variables in combined dataset (data2) to factors
data2$season=as.factor(data2$season)
data2$weather=as.factor(data2$weather)
data2$holiday=as.factor(data2$holiday)
data2$workingday=as.factor(data2$workingday)

#quick check to make sure conversion worked...it did
str(data2)

#put everything together and see the effect of time of day, weather, and working/non-working days...using original data sets that are not combined
#color code obtained from Dr. Johnson (faculty mentor)
colors.tempurature <- c("#5e4fa2", "#3288bd", "#66c2a5", "#abdda4", "#e6f598",
                        "#fee08b", "#fdae61", "#f46d43", "#d53e4f", "#9e0142")

gg5 <- ggplot(train, aes(timeofday, count, color=9/5*temp+32)) + #convert the Celcius temperature to Farenhiet because we are in America
    facet_grid(workingday ~ weather) +
    geom_point() +
    geom_smooth() +
    theme(plot.title = element_text(size = rel(1.5))) +
    labs(title="Daily Bike Rental Demand \nPer Time of Day, Work/Nonwork day, and Weather") + 
    labs(x="Hour of Day") + 
    labs(y="Bike Demand") +
    scale_colour_gradientn("Temp (°F)", colours=colors.tempurature)

print(gg5)

#There are several takeaways from this:
#There are peaks of demand around 7-8am and 5-6pm for the workday graphs across all weather types.
#Weather does not really affect the shape of the demand curve with respect to time of day, just the overall magnitude. Even then, the overall magnitude is not too affected going from nice weather to cloudy/misty weather, but there is some drop-off when looking at light weather demand for both workdays and holiday/weekends.
#There is a fairly even distribtion of different temperatures across all weather types. In other words, weather type does not affect average temperature too much.
#Distinction of demand curves between working and holiday/weekend days can be explained by user type. Casual users are more likely to use bikes when they are free to explore the city or for other leisure activies, which will typically be done on weekends/holidays. Registered users are more likely to stick with a schedule and use the bikes to go to work and come home from work. This explains the observed peaks in the workday demand curves. 


#Further breakdown and analysis of above graph
#create hour variable in combined dataset (data2) and turn it into a factor...do this by extracting the hour from 'datetime'
data2$hour = substr(data2$datetime,12,13)
data2$hour=as.factor(data2$hour)

#split combined data set (data2) to make training and test data sets as they were received (will not use previous 'test' and 'train' datasets for organizational purposes)
train2=data2[as.integer(substr(data2$datetime,9,10))<20,]
test2=data2[as.integer(substr(data2$datetime,9,10))>19,]

#create boxplot of users over every hour by type of user
boxplot(train2$count~train2$hour,xlab="hour", ylab="count of users", main = "Total Users")
boxplot(train2$casual~train2$hour,xlab="hour", ylab="count of users", main = "Casual Users")
boxplot(train2$registered~train2$hour,xlab="hour", ylab="count of users", main = "Registered Users")
#It can be seen that 'registered' users have a similar trend as 'count'. Therefore, registered users make up most of the users. These boxplots confirm the analysis made earlier regarding different user types and peak hours. Also, 'hour' is significant variable and can be used in our model. 


#To treat outliers, we can use log transform (this will make data less skewed)...suggested by Dr. Johnson (faculty advisor)
boxplot(log(train2$count)~train2$hour,xlab="hour",ylab="log(count)", main = "Log Tranform Total Users")


#let's take a look at the relationships between temperature, windspeed and humidity...because all these are continuous variables, we can get correlations
sub2=data.frame(train2$registered,train2$casual,train2$count,train2$temp,train2$humidity,train2$atemp,train2$windspeed)
cor(sub2)

#Takeaways:
#Windspeed is not as strongly (positive or negative) correlated to other variables as compared to 'temp' or 'humidity'. 

#look at days of the week to see if there is anything trend

date2=substr(data2$datetime,1,10)
days2<-weekdays(as.Date(date2))
data2$day=days2
boxplot(data2$registered~data2$day,xlab="day", ylab="registered users")
boxplot(data2$casual~data2$day,xlab="day", ylab="casual users")

#Casual riders tend to use bikes more on weekends and registered users use bikes more on working days. This confirms earlier speculation. 

```


```{r, eval=TRUE}
####DEMAND DIVERS####

#use a principle component analysis (PCA) to show how the input data can be explained by a same-sized set of orthogonal variables; I want to see where most of the information is contained in the dataset


tr.inputs <- select(train, datetime:windspeed)
train.svd <- svd(scale(sapply(tr.inputs, unclass)))

gg2 <- ggplot() + 
    geom_bar(aes(x=1:length(train.svd$d), y=train.svd$d^2/sum(train.svd$d^2)),
             stat="identity",
             fill="red") + 
    scale_x_discrete(limits=1:length(tr.inputs)) +
    labs(title="Feature Variance") +
    labs(x="Orthogonal variables") +
    labs(y="Proportion of variance explained")
print(gg2)

#From looking at the graph, it can be seen that the last eigenvalue (variable 9) contributes the least (almost negligible) amount of information to the input data. 

#breakdown of eigenvalue 9

eigval <- 9
gg3 <- ggplot() + 
    geom_bar(aes(x=1:length(tr.inputs), y=train.svd$v[,eigval]),
             stat="identity",
             fill="red") + 
    scale_x_discrete(limits=names(tr.inputs)) +
    labs(title=paste("Composition of eigenvalue",eigval)) +
    labs(x="Feature") +
    labs(y="Scaled Column Means")

print(gg3)

#we can see that eigenvalue 9 is is comprised of temp, atemp and some wind and humidity. The low eigenvalue here means that one of the 4 variables can be completely neglected. 


#from the first PCA graph, we can see that 25% of the data variation comes from eigenvalue 1. Let's see a breakdown of eigenvalue 1.

eigval2 <- 1
gg4 <- ggplot() + 
    geom_bar(aes(x=1:length(tr.inputs), y=train.svd$v[,eigval2]),
             stat="identity",
             fill="red") + 
    scale_x_discrete(limits=names(tr.inputs)) +
    labs(title=paste("Composition of eigenvalue",eigval2)) +
    labs(x="Feature") +
    labs(y="Scaled Column Means")

print(gg4)


#It looks like the important components of eigenvalue 1 are datetime, season, temp (also atemp) and windspeed. The main takeaway here is that all these important variables (that account for 25% of variability) are intertwined. Pratically, this makes complete sense. 

#Because of the broad spread of orthogonal data across a variety of featurs, an optimal machine learning model will include all most of the features.

```

```{r, eval=TRUE}
####Decision trees/feature division####

#make hour into an integer so we can use 'rpart' properly
data2$hour=as.integer(data2$hour)

#re-split and combine everything so we know hour is integer
train2=data2[as.integer(substr(data2$datetime,9,10))<20,]
test2=data2[as.integer(substr(data2$datetime,9,10))>19,]
data2=rbind(train2,test2)

#make the decision trees for day part for casual and registered users (dp_cas, dp_reg)
d=rpart(registered~hour,data=train2)
fancyRpartPlot(d)
d=rpart(casual~hour,data=train2)
fancyRpartPlot(d)

#make day part  categories in 'data2' according to the decision trees given by 'rpart'
data2=rbind(train2,test2)
data2$dp_reg=0
data2$dp_reg[data2$hour<8]=1
data2$dp_reg[data2$hour>=22]=2
data2$dp_reg[data2$hour>9 & data2$hour<18]=3
data2$dp_reg[data2$hour==8]=4
data2$dp_reg[data2$hour==9]=5
data2$dp_reg[data2$hour==20 | data2$hour==21]=6
data2$dp_reg[data2$hour==19 | data2$hour==18]=7

data2$dp_cas=0
data2$dp_cas[data2$hour<=8]=1
data2$dp_cas[data2$hour==9]=2
data2$dp_cas[data2$hour>=10 & data2$hour<=19]=3
data2$dp_cas[data2$hour>19]=4

#decision tree breakdown for temperature for casual and registered users (temp_reg, temp_cas)
f=rpart(registered~temp,data=train2)
fancyRpartPlot(f)
f=rpart(casual~temp,data=train2)
fancyRpartPlot(f)

#make temperature categories in 'data2' according to the decision trees given by 'rpart'
data2$temp_reg=0
data2$temp_reg[data2$temp<13]=1
data2$temp_reg[data2$temp>=13 & data2$temp<23]=2
data2$temp_reg[data2$temp>=23 & data2$temp<30]=3
data2$temp_reg[data2$temp>=30]=4


data2$temp_cas=0
data2$temp_cas[data2$temp<15]=1
data2$temp_cas[data2$temp>=15 & data2$temp<23]=2
data2$temp_cas[data2$temp>=23 & data2$temp<30]=3
data2$temp_cas[data2$temp>=30]=4

# extracting year and month from data2, make buckets according to part of the year (year_part) 
data2$year=substr(data2$datetime,1,4)
data2$year=as.factor(data2$year)
data2$month=substr(data2$datetime,6,7)
data2$month=as.factor(data2$month)

data2$year_part[data2$year=='2016']=1
data2$year_part[data2$year=='2016' & (data2$month == '04' | data2$month == '05' | data2$month == '06')]=2
data2$year_part[data2$year=='2016' & (data2$month == '07' | data2$month == '08' | data2$month == '09')]=3
data2$year_part[data2$year=='2016' & (data2$month == '10' | data2$month == '11' | data2$month == '12')]=4
data2$year_part[data2$year=='2017']=5
data2$year_part[data2$year=='2017' & (data2$month == '04' | data2$month == '05' | data2$month == '06')]=6
data2$year_part[data2$year=='2017' & (data2$month == '07' | data2$month == '08' | data2$month == '09')]=7
data2$year_part[data2$year=='2017' & (data2$month == '10' | data2$month == '11' | data2$month == '12')]=8

#see the breakdown by year_part to see if it was done right; we should see an even distribution among all 8 categories...this is confirmed
table(data2$year_part)


#creating another variable day_type which may affect our accuracy as weekends and weekdays are important in deciding rentals/demand
data2$day_type=0
data2$day_type[data2$holiday==0 & data2$workingday==0]="weekend"
data2$day_type[data2$holiday==1]="holiday"
data2$day_type[data2$holiday==0 & data2$workingday==1]="working day"

#combine everthing so 'data2' is up to do...note: there is no need to keep doing this, doing it once at the end should suffice but I like to check the data set (data2) as I go
train2=data2[as.integer(substr(data2$datetime,9,10))<20,]
test2=data2[as.integer(substr(data2$datetime,9,10))>19,]
#plot(train2$temp,train2$count)
data2=rbind(train2,test2)

#make weekend variable
data2$weekend=0
data2$weekend[data2$day=="Sunday" | data2$day=="Saturday"]=1

#double-check to see if all the variables are the right type
str(data2)


#converting all relevant categorical variables into factors for random forest model
data2$season=as.factor(data2$season)
data2$holiday=as.factor(data2$holiday)
data2$workingday=as.factor(data2$workingday)
data2$weather=as.factor(data2$weather)
data2$hour=as.factor(data2$hour)
data2$month=as.factor(data2$month)
data2$day_part=as.factor(data2$dp_cas)
data2$day_type=as.factor(data2$dp_reg)
data2$day=as.factor(data2$day)
data2$temp_cas=as.factor(data2$temp_cas)
data2$temp_reg=as.factor(data2$temp_reg)

train2=data2[as.integer(substr(data2$datetime,9,10))<20,]
test2=data2[as.integer(substr(data2$datetime,9,10))>19,]


# log transformation for some skewed variables, which can be seen from their distribution
train2$reg1=train2$registered+1
train2$cas1=train2$casual+1
train2$logcas=log(train2$cas1)
train2$logreg=log(train2$reg1)
test2$logreg=0
test2$logcas=0

#want to see new distribution after transformation
boxplot(train2$logreg~train2$weather,xlab="weather", ylab="registered users")
boxplot(train2$logreg~train2$season,xlab="season", ylab="registered users")

```

```{r, eval=TRUE}
####Final Model building via random forest####
#note: different models made for registered and casual users
#a random forests model is used because it is a robust method for predicting rental counts given a specific set of variables and is not affected greatly by irrelevant features (which is why I inlcuded a lot of features)

set.seed(123)

#registered
fit1 <- randomForest(logreg ~ hour +workingday+day+holiday+ day_type +temp_reg+humidity+atemp+windspeed+season+weather+dp_reg+weekend+year+year_part, data=train2,importance=TRUE, ntree=250)

pred1=predict(fit1,test2)
test2$logreg=pred1

#casual
set.seed(123)

fit2 <- randomForest(logcas ~hour + day_type+day+humidity+atemp+temp_cas+windspeed+season+weather+holiday+workingday+dp_cas+weekend+year+year_part, data=train2,importance=TRUE, ntree=250)

pred2=predict(fit2,test2)
test2$logcas=pred2


test2$registered=exp(test2$logreg)-1
test2$casual=exp(test2$logcas)-1
test2$count=test2$casual+test2$registered 

#let's take a look at the relative importance of each feature (even tough random forest is robust and can handle extra features)
library(mlbench)
library(lattice)
library(caret)
importance <- varImp(fit2, scale=FALSE)
print(importance)

#The Relative Importance chart shows that the 'hour' feature contributes the most in terms of effect on count (demand). This can be more explicitly seen below.

#code below will use different a random forest model that I attempted earlier, but the resultant graph showing relative importance of each feature will be the same or have very similar characteristics for the model and variables done above; the point is to show that the hour of the day has the greatest importance in the model (or any random forest model using this data set)

train.features <- select(train, season, dayofweek, timeofday, workingday, 
                         weather, temp, humidity, windspeed)
train.rf <- randomForest(train.features, train$count, ntree=100, importance=TRUE)

partials <- data.frame()

for (i in seq_along(names(train.features))) {
  partial <- partialPlot(train.rf, sapply(train.features, unclass), 
                         names(train.features)[i], 
                         plot=FALSE)
  xt <- rescale(partial$x)
  partials <- rbind(partials, data.frame(x=partial$x, xt=xt, y=partial$y, 
                                         feature=names(train.features)[i]))
}

ranges <- ddply(partials, "feature", function(d) {
  r <- range(d$y)
  data.frame(feature=d$feature[1], range=r[2]-r[1])
})

features_to_plot <- ranges[ranges$range>0.05*max(ranges$range),"feature"]

ggplot(partials[partials$feature %in% features_to_plot,], 
       aes(x=xt, y=y, color=feature)) +
  geom_line(size=2) +
  theme_light(base_size=16) +
  xlab("Feature Range (Min to Max)") +
  ylab("Hourly Bike Rentals") 


#It can be seen that when the value of 'timeofday' (or 'hour') changes, the corresponding bike demand changes greatly. The same can be said for 'temp' but not to the extend of 'timeofday'. The other variables do not affect bike demand as much, even as their values change. 



#check accuracy of model using only complete data set we have (train2)...split 'train2' into 2 sets (one for training, one for testing)

#just split 'train2' in half
train3 = train2[1:5443,]
test3 = train2[5444:10866,]

train3$reg1=train3$registered+1
train3$cas1=train3$casual+1
train3$logcas=log(train3$cas1)
train3$logreg=log(train3$reg1)
test3$logreg=0
test3$logcas=0


set.seed(123)

#registered - validation
fit3 <- randomForest(logreg ~ hour +workingday+day+holiday+ day_type +temp_reg+humidity+atemp+windspeed+season+weather+dp_reg+weekend+year+year_part, data=train3,importance=TRUE, ntree=250)

pred3=predict(fit3,test3)
test3$logreg_pred=pred3

#casual - validation
set.seed(123)

fit4 <- randomForest(logcas ~hour + day_type+day+humidity+atemp+temp_cas+windspeed+season+weather+holiday+workingday+dp_cas+weekend+year+year_part, data=train3,importance=TRUE, ntree=250)

pred4=predict(fit4,test3)
test3$logcas_pred=pred4


test3$registered_pred=exp(test3$logreg_pred)-1
test3$casual_pred=exp(test3$logcas_pred)-1
test3$count_pred=test3$casual_pred+test3$registered_pred 

#made custom function to measure root mean square for model; this will help show accuracy
RMSE2 = function (m, o) {
  sqrt(mean(m-o)^2)}

RMSE2(test3$count_pred, test3$count) #value obtained is pretty good considering how large the range of counts is, which indicates the model is fairly accurate. But let's get a better visualiztion by looking at the boxplot distributions of the observed counts and predicted counts.

boxplot(test3$count~test3$hour,xlab="hour", ylab="count of users", main = "Total Users [test3]")
boxplot(test3$count_pred~test3$hour,xlab="hour", ylab="count of users", main = "Total Predicted Users [test3]")

#As can be seen from the boxplots, thet overall shape and distribution of the predicted values is very close to the observed ones. This indicates a fairly good model. 

```



