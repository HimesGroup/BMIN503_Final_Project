---
title: "Predicting Multi-Drug Resistance Status in Canine Admissions"
author: "Sondra Calhoun"
output: 
  html_document:
    toc: false 
    depth: 3 
    theme: paper 
    highlight: tango
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  
***

### Overview
  The goal of this project is to develop and compare Random Forest models designed to predict whether or not a hospitalized dog is infected with multi-drug resistant (MDR) bacteria. MDR-bacteria pose a threat to other hospital patients and staff, and identifying high risk patients before the results of culture and sensitivity testing are available may reduce the risk.  This project will leverage two years of demographic, procedure, and prescription data for canines at a large tertiary veterinary clinic to predict canine MDR status. 
  
Faculty consulted for this project included clinical microbiologist Dr. Shelley Rankin, data scientist Dr. William La Cava, and bioinformatician Dr. John Holmes. 
  
Please note that the final results of the project differ from those presented on 12/5/2017.  After further research, I updated my code to include the sample balancing methodologies within for-loop for cross validation instead of externally. I also implemented a cost-optimizing protocol to address the imbalance in the cost of missclassifying an MDR patient versus a control. These two measures led me to choose a different final model, which led to enhanced predictive performance in the reserved validation data set. 


### Introduction 
  Antimicrobial resistance in bacteria, particularly multi-drug resistance (MDR) is a growing concern in not only human but veterinary medicine. Animals with MDR infections have poor treatment outcomes, the antimicrobials needed to treat MDR infections are substantially more expensive for pet owners, and many MDR organisms are zoonotic, putting not only other animals but humans at risk. When a patient's bacterial culture and sensitivity tests reveals MDR, various additional infectious disease control procedures are initiated to protect other animal patients and human staff. The ability to identify patients at high risk of MDR would allow for these procedures to be initiated earlier, reducing the spread of MDR bacteria and the risk to animal and human health.
  
  Dr. Shelley Rankin's experience and knowledge of culture and sensitivity testing as Chief of Clinical Microbiology at PennVet's Ryan Veterinary Hospital helped me develop the initial question for this project. My studies in veterinary medicine, epidemiology and the sociology of antimicrobial prescribing influenced the choice of the variables to include in the data set.  Many veterinary studies are limited to small case-control or cohort studies and prospective clinical trials, largely due to a lack of electronic records at many clinics. This study will utilize a data set of approximately 45,000 canine admissions to a tertiary veterinary hospital.   The scale of the data for this project allows the use of "big data" approaches that are rare in veterinary medical research.  As such, my methods are developed with the assistance of Dr. William La Cava, a Data Scientist post-doctoral fellow, and John Holmes, an faculty member in the Department of Epidemiology, Bio-statistics, and Informatics.


### Methods

The data used for this project is administrative data from a large tertiary veterinary hospital. It is de-identified but otherwise drawn straight from the hospitals data system and requires significant cleaning. Although the data is de-identified, it will not be made publicly available for privacy purposes. 

In order to predict MDR status, I will employ the supervised machine learning method, Random Forest.  The development of MDR is very rare, leading to a severe imbalance between cases and controls. As only 0.5-1% of the admissions in the data set have MDR (depending on the stage of data cleaning), the models could select control status and still have an accuracy of 99%.  In this analysis we will explore different methods of managing this imbalance (under sampling controls, over sampling cases, a combination of both, and a synthetic sampling mechanism, ROSE) in training and test data and their effect on prediction in a separate validation data set. Because this is a rare but very serious event, we will prioritize models with high sensitivity for detecting true MDR over those that produce a higher overall accuracy.      

#####Data Cleaning
Load relevant libraries for analysis.
```{r eval=TRUE, message=FALSE}
library(dplyr)
library(tidyverse)
library(GGally)
library(randomForest)
library(caret)
library(broom)
library(ROSE)
library(rpart)
library(ROCR)
library(pROC)
library(knitr)
library(kableExtra)

```


Retrieve the vet data (given as a .csv), and save as a data frame in R. 
```{r eval=FALSE}

#vet<-read.csv("FullData.csv", header=TRUE, na.strings= "", stringsAsFactors = FALSE)
#save(vet,file="vet.Rda")

```

Load the data frame and clean the data. 
```{r eval=TRUE}

load("vet.Rda")

#Stripping the time and date from the MDR-Alert and fee code so that only procedures and visits prior to the MDR alert are included in the analysis. 
vet<-mutate(vet,MDRtime=MDRAert)
vet<-mutate(vet,MDRtime=(gsub('MDR-Alert! ','',MDRtime)))
vet$MDRtime<-strptime(vet$MDRtime, "%H:%M:%S %d %b %Y")
vet$MDRtime<-as.POSIXct(vet$MDRtime)

vet <- mutate(vet, feedate=paste(vet$PostDate, vet$PostTime, sep=" ")) 
vet$feedate<-strptime(vet$feedate, "%m/%d/%Y %H:%M:%S")
vet$feedate<-as.POSIXct(vet$feedate)

#Identifying the total number of patients with an MDR alert by year. Removing patients with an MDR alert prior to 2013 from the analsis.  
visit<-vet[!duplicated(vet$VisitID),]
visit$year = factor(format(visit$MDRtime,'%Y'))
vet$year = factor(format(vet$MDRtime,'%Y'))
summary(visit$year)

#Include only non-MDR patients and those with MDRs diagnosed after 2013 
#(i.e. Exclude patients with MDR statuses determined prior to 2013)
vet<-subset(vet, year=="2013"|year=="2014"|year=="2015" |year=="2016"| is.na(year))
visit<-subset(visit, year=="2013"|year=="2014"|year=="2015" |year=="2016"| is.na(year))


#Generate Case status--Do not include patients with MDR diagnoses outside the study period
vet <-mutate(vet, case = ifelse(is.na(year)|year == "2015" | year== "2016" |year=="2008"|year=="2009"|year=="2010"|year=="2011"|year=="2012", 0,1))
visit <-mutate(visit, case = ifelse(is.na(year)|year == "2015" | year== "2016" |year=="2008"|year=="2009"|year=="2010"|year=="2011"|year=="2012", 0,1))
vet <- mutate(vet, case = factor(case, levels=c(0, 1), labels=c("No", "Yes"))) 
visit <- mutate(visit, case = factor(case, levels=c(0, 1), labels=c("No", "Yes"))) 
table(vet$case)
table(visit$case)

```
Of the 45,312 canine admission seen in 2013-2014, 423 had MDR alerts.  Of these, 320 had MDR alerts placed during in the study period. The 68 admissions with MDR alerts prior to 2013 were removed from analysis. The 35 admissions with MDR alerts after 2014 will be included in the analysis, but will be treated as non-MDR admissions since during the study period they did not have an MDR. Of 45,244 admissions, 320 were for patients with an MDR diagnosed during the study period. Some of the 320 admissions may have been repeat admissions for the same animal after MDR diagnosis. 

Because procedures after the MDR alert are not relevant to predicting MDR, I will remove all the procedure codes after the MDR alert time from the patients that were diagnosed with a MDR. This code also removes all entries for re-admissions after MDR diagnosis.  
```{r eval=TRUE}
vet<-subset(vet,vet$case=="No"|(vet$case=="Yes"&!(vet$feedate > vet$MDRtime)))
table(vet$case)
visitct<-vet[!duplicated(vet$VisitID),]
table(visitct$case)
```
Over half of the admissions (179/320) for MDR patients in this period were admissions from patients who had been previously diagnosed with MDR. Since this occurred after MDR diagnosis, these admissions were removed from the predictive analysis, furthering the severity of the imbalance. 

Next I will create new cleaned variables to be used in the analysis: antibiotic use (y/n), antibiotic prescriptions per visit, length of stay, month, admit service, castration status(sex), breed status, referred (y/n), admission date (days since January 1,2016) and age at admission.  

```{r eval=TRUE}
#Age
vet<-rename(vet, dob=MDR)
vet$dob<-strptime(vet$dob, "%m/%d/%Y")
vet$dob<-as.POSIXct(vet$dob)
vet$AdmitDate<-strptime(vet$AdmitDate, "%d%b%Y")
vet$AdmitDate<-as.POSIXct(vet$AdmitDate)
vet<-mutate(vet, age=as.numeric(difftime(AdmitDate, dob, 
                                        unit="weeks"))/52.25)

#Casrated or Spayed Status
vet$cast<-factor(vet$CasSpayDesc)

#Admit month
vet$month<-months(vet$AdmitDate)
vet$month<-factor(vet$month)

#Admit Service
summary(vet$AdmitService)
#Set inappropriate categories to "NA"
vet<-mutate(vet, AdmitService=ifelse(AdmitService==" 4/14/2014"|AdmitService==" 8/13/2013",NA,AdmitService))
vet$AdmitService<-factor(vet$AdmitService)

#Pure Breed (mixed/unknown vs purebred)
length(unique(vet$BreedDescription))
#Simplify this variable from 180 breeds to 2 categories
vet<-mutate(vet, purebreed=ifelse(BreedDescription=="UNSPECIFIED"| BreedDescription=="UNKNOWN"|BreedDescription=="MIXED CANINE",0,1))
vet <- mutate(vet, purebreed = factor(purebreed, levels=c(0, 1), labels=c("Mixed Breed/Unknown", "Purebreed"))) 

#Length of Stay--Note: this is until MDR status for MDR patients and until discharge for patients without MDR, because this represents their time at risk of MDR. 
vet$DischargeDate<-strptime(vet$DischargeDate, "%m/%d/%Y")
vet$DischargeDate<-as.POSIXct(vet$DischargeDate)
vet<-mutate(vet, finalday=ifelse(case=="Yes" & MDRtime<DischargeDate ,MDRtime,DischargeDate))
vet$finalday<-as.POSIXct(vet$finalday, origin="1970-01-01")
vet<-mutate(vet, LOS=(as.numeric(difftime(finalday, AdmitDate, unit="days"))+1))
test1<-filter(vet, LOS>100) #check patients with abnormally long LOS
vet<-subset(vet,vet$PatientID!=802715) #This record was for a group cremation but the discharge date was listed as 2017. Removed from analysis
test1<-filter(vet, LOS>100) #Recheck patients. Two patients had stays of 120 days, both were research animals, which is plausible. 

#Weight (Weights over 150kg are beyond the maximum recorded for dogs--set to NA)
summary(vet$WeightKG)
vet<-mutate(vet, weight=ifelse(WeightKG<150,WeightKG,NA))
vet<-mutate(vet, weight=ifelse(weight>0,weight,NA))

#Referral Status
vet <-mutate(vet, referral = ifelse(is.na(RefVetName), 0,1))
vet <- mutate(vet, referral = factor(referral, levels=c(0, 1), labels=c("No", "Yes")))

#Antibiotics during Admission (Yes vs No)
#Codes derived from list provided by mentor.
class(vet$ProcedureCode)
vet <-mutate(vet, ab = ifelse(ProcedureCode=="6160"|ProcedureCode=="2028"|ProcedureCode=="6121"|ProcedureCode=="6130"|ProcedureCode=="6131"|ProcedureCode=="6132"|ProcedureCode=="6133" |ProcedureCode=="6134"|ProcedureCode=="6135"|ProcedureCode=="6136"|ProcedureCode=="6090" |ProcedureCode=="6137"|ProcedureCode=="6138"|ProcedureCode=="6139" |ProcedureCode=="6140" |ProcedureCode=="6510"|ProcedureCode=="680D" |ProcedureCode=="633E"|ProcedureCode=="661J"|ProcedureCode=="627I" |ProcedureCode=="6198"|ProcedureCode=="6199"|ProcedureCode=="6200" |ProcedureCode=="6201" |ProcedureCode=="6202"|ProcedureCode=="622G" |ProcedureCode=="697C" |ProcedureCode=="698C" |ProcedureCode=="6204"|ProcedureCode=="6775"| ProcedureCode=="6229" |ProcedureCode=="6207" |ProcedureCode=="6203" |ProcedureCode=="6205" |ProcedureCode=="6206" |ProcedureCode=="6212" |ProcedureCode=="6213" |ProcedureCode=="6214" |ProcedureCode=="6215" |ProcedureCode=="6216" |ProcedureCode=="6219" |ProcedureCode=="692C" |ProcedureCode=="643C" |ProcedureCode=="601E" |ProcedureCode=="629G" |ProcedureCode=="6234" |ProcedureCode=="6235" |ProcedureCode=="6236" |ProcedureCode=="6237" |ProcedureCode=="6238" |ProcedureCode=="615J" |ProcedureCode=="6242" |ProcedureCode=="6243" |ProcedureCode=="6244" |ProcedureCode=="6245" |ProcedureCode=="1193" |ProcedureCode=="652G" |ProcedureCode=="657G" |ProcedureCode=="697B"|ProcedureCode=="6336" |ProcedureCode=="6337" |ProcedureCode=="6338" |ProcedureCode=="6152" |ProcedureCode=="603C" |ProcedureCode=="655B" |ProcedureCode=="6357" |ProcedureCode=="6358" |ProcedureCode=="6399" |ProcedureCode=="666B" |ProcedureCode=="695D" |ProcedureCode=="699C" |ProcedureCode=="6365" |ProcedureCode=="6366" |ProcedureCode=="696F" |ProcedureCode=="6497" |ProcedureCode=="6613" |ProcedureCode=="6508" |ProcedureCode=="6509" |ProcedureCode=="617I" |ProcedureCode=="6506" |ProcedureCode=="2029" |ProcedureCode=="6538" |ProcedureCode=="671J" |ProcedureCode=="633H" |ProcedureCode=="621G" |ProcedureCode=="680J" |ProcedureCode=="672G" |ProcedureCode=="673G" |ProcedureCode=="6559" |ProcedureCode=="6652" |ProcedureCode=="6593" |ProcedureCode=="651C" |ProcedureCode=="633B" |ProcedureCode=="659E" |ProcedureCode=="676D" |ProcedureCode=="678D" |ProcedureCode=="620B" |ProcedureCode=="6562" |ProcedureCode=="6564" |ProcedureCode=="6585" |ProcedureCode=="6766" |ProcedureCode=="6595" |ProcedureCode=="6607" |ProcedureCode=="6608" |ProcedureCode=="627B" |ProcedureCode=="6653" |ProcedureCode=="6654" |ProcedureCode=="6655"|ProcedureCode=="6732" |ProcedureCode=="6733" |ProcedureCode=="6734"|ProcedureCode=="6735" |ProcedureCode=="6795"|ProcedureCode=="6745" |ProcedureCode=="6746" |ProcedureCode=="6748" |ProcedureCode=="6758" |ProcedureCode=="697E" |ProcedureCode=="6761" |ProcedureCode=="6032" |ProcedureCode=="6747" |ProcedureCode=="6749" |ProcedureCode=="668A" |ProcedureCode=="648A" |ProcedureCode=="649A" |ProcedureCode=="6765" |ProcedureCode=="6768" |ProcedureCode=="6771" |ProcedureCode=="637B"|ProcedureCode=="6894", 1,0))

vet <- vet %>% group_by(VisitID) %>% mutate(ab.yn=max(ab))
vet <- mutate(vet, ab.yn = factor(ab.yn, levels=c(0, 1), labels=c("No", "Yes")))

#Number of Antibiotic Prescriptions per Admission
vet <- vet %>% group_by(VisitID) %>% mutate(num.ab.rx=sum(ab))

#Days since Jan. 1, 2013
time1<-as.Date("2013-01-01", tz="America/New_York")
vet<-mutate(vet, AdmitDate=difftime(as.POSIXct(AdmitDate), as.POSIXct(time1), units="days"))#Dates do not work with the imputation method, even though the variable has complete data.
vet<-mutate(vet, AdmitDate=as.numeric(AdmitDate))
```

After using the fee code data to generate these variables, I will collapse the data to one row per visit to analyze at the admission level. 

```{r eval=TRUE}
visit<-vet[!duplicated(vet$VisitID),]
table(visit$case) 

#Only keep variables still relevant for the analysis
 var <- c("num.ab.rx","ab.yn","referral", "LOS","weight", "case", "AdmitService", "purebreed", "month", "cast", "age", "AdmitDate")
names.use <- names(visit)[(names(visit) %in% var)]
data <- visit[, names.use]
save(data,file="data2.rda")


```

### Results

#####Descriptive Results


```{r eval=TRUE}

#Age
ggplot(data, aes(age, fill=case, colour=case)) +
  geom_histogram(aes(y=..density..), breaks=seq(0,25,1), alpha=0.6, 
                 position="identity", lwd=0.2) +
  labs(fill="MDR Case", colour="MDR Case")+
  xlab("Age")+
  ggtitle("MDR Status by Age")

#Weight
ggplot(data, aes(weight, fill=case, colour=case)) +
  geom_histogram(aes(y=..density..), alpha=0.6, 
                 position="identity", lwd=0.2) +
  labs(fill="MDR Case", colour="MDR Case")+
  xlab("Weight (Kg)")+
  ggtitle("MDR Status by Weight")

#Castration Status(Sex)
options(digits = 2)
cast.table<-prop.table(table(data$case, data$cast),1)
cast.table
barplot(cast.table,legend=T,beside=T, col=c("#F8766D" , "#00BFC4"), main='MDR Status by Castration Status')

#Month of Admission
options(digits = 2)
data$month <- factor(data$month,levels=month.name)
month.table<-prop.table(table(data$case, data$month),1)
month.table
barplot(month.table,legend=T,beside=T,las=2, col=c("#F8766D" , "#00BFC4"), main='MDR Status by Month of Admission')

#Referral Status
options(digits = 2)
referral.table<-prop.table(table(data$case, data$referral),1)
referral.table
barplot(referral.table,legend=T,beside=T, args.legend = list(x = "topright", bty = "n", inset=c(-0.09, 0)),col=c("#F8766D" , "#00BFC4"), main='MDR Status by Referral Status')

#Admit Service
options(digits = 2)
admit.table<-prop.table(table(data$case, data$AdmitService),1)
barplot(admit.table,legend=T,beside=T,las=2, col=c("#F8766D" , "#00BFC4"), cex.names=0.8, main='MDR Status by Admission Service')

admit.table<-t(admit.table)
knitr::kable(admit.table)%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))


#Purebreed Status
options(digits = 2)
purebreed.table<-prop.table(table(data$case, data$purebreed),1)
purebreed.table
barplot(purebreed.table,legend=T,beside=T, args.legend = list(x = "topright", bty = "n", inset=c(-0.09, 0)), col=c("#F8766D" , "#00BFC4"), main='MDR Status by Breed Status')

#LOS
ggplot(data, aes(LOS, fill=case, colour=case)) +
  geom_histogram(aes(y=..density..), alpha=0.6, 
                 position="identity", lwd=0.2) +
  labs(fill="MDR Case", colour="MDR Case")+
  xlab("Length of Stay (Days)")+
  ggtitle("MDR Status by Length of Stay Prior to MDR") #Total LOS

ggplot(data, aes(LOS, fill=case, colour=case)) +
  geom_histogram(aes(y=..density..), breaks=seq(0,40,1), alpha=0.6, 
                 position="identity", lwd=0.2) +
    labs(fill="MDR Case", colour="MDR Case")+
  xlab("Length of Stay (Days)")+
  ggtitle("MDR Status by Length of Stay Prior to MDR") #LOS Detail

#Antibiotics used
options(digits = 2)
abx.table<-prop.table(table(data$case, data$ab.yn),1)
abx.table
barplot(abx.table,legend=T,beside=T,col=c("#F8766D" , "#00BFC4"), main='MDR Status by Antibiotic Use')

#Days of Antibiotic Therapy
ggplot(data, aes(num.ab.rx, fill=case, colour=case)) +
  geom_histogram(aes(y=..density..), alpha=0.6, 
                 position="identity", lwd=0.2) +
    labs(fill="MDR Case", colour="MDR Case")+
  xlab("Number of Antibiotic Orders")+
  ggtitle("MDR Status by Number of Orders for Antibiotics Received Prior to MDR")

#Date of Admission (Days since 1/1/2013)
ggplot(data, aes(AdmitDate, fill=case, colour=case)) +
  geom_histogram(aes(y=..density..), alpha=0.6, 
                 position="identity", lwd=0.2) +
    labs(fill="MDR Case", colour="MDR Case")+
  xlab("Number of Days from Jan.1, 2013 to Admission Date")+
  ggtitle("MDR Status by Date of Admission")

```

Patients that develop an MDR tend to be older and slightly heavier than the general population. There is a greater proportion of females in the MDR group than in the control group. Almost all of the dogs were spayed or neutered. MDRs occurred in greater than expected proportions in August through November.  A greater proportion of admissions where an MDR was diagnosed were referrals compared to non-MDR admissions. A greater proportion of MDRs appear to be admitted into the oncology service (28%), and a smaller proportion to the Emergency service (21%) compared to the non-MDR admissions (12% and 27% respectively). There does not appear to be a difference in breed status. 

Although among both groups, the most common length of stay (or time at risk) is 1 day, the length of stays for MDR admissions appear to be longer on average.  MDR patients appear to be more likely to have received an antibiotic than non-MDR patients, although the majority of MDR patients did not receive antibiotics in their admission. The MDR patients also appear to have received more antibiotic orders during their admission, on average. Finally, fewer MDR admissions occurred in 2014 than 2013, while admissions for the overall population stayed stable. The source of the sudden change is unclear. Perhaps changes in hospital policy regarding testing or definition of MDR contributed to this difference. 


####Adjusting for Unbalanced Data
As stated previously, the sample is highly unbalanced.  I will first attempt to conduct the analysis without accounting for this imbalance, and then use four methods of accounting for this unbalance to compare the results.  Finally, I will assess variable importance and use a subset of the most important variables to develop a reduced random forest model predicting MDR.

#####Imputation of Missing Data
The Random Forest package used for this analysis requires complete data.  To address the missingness for this exercise, I will use the "na.roughfix" command to impute the missing values. Numeric variables with missing values are replaced with column medians, factors with the most frequent levels. This was chosen to reduce computation time (K-nearest neighbors imputation and random forest imputation were attempted but were too computationally expensive to be feasible).  
```{r eval=TRUE}
class(data)
data<-as.data.frame(data)
miss<-sapply(data, function(x) (sum(is.na(x))/450.64))
library(knitr)
knitr::kable(miss, "html",caption="Percent missing ")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
data<-na.roughfix(data)
sum(is.na(data))
```

#####Splitting into Training, Testing and Validation Data Sets
I will split the data into two sets: a training/testing set and a validation set. I will retain 66% for training/testing and 34% for final validation, as recommended by Dr. Holmes.
```{r eval=TRUE}
#Splitting into training and validation data sets
set.seed(301)
index <- createDataPartition(data$case, p = 0.66, list = FALSE)
train_data <- data[index, ]
validation_data  <- data[-index, ]

sum(is.na(train_data))
```

#####Developing Balanced Data Sets
Next I will train the model without regard to the imbalance in the data. Dr. Holmes and Dr. La Cava differed in their advice on whether or not to center and scale the data. I chose to leave the data as is and avoid bias that might derive from these changes since Random Forests are non-parametric. If I were conducting a regression analysis in parallel, I would have chosen to scale and center the data. 

I will compare four methods: oversampling cases, under sampling controls, both, and a synthetic sampling protocol (rose) that combines these methods to evaluate their performance.

```{r eval=TRUE}
table(train_data$case)

#over sampling
data_balanced_over <- ovun.sample(case ~ ., data = train_data, method ="over", N=59300, seed=1)$data 
table(data_balanced_over$case)

#undersampling
data_balanced_under <- ovun.sample(case ~ ., data = train_data, method = "under", N=188, seed=1)$data
table(data_balanced_under$case)

#Both over and undersampling
data_balanced_both <- ovun.sample(case ~ ., data = train_data, method = "both",p=0.5,   N=nrow(train_data), seed =1)$data
table(data_balanced_both$case)

#ROSE Synthetic Sampling
 data.rose <- ROSE(case ~ ., data = train_data, seed = 301)$data
table(data.rose$case)
```

Now I will build the decision tree models using each sampling strategy (the full imbalanced data set, over sampling controls, under sampling cases, both, and ROSE synthetic sampling).

#####Training Models Using Balanced Data Sets
I will first create these models and test them on the full training data.
```{r eval=TRUE}
set.seed(301) 
#Imbalanced (i.e. original data)
tree.imb <- randomForest(case ~ ., data =train_data, ntree=100)
rf.pred.imb<- predict(tree.imb, train_data, type="prob")
head(rf.pred.imb)
pred.case.imb <- rf.pred.imb[, 2]

#Oversampling
tree.over <- randomForest(case ~ ., data =data_balanced_over, ntree=100, importance=TRUE)
rf.pred.over<- predict(tree.over, data_balanced_over, type="prob")
pred.case.over <- rf.pred.over[, 2]

#Undersampling
tree.under <- randomForest(case ~ ., data =data_balanced_under, ntree=100, importance=TRUE)
rf.pred.under<- predict(tree.under, data_balanced_under, type="prob")
pred.case.un <- rf.pred.under[, 2]

#Both
tree.both <- randomForest(case ~ ., data =data_balanced_both, ntree=100, importance=TRUE)
rf.pred.both<- predict(tree.both, data_balanced_both, type="prob")
pred.case.both <- rf.pred.both[, 2]

#ROSE
tree.rose <- randomForest(case ~ ., data =data.rose, ntree=100, importance=TRUE)
rf.pred.rose<- predict(tree.rose, data.rose, type="prob")
pred.case.rose <- rf.pred.rose[, 2]
```

I will first evaluate the ROC curves for each of these training models:
```{r eval=TRUE}
#Training Set
roc(train_data$case, pred.case.imb, ci=TRUE)
plot.roc(train_data$case, pred.case.imb, ci=TRUE, col="black")
roc(data_balanced_over$case, pred.case.over, ci=TRUE)
plot.roc(data_balanced_over$case, pred.case.over, ci=TRUE, col="blue", add=TRUE)
roc(data_balanced_under$case, pred.case.un, ci=TRUE)
plot.roc(data_balanced_under$case, pred.case.un, ci=TRUE, col="brown", add=TRUE)
roc(data_balanced_both$case, pred.case.both, ci=TRUE)
plot.roc(data_balanced_both$case, pred.case.both, ci=TRUE, col="green", add=TRUE)
roc(data.rose$case, pred.case.rose, ci=TRUE)
plot.roc(data.rose$case, pred.case.rose, ci=TRUE, col="pink", add=TRUE)

```

Then look at the confusion matrices:
```{r eval=TRUE}
options(digits = 3)
tree.imb$confusion 
tree.over$confusion
tree.under$confusion
tree.both$confusion
tree.rose$confusion
```

While the AUC's of all of these models are very close to or equal to one, the sensitivity of the models varies.  This is a rare event, but the consequence of missing a MDR patient is greater than the consequence of mis-identifying a non-MDR patient as MDR.  As such we need to specifically look for a model with a high sensitivity.  We can see that in the training data, the "oversampling" and "both " (over and under sampling) methods appear to reduce the false negative rates. 

#####Cross Validation of the Models
I will use 10 fold cross validation to further evaluate these models. Each sampling methodology is included within each fold of cross validation to reduce over fitting of the models. 

```{r eval=TRUE}
#Imbalanced 
N = nrow(train_data)
K = 10
set.seed(1234)
s = sample(1:K, size=N, replace=T)
pred_outputs.imb <- vector(mode="numeric", length=N)
obs_outputs.imb <- vector(mode="numeric", length=N)

offset <- 0
for(i in 1:K){
    train <- filter(train_data, s != i)
    test <- filter(train_data, s == i)
    obs_outputs.imb[1:length(s[s==i]) + offset] <- test$case

     #RF train/test
    rf <- randomForest(case~., data=train, ntree=100)
    rf.pred.curr.imb <- predict(rf, newdata=test, type="prob")
    pred_outputs.imb[1:length(s[s==i]) + offset] <- rf.pred.curr.imb[,2]

    offset <- offset + length(s[s==i])
}

```

```{r eval=TRUE}
#Over sampling
N= nrow(train_data)
K = 10
set.seed(1235)
s = sample(1:K, size=N, replace=T)
pred_outputs.ov <- vector(mode="numeric", length=N)
obs_outputs.ov <- vector(mode="numeric", length=N)

offset <- 0
for(i in 1:K){
    train <- filter(train_data, s != i)
    test <- filter(train_data, s == i)
    cv.over <- ovun.sample(case ~ ., data = train, method ="over")$data
    obs_outputs.ov[1:length(s[s==i]) + offset] <- test$case

     #RF train/test
    rf <- randomForest(case~age+AdmitService+month+AdmitDate, data=cv.over, ntree=100)
    rf.pred.curr.ov <- predict(rf, newdata=test, type="prob")
    pred_outputs.ov[1:length(s[s==i]) + offset] <- rf.pred.curr.ov[,2]

    offset <- offset + length(s[s==i])
}

#Under sampling
N = nrow(train_data)
K = 10
set.seed(1234)
s = sample(1:K, size=N, replace=T)
pred_outputs.un <- vector(mode="numeric", length=N)
obs_outputs.un <- vector(mode="numeric", length=N)

offset <- 0
for(i in 1:K){
    train <- filter(train_data, s != i)
    test <- filter(train_data, s == i)
   cv.under <- ovun.sample(case ~ ., data = train, method = "under")$data
    obs_outputs.un[1:length(s[s==i]) + offset] <- test$case

     #RF train/test
    rf <- randomForest(case~., data=cv.under, ntree=100)
    rf.pred.curr.un <- predict(rf, newdata=test, type="prob")
    pred_outputs.un[1:length(s[s==i]) + offset] <- rf.pred.curr.un[,2]

    offset <- offset + length(s[s==i])
}


#Both
N = nrow(train_data)
K = 10
set.seed(1234)
s = sample(1:K, size=N, replace=T)
pred_outputs.b <- vector(mode="numeric", length=N)
obs_outputs.b <- vector(mode="numeric", length=N)

offset <- 0
for(i in 1:K){
    train <- filter(train_data, s != i)
    test <- filter(train_data, s == i)
    cv.both <- ovun.sample(case ~ ., data = train, method = "both",p=0.5,   N=nrow(train), seed =1)$data
    obs_outputs.b[1:length(s[s==i]) + offset] <- test$case

     #RF train/test
    rf <- randomForest(case~., data=cv.both, ntree=100)
    rf.pred.curr.b <- predict(rf, newdata=test, type="prob")
    pred_outputs.b[1:length(s[s==i]) + offset] <- rf.pred.curr.b[,2]

    offset <- offset + length(s[s==i])
}

#ROSE
N = nrow(train_data)
K = 10
set.seed(1234)
s = sample(1:K, size=N, replace=T)
pred_outputs.r <- vector(mode="numeric", length=N)
obs_outputs.r <- vector(mode="numeric", length=N)

offset <- 0
for(i in 1:K){
    train <- filter(train_data, s != i)
    test <- filter(train_data, s == i)
     cv.rose <- ROSE(case ~ ., data = train)$data
    obs_outputs.r[1:length(s[s==i]) + offset] <- test$case

     #RF train/test
    rf <- randomForest(case~., data=cv.rose, ntree=100)
    rf.pred.curr.r <- predict(rf, newdata=test, type="prob")
    pred_outputs.r[1:length(s[s==i]) + offset] <- rf.pred.curr.r[,2]

    offset <- offset + length(s[s==i])
}

```


Here are the ROCs for the 10 fold cross validation:
```{r eval=TRUE}
#Cross Validation
roc(obs_outputs.imb, pred_outputs.imb, ci=TRUE)
plot.roc(obs_outputs.imb, pred_outputs.imb, ci=TRUE, col="black")
roc(obs_outputs.ov, pred_outputs.ov, ci=TRUE)
plot.roc(obs_outputs.ov, pred_outputs.ov, ci=TRUE, col="blue", add=TRUE)
roc(obs_outputs.un, pred_outputs.un, ci=TRUE)
plot.roc(obs_outputs.un, pred_outputs.un, ci=TRUE, col="brown", add=TRUE)
roc(obs_outputs.b, pred_outputs.b, ci=TRUE)
plot.roc(obs_outputs.b, pred_outputs.b, ci=TRUE, col="green", add=TRUE)
roc(obs_outputs.r, pred_outputs.r, ci=TRUE)
plot.roc(obs_outputs.r, pred_outputs.r, ci=TRUE, col="pink", add=TRUE)
legend("bottomright", legend=c("Unbalanced", "Oversampling", "Undersampling", "Both", "ROSE"), col=c("black", "blue", "brown", "green", "pink"), lwd=1)
```

The areas under the curve for the cross validation are much smaller than those for the single training set, as expected. The oversampling method has the smallest AUC (0.75) while the undersampling has the highest (0.85). Looking at the curves, the unbalanced and oversampling methods have a sharp drop in the increase in sensitivity per decrease in specificity when viewing from left to right. 

The ROC evaluation compares the true positive to the false positive rate. Avoiding false negatives, is again far more important in these models , so I will use the package caret's confusionMatrix command to evaluate it and other measures.
```{r eval=TRUE}
#Confusion Matrices from cross validation
pred.imb= nrow(pred_outputs.imb) #Imbalanced
pred.imb<-ifelse(pred_outputs.imb<0.5,1,2)
confusionMatrix(pred.imb, obs_outputs.imb, positive="2")
pred.ov= nrow(pred_outputs.ov)#Oversampling
pred.ov<-ifelse(pred_outputs.ov<0.5,1,2)
confusionMatrix(pred.ov, obs_outputs.ov, positive="2")
pred.un= nrow(pred_outputs.un)#Undersampling
pred.un<-ifelse(pred_outputs.un<0.5,1,2)
confusionMatrix(pred.un, obs_outputs.un, positive="2")
pred.both= nrow(pred_outputs.b)#Both
pred.both<-ifelse(pred_outputs.b<0.5,1,2)
confusionMatrix(pred.both, obs_outputs.b, positive="2")
pred.r= nrow(pred_outputs.r)#Rose
pred.r<-ifelse(pred_outputs.r<0.5,1,2)
confusionMatrix(pred.r, obs_outputs.r, positive="2")

```

But, the cost of a false negative and false positive are not the same. As such, the optimal cut-point for determining an MDR case and control may not be 0.5, as shown above.  The following code assumes that miss-classifying a true MDR case as a control is ten times as costly (in terms of risk to other patients and staff) as miss-classifying a non-MDR patient as a case. 

```{r eval=TRUE}
#Function to determine the optimal cutpoint (assumes equal costs for false negatives and false positives). This uses the ROCR package.
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}

#Imbalance: Determining optimal cutpoint based on 10:1 FN:FP cost
predict.imb<-prediction(pred_outputs.imb, obs_outputs.imb)
perf.imb = performance(predict.imb, "cost", cost.fp = 1, cost.fn = 10)
predict.imb@cutoffs[[1]][which.min(perf.imb@y.values[[1]])]

#Oversampling
predict.ov<-prediction(pred_outputs.ov, obs_outputs.ov)
perf.ov = performance(predict.ov, "cost", cost.fp = 1, cost.fn = 10)
predict.ov@cutoffs[[1]][which.min(perf.ov@y.values[[1]])]

#Undersampling
predict.un<-prediction(pred_outputs.un, obs_outputs.un)
perf.un = performance(predict.un, "cost", cost.fp = 1, cost.fn = 10)
predict.un@cutoffs[[1]][which.min(perf.un@y.values[[1]])]

#Both
predict.b<-prediction(pred_outputs.b, obs_outputs.b)
perf.b = performance(predict.b, "cost", cost.fp = 1, cost.fn = 10)
predict.b@cutoffs[[1]][which.min(perf.b@y.values[[1]])]

#ROSE
predict.r<-prediction(pred_outputs.r, obs_outputs.r)
perf.r = performance(predict.r, "cost", cost.fp = 1, cost.fn = 10)
predict.r@cutoffs[[1]][which.min(perf.r@y.values[[1]])]
```

Now, I will re-evaluate the confusion matrices with these new optimal cut-points. 
```{r eval=TRUE}
pred.imb= nrow(pred_outputs.imb) #Imbalanced
pred.imb<-ifelse(pred_outputs.imb<0.18,1,2)
confusionMatrix(pred.imb, obs_outputs.imb, positive="2")
pred.ov= nrow(pred_outputs.ov)#Oversampling
pred.ov<-ifelse(pred_outputs.ov<0.24,1,2)
confusionMatrix(pred.ov, obs_outputs.ov, positive="2")
pred.un= nrow(pred_outputs.un)#Undersampling
pred.un<-ifelse(pred_outputs.un<0.94,1,2)
confusionMatrix(pred.un, obs_outputs.un, positive="2")
pred.both= nrow(pred_outputs.b)#Both
pred.both<-ifelse(pred_outputs.b<0.34,1,2)
confusionMatrix(pred.both, obs_outputs.b, positive="2")
pred.r= nrow(pred_outputs.r)#Rose
pred.r<-ifelse(pred_outputs.r<0.91,1,2)
confusionMatrix(pred.r, obs_outputs.r, positive="2")

```

Assuming a 10:1 false negative to false positive cost ratio, oversampling has the highest sensitivity.  However, it has the lowest overall area under the curve for cross validation and does not perform well at a less extreme cut-point. The method "Both" which both over samples cases and under samples controls has a much larger AUC, has the second highest sensitivity under the 10:1 cost ratio assumption, which only moderately decreases (from 0.26 to 0.18) under an assumption of equal costs for false negatives and false positives. Since this model identified almost as many cases as the "oversampling" protocol with an optimized cut-point, but seems more robust to violations of the cost ratio assumptions, I will chose it as my method of addressing the data imbalance.

#####Assess Variable Importance
I will use this model's importance scores for each variable to determine which aspects of the admission are most important for predicting MDR.
```{r eval=TRUE}
# Variable Importance Table--oversampling
var.imp.b <- data.frame(importance(tree.both,
           type=2))
var.imp.b$Variables <- row.names(var.imp.b)
var.imp.b<-var.imp.b[order(var.imp.b$MeanDecreaseGini, decreasing = T),]
knitr::kable(var.imp.b)%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

Judging by these scores the most important variables are age and AdmitService.  Interestingly, the number of antibiotics that a patient received is in the bottom half of the variables in terms of importance.  Typically the receipt of antimicrobials is considered the greatest risk factor for the development of infection.  

#####Reduced Model Assessment
To avoid an over fitted data set, I will also try reducing the number of features to see if this improves cross validation performance. I will use a cutoff Mean Decrease in Gini of 1000.  If the model is superior, I will use the reduced model as my final model.  

```{r eval=TRUE}
#Oversampling
tree.b.2 <- randomForest(case ~age+AdmitService+month+AdmitDate, data =data_balanced_both, ntree=100, importance=TRUE)
rf.pred.b<- predict(tree.b.2, data_balanced_both, type="prob")
pred.case.b <- rf.pred.b[, 2]

#Both--Reduced Model Cross Validation

#Both
N = nrow(train_data)
K = 10
set.seed(1234)
s = sample(1:K, size=N, replace=T)
pred_outputs.b <- vector(mode="numeric", length=N)
obs_outputs.b <- vector(mode="numeric", length=N)

offset <- 0
for(i in 1:K){
    train <- filter(train_data, s != i)
    test <- filter(train_data, s == i)
    cv.both <- ovun.sample(case ~ ., data = train, method = "both",p=0.5,   N=nrow(train), seed =1)$data
    obs_outputs.b[1:length(s[s==i]) + offset] <- test$case

     #RF train/test
    rf <- randomForest(case~age+AdmitService+month+AdmitDate, data=cv.both, ntree=100)
    rf.pred.curr.b <- predict(rf, newdata=test, type="prob")
    pred_outputs.b[1:length(s[s==i]) + offset] <- rf.pred.curr.b[,2]

    offset <- offset + length(s[s==i])
}

#Predictions with shorter variable list
pred.b= nrow(pred_outputs.b)
pred.b<-ifelse(pred_outputs.b<0.5,1,2)
roc(obs_outputs.b, pred_outputs.b, ci=TRUE)
plot.roc(obs_outputs.b, pred_outputs.b, ci=TRUE, col="green")
confusionMatrix(pred.b, obs_outputs.b, positive="2")
```

The AUC for the reduced model (0.75) is less than the full model. Its sensitivity at the cutpoint of 0.5 is slightly better than the full model, but we should compare using the optimized cut-points. 

```{r eval=TRUE}
#Both--CV optimal cutpoint
predict.b<-prediction(pred_outputs.b, obs_outputs.b)
perf.b = performance(predict.b, "cost", cost.fp = 1, cost.fn = 10)
predict.b@cutoffs[[1]][which.min(perf.b@y.values[[1]])]

```

The optimal cut-point for this reduced model is 0.64.  Using that in our analysis:

```{r eval=TRUE}
pred.b= nrow(pred_outputs.b)
pred.b<-ifelse(pred_outputs.b<0.64,1,2)
confusionMatrix(pred.b, obs_outputs.b, positive="2")
```

The shift in the optimal cut-point did not change the sensitivity (0.22), but did decrease the number of false positives in this reduced model instance.  However, the full model has a higher sensitivity (0.27) at the optimal cut-point, we will use the full model instead of the reduced. 

####Evaluation of Final Model
Finally, we will test this model using the retained validation data.
```{r eval=TRUE}
#Predicting values of reserved validation data. This validation data set was not used in the model development.
pred.tree.both <- predict(tree.both, newdata = validation_data, type="prob")
pred.tree.both<- pred.tree.both[,2]
obs_outputs<-validation_data$case
obs_outputs<-as.numeric(obs_outputs)

```

Now we will evaluate the models performance using the optimized cut-point determined after cross-validation. 

```{r eval=TRUE}
pred.tree.both<-ifelse(pred.tree.both<0.34,1,2)
confusionMatrix(pred.tree.both, obs_outputs, positive="2")
```
```{r eval=TRUE}
roc(validation_data$case, pred.tree.both, ci=TRUE)
plot.roc(validation_data$case, pred.tree.both, ci=TRUE, col="green")

```

The area under the curve for this model is quite low (0.69). Using the cost-optimized cut-point of 0.34, however, the final model was able to correctly identify more of the MDR admissions than in cross validation, with a sensitivity of 38%. The specificity of this model at this cut-point was over 99%.  For each true positive the model identified, there were 2.4 false positives, which is acceptable given the false negative to false positive cost ratio of 10:1.  

### Conclusion  

In this analysis, I developed a predictive random forest model for MDR using administrative data from a tertiary veterinary hospital. Canines diagnosed with MDR appeared more likely (qualitatively--statistical significance was not assessed) to be older, heavier, spayed females, referred to the hospital, with longer lengths of stay, and greater antibiotic use than the general population. 

A method incorporating both over sampling MDR cases and under sampling controls was most effective for addressing the case/control imbalance in this data set, as judged by 10 fold cross validation results. The development of an optimized cut-point for case vs control prediction based on a 10:1 cost ratio (false negative:false positive) improved performance. When only the top five variables of importance were included, the model performance decreased slightly in cross validation. The full model using both over and under sampling was chosen as the final model.

Of the variables derived from the hospital information system database, age, admission service, month, admission date, and weight were the most important for predicting the diagnosis of multi-drug resistant bacteria. Receipt of an antibiotic and the total number of antibiotic orders received were relatively unimportant variables. This was a surprising finding as the administration of antibiotics is generally considered the most important risk factor for the development of resistance. However, only approximately 20% of the overall population population received antibiotics in these records. Other studies have estimated that approximately 35% of veterinary patients at small animal clinics receive antibiotics, and one would expect this percentage to be even higher at a tertiary hospital.  This may reflect unusually high levels of antimicrobial stewardship at this hospital, omissions from the list of antimicrobial fee codes used in this analysis, or reflect only inpatient use and not outpatient prescribing. The results may be limited by missclassification of antibiotic exposure.  

The final model had an AUC of 0.69 and correctly identified 38% of MDR patients when the cost optimized cut-point was used. Although in many circumstances, this would be considered poor performance, this may be of clinical use, particularly if the addition of more variables or a larger training data set could improve the model. One limitation is that one of the top variables of importance was weight, which was also one of the most missing variables. The simple imputation used at the beginning of the analysis may not have been sufficient and may have contributed to this poor prediction. Imputing this variable more precisely (for example using multiple imputation) may improve the model's performance significantly. The addition of other variables, including classes of antibiotic used, severity of disease, and diagnosis prior to MDR may also have the ability to improve the model. 
