---
title: "BMIN503/EPID600 Project Template"
author: "Godefroy Chery"
output: 
  html_document:
    theme: paper 
    highlight: tango
---


***
Use this template to complete your project throughout the course. Your Final Project presentation will be based on the contents of this document. Replace the title/name above and text below with your own, but keep the headers. Feel free to change the theme and other display settings, but this is not required.

### Overview
For this project, we are seeking to determine the feasibility, diagnostic and predictive power of unstructured text data (from medical notes) to diagnose heart failure with reduced ejection fraction (HFrEF). We are using the Medical Information Mart for Intensive Care (MIMIC)-III dataset for this project. Specifically, we are seeking to determine the feasibility of extracting pertinent components of the unstructured text of medical note (e.g. discharge summary) using natural language processing (NLP) methods to assist in making a heart failure diagnosis. Secondarily, we aim to compare the diagnostic and predictic power of the unstructured text to conventional factors in making a heart failure diagnosis. 

### Introduction 
Heart failure (HF) is a progressive clinical syndrome resulting from any structural or functional cardiac disorder that impairs ability of the ventricle to fill or eject blood. It carries a poor prognosis with 50% mortality within the first 5 years of diagnosis, and is associated with significant co-morbidity and remarkable decrease in quality of life and functional status. Unfortunately, recent data suggest HF-associated mortality and morbidity are on the rise. While the reason for the HF-related poor outcomes is multifactorial, underdiagnosis and time-sensitive diagnosis play a significant contributory role leading to poor cardiovascular outcomes. This is mainly because the diagnosis of heart failure is a clinical and thus, it requires compilation of various data points including detailed and thorough history and physical exam.


### Methods
For this project, we are using MIMIC-III which is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. This is a vast dataset with many variables and limited access. It comes in form of various datasets which were then cleaned. Datatables containing basic demographics, ICD codes and note events were then merged. Heart failure with reduced ejection fraction (HFrEF) definition (using definition from https://phekb.org/  was used to define case and control populations. 

#Retrieving datatables
```{r eval = TRUE}
library(data.table)
library(ggplot2)
library(dplyr)
library(magrittr)
library(pROC)
NOTEEVENTS <- read.csv("~/Downloads/NOTEEVENTS.csv", comment.char="#")
PATIENTS <- read.csv("~/Downloads/PATIENTS.csv")
DIAGNOSES_ICD <- read.csv("~/Downloads/DIAGNOSES_ICD_3.csv", header=TRUE)
ADMISSIONS <- read.csv("~/Downloads/ADMISSIONS.csv", header=TRUE)
```

#Query the data, QI and cleaning
```{r eval = TRUE}
#Quering the dataset
head(PATIENTS)
head(DIAGNOSES_ICD)
head(NOTEEVENTS)

#uniquea <-length(unique(ADMISSIONS$SUBJECT_ID)) #46,520 unique subject IDs
#uniqued <-length(unique(DIAGNOSES_ICD$SUBJECT_ID)) #46,520 unique subject IDs
#uniquep <-length(unique(PATIENTS$SUBJECT_ID)) #46,520 unique subject IDs
#uniquens <-length(unique(NOTEEVENTS$SUBJECT_ID)) #46146 unique subject IDs

#Duplicates in subject ID for patient dataset. 
library(dplyr)
PATIENTS %>% 
group_by(SUBJECT_ID) %>% 
  filter(n()>1) #There is no duplicate. 

#First let's look at the various types of notes in category 
#unique_c <-unique(NOTEEVENTS$CATEGORY) 
#unique_nd <-unique(NOTEEVENTS$DESCRIPTION)
#NoteSum <-sum(NOTEEVENTS$CATEGORY == "Discharge summary")
```

##Results
#Creating a basic demographic datasets
```{r eval = TRUE}
library(dplyr)
library(plyr)
#Joining datasets PATIENTS with ADMISSIONS which contain demographics including ethnicity, insurance, etc. 
Demographic<- dplyr::inner_join(PATIENTS, ADMISSIONS, by = "SUBJECT_ID")
Demographic <-select(Demographic, SUBJECT_ID, GENDER, ADMISSION_TYPE, ETHNICITY, MARITAL_STATUS, INSURANCE) 

#Removing duplicate rows by subject_ID. This was done after querying the data to visualize what constitutes duplicated rows and what data is contained within those rows. 
Demographic <- Demographic[!duplicated(Demographic$SUBJECT_ID), ]
```

#Creating a basic demographic tables
```{r eval = TRUE}
#Table 1. Basic demographic of the cohort
library (gtsummary)
Demographic  %>% gtsummary::tbl_summary()

#Table 2. Basic demographic of the cohort by gender
Demographic %>% gtsummary::tbl_summary(by = GENDER)

#Table 3. Basic demographic of the cohort by insurance status 
Demographic %>% gtsummary::tbl_summary(by = INSURANCE)
```

#Visualizing our cohort dataset
```{r eval = TRUE}
#Visualizing the data
library(ggplot2)

#Figure 1.Visualizing Insurance status in cohort
ggplot(data = Demographic, aes(x = INSURANCE)) +
    geom_bar()

#Figure 2. Visualizing Insurance status across age in cohort 
ggplot(data = Demographic, aes(x =INSURANCE, fill = GENDER)) +
    geom_bar(position = "dodge")
    geom_bar()
    
#Figure 3. Visualizing marital status across age in cohort
ggplot(data = Demographic, aes(x =  MARITAL_STATUS, fill = GENDER)) +
    geom_bar(position = "dodge")
    geom_bar()
    
#Figure 4. Visualizing Ethnicity breakdown in cohort
ggplot(data = Demographic, aes(x = ETHNICITY)) + 
    geom_bar()

#Figure 5. Visualizing various demographics in one chart. Very interesting the breakdown of admission types by insurers. 
ggplot(data = Demographic, aes(x = ADMISSION_TYPE, fill = (GENDER))) + 
    geom_bar(position = "dodge") +
    facet_grid(. ~INSURANCE) + #Split by another variable
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#Creating a new heart failure status variable. 
```{r eval = TRUE}
#Creating a new binary column called HF Status indicative of heart failure (HF) or non-HF (noHF)
DIAGNOSES_ICD<- DIAGNOSES_ICD %>%
  mutate(HF_Status = ifelse(ICD9_CODE == 42840 | ICD9_CODE == 42841 | ICD9_CODE == 42842, "HF", "noHF"))

#Pulling ICD codes from Diagnoses table for case/cohort definition.
HF_ICD9 <- filter(DIAGNOSES_ICD, HF_Status == "HF")

#Pulling ICD codes from Diagnoses table for case/cohort definition.
nHF_ICD9 <- filter(DIAGNOSES_ICD, HF_Status == "noHF")

#Table 4. Heart failure vs non-heart failure cohort. 
print(table(DIAGNOSES_ICD$HF_Status))

```


#Creating the heart failure case cohort (NOTEEVENTS_ICD9_HF)
#Joining tables NOTEEVENTS data to ICD codes
```{r eval = TRUE}
#Dropping row_id as it is not needed. 
library(dplyr)
library(plyr) 
HF_ICD9 <- select(HF_ICD9, select = -ROW_ID)
nHF_ICD9 <- select(nHF_ICD9, select = -ROW_ID)

#Defining case cohort
#Joining by unique hospital admission ID aka HADM_ID. That is we are joining a specific ICD9 code of an encounter to that same encounter in the NOTEEVENTS
NOTEEVENTS_ICD9_HF <- dplyr::inner_join(NOTEEVENTS, HF_ICD9, by = "HADM_ID")

#Selecting for encounters with discharge summary and echo reports.  
NOTEEVENTS_ICD9_HF<- filter(NOTEEVENTS_ICD9_HF, CATEGORY == 'Discharge summary'| CATEGORY == 'Echo' )

#Dropping a few more variables (charttime, storetime and iserror) which are not needed
NOTEEVENTS_ICD9_HF <- dplyr::select(NOTEEVENTS_ICD9_HF, -CHARTTIME, -STORETIME, -ISERROR)
NOTEEVENTS_ICD9_HF<- filter(NOTEEVENTS_ICD9_HF, DESCRIPTION != 'Addendum')

#Characteristics of heart failure cohort
HF_demo <- dplyr::inner_join (Demographic, HF_ICD9, by = "SUBJECT_ID") 

#Figure 6. Visualizing the heart failure cohort 
ggplot(data = HF_demo, aes(x =INSURANCE, fill = GENDER)) +
    geom_bar(position = "dodge")
    geom_bar()
```

#Joining table to create our our overall cohort (both case and control)
```{r eval = TRUE}
#Herein, we want to create a dataset including the patient ID, note type (discharge summary, echoreport), unique note ID, HADM_ID, column text, HF_case (1 or 0, 1 for case, 0 for cohort), column text. 

#Joining notevents with icd9 codes
DIAGNOSES_ICD_n <- select(DIAGNOSES_ICD, -ROW_ID)
NOTEEVENTS_ICD9 <- dplyr::inner_join(NOTEEVENTS, DIAGNOSES_ICD_n, by = "HADM_ID")

#Removing obs or noted encounters without associated discharge summaries or echo reports
NOTEEVENTS_ICD9 <- filter(NOTEEVENTS_ICD9, CATEGORY == 'Discharge summary'| CATEGORY == 'Echo') #large datset as one patient or one admission will have 7-8 different ICD9 codes for billing. 

#Turning the HF_status into a factor (0= noHF, 1 = HF)
NOTEEVENTS_ICD9$HF_Status <- ifelse(NOTEEVENTS_ICD9$HF_Status == "HF", 1,0)
table(NOTEEVENTS_ICD9$HF_Status)
  
#Dropping a few more variables (charttime, storetime and iserror) which are not needed
NOTEEVENTS_ICD9 <- dplyr::select(NOTEEVENTS_ICD9, -CHARTTIME, -STORETIME, -ISERROR)
NOTEEVENTS_ICD9 <- filter(NOTEEVENTS_ICD9, DESCRIPTION != 'Addendum') #removing discharge with addendum as not needed.
```

#Creating the heart failure control cohort (NOTEEVENTS_ICD9_nHF)
```{r eval = TRUE}
#Joining by unique hospital admission ID aka HADM_ID. That is we are joining a specific ICD9 code of an encounter to that same encounter in the NOTEEVENTS
NOTEEVENTS_ICD9_nHF <- dplyr::inner_join(NOTEEVENTS, nHF_ICD9, by = "HADM_ID")

#Selecting for encounters with discharge summary and echo reports.  
NOTEEVENTS_ICD9_nHF<- filter(NOTEEVENTS_ICD9_nHF, CATEGORY == 'Discharge summary'| CATEGORY == 'Echo' )

#Dropping a few more variables (charttime, storetime and iserror) which are not needed
NOTEEVENTS_ICD9_nHF <- dplyr::select(NOTEEVENTS_ICD9_nHF, -CHARTTIME, -STORETIME, -ISERROR)
NOTEEVENTS_ICD9_nHF <- filter(NOTEEVENTS_ICD9_nHF, DESCRIPTION != 'Addendum')

#Characteristics of heart failure cohort
nHF_demo <- dplyr::inner_join (Demographic, nHF_ICD9, by = "SUBJECT_ID") 
```



#Doing more analysis to visualize the data
```{r eval = TRUE}
#Figure 7. Visualizing the heart failure cohort 
ggplot(data = nHF_demo, aes(x =INSURANCE, fill = GENDER)) +
    geom_bar(position = "dodge")
    geom_bar()
```


#Doing a logistic regression model to***
```{r eval = TRUE}
#Looking at the results from above, there is difference in the distribution of insurance types by cohorts. We will run a basic logistic regression looking at insurance and HF outcomes. 

#Logistic regression
#*****
```



#Further subdiving the cohorts into subcohorts containing echos and discharge summaries cases. 
```{r eval = TRUE}
#Selecting for specific variables needed for the text file analysis.
HF_Sub <- dplyr::select(NOTEEVENTS_ICD9_HF, SUBJECT_ID.x, HADM_ID, ROW_ID, CATEGORY, HF_Status, TEXT)
nHF_Sub <- dplyr::select(NOTEEVENTS_ICD9_nHF, SUBJECT_ID.x, HADM_ID, ROW_ID, CATEGORY, HF_Status, TEXT)

#Creating a heart failure subcohort containing just 'discharge' notes in the text column
HF_Sub_disch <- filter(HF_Sub, CATEGORY == 'Discharge summary')
nHF_Sub_disch <- filter(nHF_Sub, CATEGORY == 'Discharge summary')

#Create a second heart failure subcohort with just 'echo" notes in the text column. 
HF_Sub_echo <-filter(HF_Sub, CATEGORY == 'Echo')
nHF_Sub_echo <-filter(nHF_Sub, CATEGORY == 'Echo')


#Will create two subcohorts with 50 patients each for the text file analysis in each subgroups. 
HF_Sub_disch_c <- HF_Sub_disch [1:50, ]
HF_Sub_echo_c <- HF_Sub_echo[1:75, ]
  
nHF_Sub_echo_c <-nHF_Sub_echo [1:50, ]
nHF_Sub_disch_c <-nHF_Sub_disch [1:75, ]
```


#create a text file to be used in Python and cTakes for NLP text analysis.
```{r eval = TRUE}

#Text files for heart failure case cohort_echo
for (row in 1:nrow(HF_Sub_echo_c)) {
    A <- HF_Sub_echo_c[row, "SUBJECT_ID.x"]
    B <- HF_Sub_echo_c[row, "HADM_ID"] 
    C <- HF_Sub_echo_c[row, "ROW_ID"] 
    D <- HF_Sub_echo_c[row, "CATEGORY"]
    E <- HF_Sub_echo_c[row, "HF_Status"]
    G <- HF_Sub_echo_c[row, "TEXT"]
    Filename <-paste(A,B,C,D,E,".txt", sep = "_") 

    write.table(G, file = file.path("/Users/godefroychery/BMIN503_Final_Project/HF_case_echo/", Filename), sep = "\t",
            row.names = TRUE, col.names = TRUE)
}

#Text files for heart failure case cohort_discharge
for (row in 1:nrow(HF_Sub_disch_c)) {
    A <- HF_Sub_disch_c[row, "SUBJECT_ID.x"]
    B <- HF_Sub_disch_c[row, "HADM_ID"] 
    C <- HF_Sub_disch_c[row, "ROW_ID"] 
    D <- HF_Sub_disch_c[row, "CATEGORY"]
    E <- HF_Sub_disch_c[row, "HF_Status"]
    G <- HF_Sub_disch_c[row, "TEXT"]
    Filename <-paste(A,B,C,D,E,".txt", sep = "_") 

    write.table(G, file = file.path("/Users/godefroychery/BMIN503_Final_Project/HF_case_disch/", Filename), sep = "\t",
            row.names = TRUE, col.names = TRUE)
}

#Now creating separate text files for the control with echo
for (row in 1:nrow(nHF_Sub_echo_c)) {
    A <- nHF_Sub_echo_c[row, "SUBJECT_ID.x"]
    B <- nHF_Sub_echo_c[row, "HADM_ID"] 
    C <- nHF_Sub_echo_c[row, "ROW_ID"] 
    D <- nHF_Sub_echo_c[row, "CATEGORY"]
    E <- nHF_Sub_echo_c[row, "HF_Status"]
    G <- nHF_Sub_echo_c[row, "TEXT"]
    Filename <-paste(A,B,C,D,E,".txt", sep = "_") 

    write.table(G, file = file.path("/Users/godefroychery/BMIN503_Final_Project/HF_control_echo/", Filename), sep = "\t",
            row.names = TRUE, col.names = TRUE)
}

#Now creating separate text files for the control with discharge summaries
for (row in 1:nrow(nHF_Sub_disch_c)) {
    A <- nHF_Sub_disch_c[row, "SUBJECT_ID.x"]
    B <- nHF_Sub_disch_c[row, "HADM_ID"] 
    C <- nHF_Sub_disch_c[row, "ROW_ID"] 
    D <- nHF_Sub_disch_c[row, "CATEGORY"]
    E <- nHF_Sub_disch_c[row, "HF_Status"]
    G <- nHF_Sub_disch_c[row, "TEXT"]
    Filename <-paste(A,B,C,D,E,".txt", sep = "_") 

    write.table(G, file = file.path("/Users/godefroychery/BMIN503_Final_Project/HF_control_disch/", Filename), sep = "\t",
            row.names = TRUE, col.names = TRUE)
}
```

#Explaination of codes
#Applying NLP using Python codes to encode for/extract left ventricular ejection fraction (LVEF) for the cohorts. 
```{r eval = TRUE}
#Upon further investigation of the "text" column, one notices that the LVEF value is often listed in a paragraph with a heading for left ventricle systolic function or within a table format listing the LVEF. At times, it is located within the body of the paragraph. As such, we adopted recently published work from Wagholikar et al. ( https://doi.org/10.1007/s10916-018-1066-7), edited and modified it for our project. Please see our codes in python for further details.

#For our project, the logic remains consistent in that it will retrieve the left ventricular ejection fraction by searching a tab for 1) a tabular pattern, 2) a section for the left ventricle with numerical and range patterns, and 3) qualitative expressions in decreasing order of precedence. Please see our codes in python for further details.

```


#Results
#Tables containing results (LVEF extraction) from subcohorts
```{r eval = TRUE}
#Import tables with LVEF from desktops
#Table containing extracted LVEF from discharge summary (unstructured text) of case cohort. 
HF_case_disch_LVEF <- HF_case_disch_EF_extractions_NLP
head(HF_case_disch_LVEF)

#Table containing extracted LVEF from echo reports (unstructured text) of case cohort.
HF_case_echo_LVEF <- HF_case_echo_EF_extractions_NLP
head(HF_case_echo_LVEF)

#Table containing extracted LVEF from discharge summary (unstructured text) of control cohort.
HF_control_disch_LVEF <- HF_control_disch_EF_extractions_NLP
head(HF_control_disch_LVEF)

#Table containing extracted LVEF from echo reports (unstructured text) of control cohort.
HF_control_echo_LVEF <- HF_control_echo_EF_extractions_NLP
head(HF_control_echo_LVEF)
```

#Results
#Confusion Matrix of the main cohort
#Evaluation and validation of the rule logic of our project by comparing the extracted predicted output with a manually annotation by an expert. 
```{r eval = TRUE}
#Creating a confusion table of the entire cohort 
head(HF_main_cohort_confusion_table)

#Calculating Precision, Recall, Accuracy and F-1 score.
#Accuracy = TP+TN / TP+TN+FP+FN = (78+20) / (78+20+13+2) = 0.79
#Precision = TP / TP+FP = 78 (78+13) = 0.86
#Recall = TP /TP+FN = 78 / (78+2) =0.97
#F-1 score = F1 = 2 * (precision * recall) / (precision + recall) = 0.91


#Confusion tables for the case and control cohorts. 
head(HF_control_confusion_table)
head(HF_case_confusion_table)

#Control cohort
#Precision, recall
#Precision = TP / TP+FP = 6/ (6+ 3) = 0.66
#Recall = TP /TP+FN = 6/ (6+0) = 1


#Case cohort
#Precision, Recall
#Precision = TP / TP+FP = 72 / (72+10) = 0.87
#Recall = TP /TP+FN = 72 / (72+2) = 0.97
```


#Conclusion


#Limitations 



