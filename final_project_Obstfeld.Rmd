---
title: "EPID 600 Project Template"
author: "Amrom Obstfeld"
output: 
  html_document:
    toc: false 
    depth: 3 
---
```{r set-options, echo=FALSE, cache=FALSE}
options(width = 400)
```  
***

# Predicting Screening Coagulation Studies Using Coagulation Factor Results

### By Amrom Obstfeld

## Overview
The interpretation of coagulation studies in clinical practice can be hampered by lack of understanding of the relationship between screening coagulation studies and followup coagulation factor results that are ordered in order to expain abnormalities in these studies. In order to allucidate these relationships I will acquire deideintified laboratory results from CHOP and HUP data warehouses and use statistical and machine learning tools. My ultimate goal is to use these tools in clinical practice to guide clinicians towards appropriate coagulation tests.

## Introduction
Screening coagulation studies such as the partial thromboplastin time (PTT) and the prothrombin time (PT) are used to assess the risk of bleeding patients. When abnormalities are seen in one or both of these, additional testing is performed to identify the specific blood  coagulation factors that are abnormal. The univariate relationships between the screening studies and the specific factors have been elucidated using empiric laboratory evidence. However these relationships have not been validated using real patient data. Furthermore the multivariate relationships are more difficult to assess in the laboratory. Using accurate real-world models will allow hematologists and coagulation laboratories to better assess the bleeding risk of patients and suggest additional studies when screening results are inconsistent with the results of the coagulation factor levels.

Addressing this problem requires a multidisciplinary approach. Specifically, in depth knowledge of the pathophysiology of several hematological disorders is necessary in order to steer clear of patients with data that wouuld obscure this relationship. For instance, patients with elevated PTT levels as a results of anti-phospholipid antibodies would confound results due to the kinetics of the antibodies present in this situation. Input from pathology and laboratory medicine brings an in depth knowledge of the way the data was generated and allows the project to avoid additional potential confounding variables. For instance, the precise relationship between screening studies and coagulation factor levels may be impacted by changes in reagent manufactururers, changes in lots, or changes in instrumentation. Finally, data scientists provide insight into the process of exploring data and using knowledge of the data, such as its size, quality, and nature, to inform the classifier development approach. 


### Methods
In the first paragraph, describe the data used and general methodological approach. Subsequently, incorporate full R code necessary to retrieve and clean data, and perform analysis. Be sure to include a description of code so that others (including your future self) can understand what you are doing and why. 

Two independent datasets containing results from coagulation testing were obtained. The first of these data sets was obtained from a clinical laboratory data repository maintained by the Department of Pathology at Penn Medicine. The second of these datasets was exported out of the CHOP Data warehouse. 

```{r}
library(readr)
# penn <- read_csv(
#   "~/Rprojects/EPID600_Final_Project/coag_res.csv",
#   col_types = cols(
#     drawn_date = col_datetime(format = "%m/%d/%Y %H:%M"),
#     ord_date = col_datetime(format = "%m/%d/%Y %H:%M"),
#     pat_adm_dt = col_datetime(format = "%m/%d/%Y %H:%M"),
#     result = col_character()
#   )
# )

penn <-
  read_csv(
    "~/Rprojects/EPID600_Final_Project/coag_res.v2.csv.gz",
    col_types = cols(
      drawn_date = col_datetime(format = "%m/%d/%Y %H:%M"),
      ord_date = col_datetime(format = "%m/%d/%Y %H:%M"),
      pat_adm_dt = col_datetime(format = "%m/%d/%Y %H:%M"),
      result = col_character()
    )
  )

```

Summary views of the data were reviewed

```{r}
str(penn)
summary(penn)
#May want to explore how NA's are correlate, may want to see how the two value fields relate to each other and to the result.
```
Columns of relavence were selected
```{r}
library(dplyr)
cols<-c(5,17,21,7,12,14,15)
penn<-penn%>%select(cols)
```
Task_name is the variable that holds the test name. Need to filter dataset to tests relavent to the PT and PTT screening tests

```{r}
table(penn$task_name)
#filter out extraneous testing pulled out of warehouse
penn<-penn%>%filter(!task_name %in% c('Factor XIII', 'PT 0 Patient',  'PT 30 Patient','PT TEMP','PTT 0 Patient', 'PTT 30 Patient','PTT 60 Patient','TTI','PT WB','PTT WB','DTT','PT 60 Patient'))
```


All 'result' values should be numeric, need to explore results that are non-numeric.
```{r}
#Index by na's introduced by as.numeric coercion
penn%>%select(2,4)%>%filter(is.na(as.numeric(result)))%>%table
```
All textbased comments in the result field indicate fundamental issues with the result and should be filtered out. Many of the remainder are due to a measurement being out of range high or low. most of these can be kept and replaced with the numeric value indicated in the field for the purposes of this analysis
```{r}
#Explore weird results
#Replace with NA those results without numbers
penn$result[grepl("[:alpha:]",penn$result)]<-NA
penn$result[grepl("[:a-z:]",penn$result)]<-NA
penn$result[grepl("[:A-Z:]",penn$result)]<-NA
#Results that are 'greater than' a number (such as 150, 90, etc) should be replaced with that number. 
penn$result<-gsub(">", "", penn$result) 
#Less than with a small number should be replaced by 0. 
penn$result<-gsub("<", "", penn$result)
#Text results should be filtered out of dataset.
penn<-penn%>%filter(!is.na(result))
#Convert result variable to numeric
penn$result<-as.numeric(penn$result)
#confirm removal of all non-numeric results
penn%>%filter(is.na(result))%>%count%>%as.numeric

```

A histogram was created in order to see the amount  of each kind of result in the dataset. The plot demonstrates that the vast majority of tests included only screening tests and not individuals factors. These will have to be filtered from the data set (see below).

```{r}
library(knitr)
library(ggplot2)
ggplot(penn)+
  geom_bar(aes(x=task_name))+
  scale_y_log10()
knitr::kable(penn%>%group_by(task_name)%>%summarise(n=n())%>%arrange(desc(n)))

```



The accession number is essentially the order number. This number groups labs that were performed on the same sample. This is the unit upon which we will group the dependent and independent variables. The 'empi' is also important as it represents the patient identifier in this de-identified data set.
To facilitate analysis, the data was reshaped to a spread format, such that the independent and dependent variables now present in the 'task_name' variables are spread out in there own columns. Results are grouped by accession number. Before doing so, data cleaning was performed.

```{r}
library(tidyr)
library(lubridate)
#Before spreading data need to ensure that all entries are unique
dblresults<-penn%>%group_by(accession,task_name)%>%summarise(n=n())%>%ungroup()%>%filter(n>1)
dupresults<-penn%>%group_by(accession,task_name,result)%>%summarise(n=n())%>%select(1,2,4)%>%ungroup()%>%filter(n>1)
```

There were `r nrow(dupresults)` entries with duplicated data. There were `r nrow(dblresults)-nrow(dupresults)` with two results for the same test in the same accession. The accessions with two different result values for the same test were further evaluated.

```{r}
acc<-left_join(dblresults,penn, by=c('accession' = 'accession', 'task_name' = 'task_name'))
```

Overall the number of duplications are fairly small. This appears to be due to duplication of a screening test (PT, PTT, or fibrinogen) off of an identical draw time. The percent difference between these was calculated. 

```{r}

acc%>%group_by(accession,task_name)%>%summarise(n=n(),mean=mean(result),stdev=sd(result),cv=sd(result)/mean(result))%>%filter(n>1)

acc.rm<-acc%>%group_by(accession,task_name)%>%summarise(n=n(),mean=mean(result),stdev=sd(result),cv=sd(result)/mean(result))%>%filter(n>1)%>%filter(cv>.2)
```

Nine of these differences were greater than 20%. As a result the agreement was considered close enough for the analysis in this project and these values were replaced with the average of these two results. The following dataframe accounts for this correction and spreads variables within task_name across new column fields with values coming from 


```{r}
# penndf1<-anti_join(penn,acc.rm,by=c('accession' = 'accession', 'task_name' = 'task_name'))%>%
#   group_by(empi,accession,task_name)%>%summarise(result=mean(result))
# 
# penndf1<-left_join(penndf1,penn,by=c('accession' = 'accession', 'task_name' = 'task_name'))
# penndf1<-penndf1%>%ungroup()%>%select(c(1:4,6))
# penndf1$recvd_dt_tm<-as.Date(penndf1$recvd_dt_tm)
# penndf1<-penndf1%>%distinct()
# penndf1<-penndf1%>%select(-c(2))
# penndf1<-penndf1%>%group_by(empi.x,recvd_dt_tm,task_name)%>%spread(task_name,result.x)
# penndf1%>%slice(c(254310,254314))


penn<-anti_join(penn,acc.rm,by=c('accession' = 'accession', 'task_name' = 'task_name'))%>%
group_by(empi,accession,task_name)%>%summarise(result=mean(result))

#Now spread tests across column names for use during regression
penndf<-penn%>%spread(task_name,result)


str(penndf)
summary(penndf)

```



```{r}
# Want to insert normal values here as well. Multiple normal values present so will use the ones most frequently in the dataset.
normals<-penn%>%select(4,6,7)%>%group_by(task_name,normal_low,normal_high)%>%summarize(n=n())%>%ungroup()%>%group_by(task_name)%>%filter(n==max(n))
normals$task_name<-gsub(" ","",normals$task_name)

```



Most of these orders will likely only have PT or PTT, the screening test and the dependent variables in this analysis, as such these need to be filtered out since they will not contribute to the analysis. 

```{r}
#The following lines labels all rows with NAs in every independent predicting variable as a '10', any number less than 10 means that there is at least one 
penndf[,15]<-as.numeric()
names(penndf)[15]<-"allna"

penndf$allna<-rowSums(apply(is.na(penndf[,c(3:11,14)]), 2, as.numeric))


#Plot distribution of number of results for each acccession
ggplot(penndf)+
  geom_bar(aes(x=allna))+
  scale_y_log10()+
  ggtitle("Distribution of number of missing data per entry")+
  labs(x="Number of factor assays missing",y="Number of entries (log)",title="Distribution of number of missing data per entry")
```

As a result of the uselessness of having numerous accessions with dependent vairables but without the predictors these have been dropped. 
```{r}

#create final dataset with filtering. Rename variables to remove spaces

names(penndf)<-gsub(" ","",names(penndf))
penndf<-penndf%>%filter(allna<10)
summary(penndf)
str(penndf)

knitr::kable(penndf%>%gather('task_name','result',3:14)%>%filter(result!='NA')%>%group_by(task_name)%>%summarise(n=n())%>%arrange(desc(n)))
```

### Results
Describe your results and include relevant tables, plots, and code/comments used to obtain them. End with a brief conclusion of your findings related to the question you set out to address. You can include references if you'd like, but this is not required.

The first goal of this project is to validate many of the assumptions and data found within published literature as well as clinical practice in the field of laboratory coagulation testing. These assumptions include (1) the range of results in a normal population, and (2) the dependence of the PT and PTT on the individual factor assays.  Since the PT, PTT, and fibrinogen tests are using as screening tests, the dataset contains many of these tests and may be used to investigate the validity of the high and low normal range limits used in clinical practice in this lab. In contrast, the individual factor assays are only used when there is a high pre-test probability of an abnormality (generally there is already an abnormal screening test).

```{r}
tests<-c('PT','PTT','Fibrinogen')
f1<-list()
for (i in tests){
f1[[i]]<-penn%>%filter(task_name==i)%>%
  ggplot() +
  geom_density(aes(result), fill = 'gray') +
  geom_vline(xintercept = as.numeric(filter(normals, task_name == i)[2])) +
  geom_vline(xintercept = as.numeric(filter(normals, task_name == i)[3])) +
  xlab(paste0('Distribution of values for ',i)) +
  labs(caption = 'Vertical lines represent high and low end of normal range') +
  theme_bw()
}
```
The results indicate that the PTT normal range values match the distribution of results relatievly well, however the upper limit of normal may need to be reevaluated. The distribution of fibinrogen results demonstrate a peak at approximately 450, representing the results that were converted from >450 to 450 during the data preperation step. The consequencce of this clinically is minimal given that the low end of the normal range is more medically meaningful. Finally, there seems to be a discrepency between the normal range for the PT test and the distribution in this data. The results may be signnificantly biased as the PT is frequently used for monitoring of warfarin therapy. Future work will need to investigate these trends subsetted on individuals not on biasing medications. 

To investigate the association of individual factors to the screening PT and PTTs, bivariate plots and univariate regression was performed.

It is helpful when performing regression analysis to have a distribution of results. The Distributions of results were explored with histograms of screening lab results and specific coagulation factors (Figure 1). 

```{r}
library(gridExtra)
myhists <- list()  # new empty list
for (i in 1:length(colnames(penndf))) {
    p1 <- eval(substitute(
        ggplot(data=data.frame(penndf),aes(x=penndf[,i]))+ 
          geom_histogram(fill="grey") +
          geom_vline(xintercept = as.numeric(filter(normals,task_name==names(penndf)[i])[2]))+
          geom_vline(xintercept = as.numeric(filter(normals,task_name==names(penndf)[i])[3]))+
          xlab(paste0(names(penndf)[i]))+
          theme_bw()
    ,list(i = i)))
    #print(i)
    #print(p1)
    myhists[[i]] <- p1  # add each plot into plot list
    names(myhists)[[i]]<-names(penndf)[i]
}


grid.arrange(myhists[[3]], 
             myhists[[4]],
             myhists[[5]],
             myhists[[6]],
             myhists[[7]],
             myhists[[8]],
             myhists[[9]],
             myhists[[10]],
             myhists[[11]],
             myhists[[12]],
             myhists[[13]],
             myhists[[14]],
             nrow=3, ncol=4,top="Distribution of lab results in data set",  bottom="Figure 1")


```
The data demonstrates that there is a relatively evenly distributed set of results for predictors lab values. 

```{r}
library(gtools)


makebx.pt<-function(x,y){
    df<-na.omit(penndf[,c(x,y)])
    df[,x]<-quantcut(as.matrix(df[,x]))
    df<-as.data.frame(df)
    ggplot(df,aes_string(x=x ,y=y))+
          geom_boxplot() +
          xlab(names(df)[1])
    #print(p1)
    #bxpt[[x]] <- p1# add each plot into plot list
    #x<-x+1
    #gg
}
tests<-names(penndf)
bxpt<-lapply(tests,y='PT',makebx.pt)
names(bxpt)<-tests


grid.arrange(bxpt$FactorII, 
             bxpt$FactorV,
             bxpt$FactorX,
             bxpt$FactorVII,
             bxpt$FactorVIII,
             bxpt$FactorIX,
             bxpt$FactorXI,
             bxpt$FactorXII,
             bxpt$Fibrinogen,
             nrow=3, ncol=4,top="Box plots depicting relationship of coagulation factor quantiles and Prothrombin Time",  bottom="Figure 2")
```

```{r}

bxptt<-lapply(tests,y='PTT',makebx.pt)
names(bxptt)<-tests


grid.arrange(bxptt$FactorII, 
             bxptt$FactorV,
             bxptt$FactorX,
             bxptt$FactorVII,
             bxptt$FactorVIII,
             bxptt$FactorIX,
             bxptt$FactorXI,
             bxptt$FactorXII,
             bxptt$Fibrinogen,
             nrow=3, ncol=4,top="Box plots depicting relationship of coagulation factor quantiles and partial thromboplastin time",bottom="Figure 3")




```



To further explore the relationships amongst these measures, bivariate plots were created.
```{r}

makedot.pt<-function(x,y){
    df<-na.omit(penndf[,c(x,y)])
    df<-as.data.frame(df)
    ggplot(df,aes_string(x=x ,y=y))+
          geom_point(alpha=0.5) +
          theme_bw()+
          xlab(names(df)[1])
    #print(p1)
    #bxpt[[x]] <- p1# add each plot into plot list
    #x<-x+1
    #gg
    }
dotpt<-lapply(tests,y='PT',makedot.pt)
names(dotpt)<-tests


grid.arrange(dotpt$FactorII, 
             dotpt$FactorV,
             dotpt$FactorX,
             dotpt$FactorVII,
             dotpt$FactorVIII,
             dotpt$FactorIX,
             dotpt$FactorXI,
             dotpt$FactorXII,
             dotpt$Fibrinogen,
             nrow=3, ncol=4,top="Box plots depicting relationship of coagulation factor quantiles and partial thromboplastin time",bottom="Figure 3")#In the future suggest adding normal range vertical bars
```

```{r}

dotptt<-lapply(tests,y='PTT',makedot.pt)
names(dotptt)<-tests


grid.arrange(dotptt$FactorII, 
             dotptt$FactorV,
             dotptt$FactorX,
             dotptt$FactorVII,
             dotptt$FactorVIII,
             dotptt$FactorIX,
             dotptt$FactorXI,
             dotptt$FactorXII,
             dotptt$Fibrinogen,
             nrow=3, ncol=4,top="Scatterplots of independent variables vs PTT",  bottom="Figure 5")
#In the future suggest adding normal range vertical bars
```

```{r}
library(RColorBrewer)
bl<-brewer.pal(7,'Blues')[4:7]
gr<-brewer.pal(7,'Greens')[4:7]
re<-brewer.pal(1,'Reds')
cols<-c(bl,'red',gr)
cols[7]<-'red'
cols[5]<-"#41AB5D"

ggplot(penndf)+
  geom_smooth(aes(x=FactorVII,y=PT,color='FactorVII'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=Fibrinogen,y=PT,color='Fibrinogen'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorV,y=PT,color='FactorV'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorII,y=PT,color='FactorII'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorX,y=PT,color='FactorX'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorVIII,y=PT,color='Factor VIII'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorIX,y=PT,color='Factor IX'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorXI,y=PT,color='Factor XI'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorXII,y=PT,color='Factor XII'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  scale_colour_manual(name="legend", values=cols)+
  geom_hline(yintercept = as.numeric(normals$normal_low[10])) +
  geom_hline(yintercept = as.numeric(normals$normal_high[10]))+
  theme_bw()+
  xlim(0,150)


ggplot(penndf)+
  geom_smooth(aes(x=FactorVII,y=PTT,color='FactorVII'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=Fibrinogen,y=PTT,color='Fibrinogen'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorV,y=PTT,color='FactorV'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorII,y=PTT,color='FactorII'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorX,y=PTT,color='FactorX'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorVIII,y=PTT,color='Factor VIII'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorIX,y=PTT,color='Factor IX'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorXI,y=PTT,color='Factor XI'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  geom_smooth(aes(x=FactorXII,y=PTT,color='Factor XII'),method = "lm", formula = y ~ log(x),show.legend = TRUE,se=FALSE)+
  scale_colour_manual(name="legend", values=cols)+                     
  geom_hline(yintercept = as.numeric(normals$normal_low[10])) +
  geom_hline(yintercept = as.numeric(normals$normal_high[10]))+
  theme_bw()+
  xlim(0,150)

  scale_colour_manual(name="legend",
                      breaks=c('Fibrinogen','FactorII','FactorV','FactorX','FactorVIII','FactorIX','FactorXI','FactorXII','FactorVII'),
                      labels=c('Fibrinogen','FactorII','FactorV','FactorX','FactorVIII','Factor IX','Factor XI','Factor XII','Factor VII')values=c("red",'#00FF33','#33CC33','#00CC00','#006600','#0000FF','#0000CC','#3399CC','#3366CC'))+
```


A seperate analysis was performed specifically on the reptilase time as it itself is considered a screen for a fibrinogen abnormality. This was again performed using boxplots and bivariate scatterplots.



```{r}

p1<-ggplot(penndf)+
  geom_boxplot(aes(y=`ReptilaseTest`, x=quantcut(penndf$Fibrinogen,na.rm = TRUE)))

p2<-ggplot(penndf)+
  geom_point(aes(y=`ReptilaseTest`, x=penndf$Fibrinogen))

grid.arrange(p1,p2,nrow=1)

```

```{r}
attach(penndf)

#perofrm lm and store as list
pt.lm<-lapply( penndf[,-1], function(x) summary(lm(penndf$PT ~ x)) )
#Extract relavent parameters and store as dataframe lmresults.pt
variable<-names(pt.lm)
cols<-as.list(c(1:14))
beta.f<-function(x){
  pt.lm[x][[1]][[4]][2,1]
}

pvalue.f<-function(x){
  pt.lm[x][[1]][[4]][2,4]
}

arsquared.f<-function(x){
  pt.lm[x][[1]][[9]]
}

beta<-lapply(cols, beta.f)
pvalue<-lapply(cols, pvalue.f)
arsquare<-lapply(cols, arsquared.f)

lmresults.pt<-as.data.frame(cbind(variable=unlist(variable),beta=unlist(beta),pvalue=unlist(pvalue),arsquare=unlist(arsquare),log='linear'),stringsAsFactors = FALSE)
lmresults.pt[2:4]<-sapply(lmresults.pt[2:4],as.numeric)

penndf[penndf==0]<-1

#perofrm lm for pt on log of dependent variable and store as list
ptlg.lm<-lapply( penndf[,-1], function(x) summary(lm(penndf$PT ~ log(x))))
#Extract relavent parameters and store as dataframe lmresults.pt
variable<-names(ptlg.lm)
cols<-as.list(c(1:14))
beta.f<-function(x){
  ptlg.lm[x][[1]][[4]][2,1]
}

pvalue.f<-function(x){
  ptlg.lm[x][[1]][[4]][2,4]
}

arsquared.f<-function(x){
  ptlg.lm[x][[1]][[9]]
}

beta<-lapply(cols, beta.f)
pvalue<-lapply(cols, pvalue.f)
arsquare<-lapply(cols, arsquared.f)

lmresultslg.pt<-as.data.frame(cbind(variable=unlist(variable),beta=unlist(beta),pvalue=unlist(pvalue),arsquare=unlist(arsquare),log='log'),stringsAsFactors = FALSE)
lmresultslg.pt[2:4]<-sapply(lmresultslg.pt[2:4],as.numeric)
lmresults.pt<-bind_rows(lmresults.pt,lmresultslg.pt)%>%filter(!variable %in% c('accession','allna','PT','PTT'))

knitr::kable(arrange(lmresults.pt,variable))
```

The results of the univariate regression support much of the information and conventional wisdom in the literature. The most significant predictor of the prothrombin time are common pathway and extrinsic pathway coagulation factors (Factors X,II,VII). Interestingly, the logarithmic model produces the most robust predictors. Unexpectadly, two common pathway factors had little impact on the Prothrombin time, Factor V and Fibrinogen. The literature suggests that biologically, very little  Factor V activity is required for coagulation, which supports the results of this analysis. The lack of association with fibrinogen is somewhat surprusing given thatn the Reptilase time, which is solely dependent on fibrinogen is correlated with Prothrombin time. This may be because the fibrinogen quantification, which is assayed in diluted plasma, is less sensitivie to the inhibitors, such as fibrin split products, than the reptilase time, which is performed undiluted.  


```{r}
attach(penndf)

#perofrm lm and store as list
ptt.lm<-lapply( penndf[,-1], function(x) summary(lm(penndf$PTT ~ x)) )
#Extract relavent parameters and store as dataframe lmresults.ptt
variable<-names(ptt.lm)
cols<-as.list(c(1:14))
beta.f<-function(x){
  ptt.lm[x][[1]][[4]][2,1]
}

pvalue.f<-function(x){
  ptt.lm[x][[1]][[4]][2,4]
}

arsquared.f<-function(x){
  ptt.lm[x][[1]][[9]]
}

beta<-lapply(cols, beta.f)
pvalue<-lapply(cols, pvalue.f)
arsquare<-lapply(cols, arsquared.f)

lmresults.ptt<-as.data.frame(cbind(variable=unlist(variable),beta=unlist(beta),pvalue=unlist(pvalue),arsquare=unlist(arsquare),log='linear'),stringsAsFactors = FALSE)
lmresults.ptt[2:4]<-sapply(lmresults.ptt[2:4],as.numeric)


#perofrm lm for ptt on log of dependent variable and store as list
pttlg.lm<-lapply( penndf[,-1], function(x) summary(lm(penndf$PTT ~ log(x))))
#Extract relavent parameters and store as dataframe lmresults.ptt
variable<-names(pttlg.lm)
cols<-as.list(c(1:14))
beta.f<-function(x){
  pttlg.lm[x][[1]][[4]][2,1]
}

pvalue.f<-function(x){
  pttlg.lm[x][[1]][[4]][2,4]
}

arsquared.f<-function(x){
  pttlg.lm[x][[1]][[9]]
}

beta<-lapply(cols, beta.f)
pvalue<-lapply(cols, pvalue.f)
arsquare<-lapply(cols, arsquared.f)

lmresultslg.ptt<-as.data.frame(cbind(variable=unlist(variable),beta=unlist(beta),pvalue=unlist(pvalue),arsquare=unlist(arsquare),log='log'),stringsAsFactors = FALSE)
lmresultslg.ptt[2:4]<-sapply(lmresultslg.ptt[2:4],as.numeric)
lmresults.ptt<-bind_rows(lmresults.ptt,lmresultslg.ptt)%>%filter(!variable %in% c('accession','allna','PT','PTT'))
knitr::kable(arrange(lmresults.ptt,variable))

```

```{r}

#Two plots to look at the distribution of the lm statistics CONSIDER SCATTERPLOTS INSTEAD
lmresults.pt%>%filter(!variable %in% c('PT','PTT','accession','allna'))%>%filter(pvalue<0.05)%>%
ggplot()+
  geom_bar(stat='identity',aes(x=variable,y=beta,fill=log),position='dodge')+
  geom_hline(yintercept=0.05)+
  theme(axis.text.x = element_text(angle=60, hjust=1))
```

```{r}
lmresults.ptt%>%filter(!variable %in% c('PT','PTT','accession','allna'))%>%filter(pvalue<0.05)%>%
ggplot()+
  geom_bar(stat='identity',aes(x=variable,y=beta,fill=log),position='dodge')+
  geom_hline(yintercept=0.05)+
  theme(axis.text.x = element_text(angle=60, hjust=1))

```


Like the results of the linear regression models for the prothrombin time, the results for the partial thromboplastin time confirm many common assumptions about the relationships between these factors and provide some surprising results. By far the best predictors are factors VIII and XI, particularly the log transformed values. THese are both coagulation factors in the intrinsic pathway that are known to have a strong impact partial thromboplastin times. Amongst the factors that were not found to be associated with PTT were the common pathway factors, despite the fact that the common pathway is required for coagulation testing using the PTT. This result is most likely due to a combination of a relatively weak impact of these factors on PTT clotting times as well as the limitations in the number of datapoints in the dataset.    

```{r}
library(modelr)
library(purrr)
library(broom)
library(tidyr)
library('randomForest')
library('caret')


ptt.df<-penndf[!with(penndf,is.na(FactorVIII)|is.na(FactorXI) |is.na(FactorV)| is.na(FactorII)),]

  
  #K-Fold Cross Validation
  N = nrow(ptt.df)
  K = 5
  
  ptt.df$s = sample(1:K, size=N, replace=T)
  pred_outputs.lm <- vector(mode="numeric", length=N)
  pred_outputs.rf <- vector(mode="numeric", length=N)
  obs_outputs <- vector(mode="numeric", length=N)
  offset <- 0
  
  for(i in 1:K){
     
      train <- ptt.df%>%filter(s != i)
      test <-  ptt.df%>%filter(s == i)
      obs_outputs[1:length(ptt.df$s[ptt.df$s==i]) + offset] <- test$PTT
      
      #LM train/test
      lm <- lm(PTT ~ log(FactorVIII)+log(FactorXI)+log(FactorV)+log(FactorII), data=train)
      lm.pred.curr <- predict(lm, test)
      pred_outputs.lm[1:length(ptt.df$s[ptt.df$s==i]) + offset] <- lm.pred.curr
  
      #RF train/test
      rf <- randomForest(PTT ~(FactorVIII)+(FactorXI)+(FactorV)+(FactorII),ntree=100, data=train,na.action = na.omit)
      rf.pred.curr <- predict(rf, newdata=test) 
      pred_outputs.rf[1:length(ptt.df$s[ptt.df$s==i]) + offset] <- (rf.pred.curr)
  
      offset <- offset + length(ptt.df$s[ptt.df$s==i])
  }
  
  predictions.ptt<-data.frame(obs_outputs,pred_outputs.lm,pred_outputs.rf)
  predictions.ptt<-predictions.ptt%>%
    mutate(residual.lm = pred_outputs.lm - obs_outputs,
    residual.rf = pred_outputs.rf - obs_outputs,
    better=ifelse(abs(residual.lm)>abs(residual.rf),'Random Forest','Linear Model'))%>%na.omit
  
  rs.ptt <- predictions.ptt %>%na.omit()%>%
    summarise(
      sst = sum((obs_outputs - mean(obs_outputs)) ^ 2),
      sse.lm = sum(residual.lm^2),
      r.squared.lm = 1 -( sse.lm / sst),
      sse.rf = sum(residual.rf^2),
      r.squared.rf = 1 - (sse.rf / sst))
   rs.ptt
  
ggplot(data=predictions.ptt,aes(obs_outputs)) +
  geom_hline(yintercept = 0) +
  geom_point(aes(y=residual.lm),color='blue',alpha=0.3) +
  stat_smooth(method = "loess",aes(y=residual.lm),color='blue',fill='blue') +
  geom_point(aes(y=residual.rf),color='red',alpha=0.5) +
  stat_smooth(method = "loess",aes(y=residual.rf),color='red',fill='red') +
  #geom_segment(aes(x = obs_outputs, y = residual.lm, xend = obs_outputs, yend = residual.rf, colour = better),arrow=arrow(length = unit(0.01, "npc")),alpha=0.3)+ 
  scale_colour_manual(values = c("blue","red"),
                      guide = guide_legend(title = "Regression Models",
                                           keywidth = 3, 
                                          keyheight = 1,
                                          label.position = "left"))+
  geom_vline(xintercept = as.numeric(filter(normals, task_name == 'PTT')[2])) +
  geom_vline(xintercept = as.numeric(filter(normals, task_name == 'PTT')[3])) +
  theme_minimal(base_size = 20)+
  #theme(legend.position = c(0.75, .85))+
  labs(title = "Regression Models for PTT",
       subtitle= paste("The R-squared for the LM is",round(rs.ptt$r.squared.lm,2),"and the R-squared for the Randomforest is",round(rs.ptt$r.squared.rf,2)),
       x="Observed PTT (s)",
       y="Residuals")+
  xlim(20,100)




```



```{r}
library(modelr)
library(purrr)
library(broom)
library(tidyr)
library('randomForest')
library('caret')
library(e1071)


pt.df<-penndf[!with(penndf,is.na(FactorVII)|  is.na(Fibrinogen)| is.na(FactorX)| is.na(FactorV)| is.na(FactorII)),]
                

#K-Fold Cross Validation
N = nrow(pt.df)
K = 5

pt.df$s = sample(1:K, size=N, replace=T)
pred_outputs.lm <- vector(mode="numeric", length=N)
pred_outputs.rf <- vector(mode="numeric", length=N)
#pred_outputs.rsvm <- vector(mode="numeric", length=N)
obs_outputs <- vector(mode="numeric", length=N)
offset <- 0

for(i in 1:K){
    train <- filter(pt.df, s != i)
    test <- filter(pt.df, s == i)
    obs_outputs[1:length(pt.df$s[pt.df$s==i]) + offset] <- test$PT
    
    #LM train/test
    lm <- lm(PT ~ log(FactorVII)+log(FactorX)+log(FactorII)+log(FactorV)+log(Fibrinogen), data=train)
    lm.pred.curr <- predict(lm, test)
    pred_outputs.lm[1:length(pt.df$s[pt.df$s==i]) + offset] <- lm.pred.curr

    #RF train/test
    rf <- randomForest(PT ~(FactorVII)+(FactorX)+(FactorII)+FactorV+Fibrinogen,na.action = na.omit, data=train, ntree=100)
    rf.pred.curr <- predict(rf, newdata=test) 
    pred_outputs.rf[1:length(pt.df$s[pt.df$s==i]) + offset] <- (rf.pred.curr)

    #RF train/test
    # rsvm<-tune(svm, PT ~(FactorVII)+(FactorX)+(FactorII)+FactorV+Fibrinogen,  data = train,ranges = list(epsilon = seq(0,0.2,0.01), cost = 2^(2:9)))
    # rsvmbest <- rsvm$best.model
    # rsvm.pred.curr <- predict(rsvmbest, test) 
    # pred_outputs.rsvm[1:length(pt.df$s[pt.df$s==i]) + offset] <- (rsvm.pred.curr)

    
    
    # #Lasso train/test
    # trainlas<-na.omit(train[,c(3,5,6,8,11,12)])
    # x=as.matrix(trainlas[,1:5])
    # y=as.numeric(unlist(trainlas[,6]))
    # las <- cv.glmnet(x, y, family="gaussian", alpha=1)
    # x.test=as.matrix(test[,c(3,5,6,8,11)])
    # las.pred.curr <- predict(las, s = las$lambda.min,newx=x.test)
    # pred_outputs.las[1:length(pt.df$s[pt.df$s==i]) + offset] <- (las.pred.curr)

    offset <- offset + length(pt.df$s[pt.df$s==i])
} 

predictions.pt<-data.frame(obs_outputs,pred_outputs.lm,pred_outputs.rf)
predictions.pt<-predictions.pt%>%
  mutate(residual.lm = pred_outputs.lm - obs_outputs,
  residual.rf = pred_outputs.rf - obs_outputs,
  #residual.rsvm = pred_outputs.rsvm - obs_outputs,
  better=ifelse(abs(residual.lm)>abs(residual.rf),'Random Forest','Linear Model'))%>%na.omit


rs.pt <- predictions.pt %>%na.omit()%>%
  summarise(
    sst = sum((obs_outputs - mean(obs_outputs)) ^ 2),
    sse.lm = sum(residual.lm^2),
    r.squared.lm = 1 -( sse.lm / sst),
    sse.rf = sum(residual.rf^2),
    r.squared.rf = 1 - (sse.rf / sst))

  
ggplot(data=predictions.pt,aes(obs_outputs)) +
  geom_hline(yintercept = 0) +
  geom_point(aes(y=residual.lm),color='blue',alpha=0.3) +
  stat_smooth(method = "loess",aes(y=residual.lm),color='blue',fill='blue') +
  geom_point(aes(y=residual.rf),color='red',alpha=0.5) +
  stat_smooth(method = "loess",aes(y=residual.rf),color='red',fill='red') +
  #geom_segment(aes(x = obs_outputs, y = residual.lm, xend = obs_outputs, yend = residual.rf, colour = better),arrow=arrow(length = unit(0.01, "npc")),alpha=0.3)+ 
  geom_vline(xintercept = as.numeric(filter(normals, task_name == 'PT')[2])) +
  geom_vline(xintercept = as.numeric(filter(normals, task_name == 'PT')[3])) +
  theme_minimal(base_size = 20)+
  theme(legend.position = c(0.75, .85))+
  labs(title = "Regression Models for PT",
       subtitle= paste("The R-squared for the LM is",round(rs.pt$r.squared.lm,2),"and the R-squared for the Randomforest is",round(rs.pt$r.squared.rf,2)),
       x="Observed PT (s)",
       y="Residuals")+
  scale_colour_manual(values = c("blue","red"),
                      guide = guide_legend(title = "Regression Models",
                                           keywidth = 3, 
                                           keyheight = 1,
                                           label.position = "left"))




```



```{r}

library(readr)
chop <- read_csv("~/Rprojects/EPID600_Final_Project/factorsxlsx.csv", 
   col_types = cols(`Patient Date of Birth` = col_character()))
names(chop)<-c('empi','task_name','result','dob','ordcre','ordpl','col')
chop$ordcre<-as.POSIXct(chop$ordcre,format = "%m/%d/%y %H:%M")
chop$ordpl<-as.POSIXct(chop$ordpl,format = "%m/%d/%y %H:%M")
chop$col<-as.POSIXct(chop$col,format = "%m/%d/%y %H:%M")

table(chop$task_name)
#filter out extraneous testing pulled out of warehouse
chop<-chop%>%filter(!task_name %in% c('INHIBITOR SCREEN', 'Procedure Name','FACTOR XIII'))

chop[grep("FACTOR II", chop$task_name), 'task_name']<-'FactorII'
chop[grep("FACTOR V", chop$task_name), 'task_name']<-'FactorV'
chop[grep("FACTOR VII", chop$task_name), 'task_name']<-'FactorVII'
chop[grep("FACTOR VIII", chop$task_name), 'task_name']<-'FactorVIII'
chop[grep("FACTOR IX", chop$task_name), 'task_name']<-'FactorIX'
chop[grep("FACTOR X", chop$task_name), 'task_name']<-'FactorX'
chop[grep("FACTOR XI", chop$task_name), 'task_name']<-'FactorXI'
chop[grep("FACTOR XII ASSAY", chop$task_name), 'task_name']<-'FactorXII'
chop[grep("PT/INR", chop$task_name), 'task_name']<-'PT'
chop[grep("PARTIAL THROMBOPLASTIN TIME", chop$task_name), 'task_name']<-'PTT'
chop[grep("PTT PROFILE", chop$task_name), 'task_name']<-'PTT'
chop[grep("FIBRINOGEN", chop$task_name), 'task_name']<-'Fibrinogen'


```

```{r}
#Index by na's introduced by as.numeric coercion
chop%>%select(2,3)%>%filter(is.na(as.numeric(result)))%>%table
```
All textbased comments in the result field indicate fundamental issues with the result and should be filtered out. Many of the remainder are due to a measurement being out of range high or low. most of these can be kept and replaced with the numeric value indicated in the field for the purposes of this analysis
```{r}
#Explore weird results
#Replace with NA those results without numbers
chop$result<-gsub("@L", "", chop$result) 
chop$result[grepl("[:alpha:]",chop$result)]<-NA
chop$result[grepl("[:a-z:]",chop$result)]<-NA
chop$result[grepl("[:A-Z:]",chop$result)]<-NA
#Results that are 'greater than' a number (such as 150, 90, etc) should be replaced with that number. 
chop$result<-gsub(">", "", chop$result) 
#Less than with a small number should be replaced by 0. 
chop$result<-gsub("<", "", chop$result)
#Text results should be filtered out of dataset.
chop<-chop%>%filter(!is.na(result))
#Convert result variable to numeric
chop$result<-as.numeric(chop$result)
#confirm removal of all non-numeric results
chop%>%filter(is.na(result))%>%count%>%as.numeric
```

```{r}
library(tidyr)
library(lubridate)
#Before spreading data need to ensure that all entries are unique
dblresults<-chop%>%group_by(empi,col,task_name)%>%summarise(n=n())%>%ungroup()%>%filter(n>1)
chop<-anti_join(chop,dblresults,by='col')
chopdf<-chop%>%mutate(coldt=as.Date(col))%>%select(c(1,2,3,8))
dblresults<-chopdf%>%group_by(empi,coldt,task_name)%>%summarise(n=n())%>%ungroup()%>%filter(n>1)
chopdf<-anti_join(chopdf,dblresults,by=c('coldt' = 'coldt', 'empi' = 'empi'))
chopdfxx<-chopdf%>%spread(task_name,result)

chopptt.df<-chopdfxx[!with(chopdfxx,is.na(FactorVIII)| is.na(FactorIX)| is.na(FactorXI)),]
```

```{r}
library(modelr)
library(purrr)
library(broom)
library(tidyr)
library('randomForest')
library('caret')


ptt.df<-penndf[!with(penndf,is.na(FactorVIII)| is.na(FactorIX)| is.na(FactorXI)),]
                  

#K-Fold Cross Validation
N = nrow(ptt.df)
K = 5

ptt.df$s = sample(1:K, size=N, replace=T)
pred_outputs.lm <- vector(mode="numeric", length=N)
pred_outputs.rf <- vector(mode="numeric", length=N)
obs_outputs <- vector(mode="numeric", length=N)
offset <- 0

for(i in 1:K){
    train <- ptt.df%>%filter(s != i)
    test <-  ptt.df%>%filter(s == i)
    obs_outputs[1:length(ptt.df$s[ptt.df$s==i]) + offset] <- test$PTT
    
    #LM train/test
    lm <- lm(PTT ~ log(FactorVIII)+log(FactorIX)+log(FactorXI), data=ptt.df)
    lm.pred.curr <- predict(lm, test)
    pred_outputs.lm[1:length(ptt.df$s[ptt.df$s==i]) + offset] <- lm.pred.curr

    #RF train/test
    rf <- randomForest(PTT ~(FactorVIII)+(FactorIX)+(FactorXI), data=ptt.df, ntree=100,na.action = na.omit)
    rf.pred.curr <- predict(rf, newdata=test) 
    pred_outputs.rf[1:length(ptt.df$s[ptt.df$s==i]) + offset] <- (rf.pred.curr)

    offset <- offset + length(ptt.df$s[ptt.df$s==i])
}

predictions.ptt<-data.frame(obs_outputs,pred_outputs.lm,pred_outputs.rf)
predictions.ptt<-predictions.ptt%>%
  mutate(residual.lm = pred_outputs.lm - obs_outputs,
  residual.rf = pred_outputs.rf - obs_outputs,
  better=ifelse(abs(residual.lm)>abs(residual.rf),'Random Forest','Linear Model'))%>%na.omit

rs.ptt <- predictions.ptt %>%na.omit()%>%
  summarise(
    sst = sum((obs_outputs - mean(obs_outputs)) ^ 2),
    sse.lm = sum(residual.lm^2),
    r.squared.lm = 1 -( sse.lm / sst),
    sse.rf = sum(residual.rf^2),
    r.squared.rf = 1 - (sse.rf / sst))

#Build final rf and lm for test
lm <- lm(PTT ~ log(FactorVIII)+log(FactorIX)+log(FactorXI), data=ptt.df)
    lm.pred.curr <- predict(lm, test)
    pred_outputs.lm[1:length(ptt.df$s[ptt.df$s==i]) + offset] <- lm.pred.curr

    #RF train/test
    rf <- randomForest(PTT ~(FactorVIII)+(FactorIX)+(FactorXI), data=ptt.df, ntree=100,na.action = na.omit)

```



#impute data for multivariate regression
library('mice')
ptt.df<-ptt.df%>%select(4,7,9,13)
imp.ptt <- mice(ptt.df, m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imp.ptt)

stripplot(imp.ptt, pch = 20, cex = 1.2)
fit.imp.pt <- with(data = imp.ptt, lm(PT ~ log(FactorVIII) + log(FactorIX) + log(FactorXI))) 

#combine results of all 5 models
combine.pt <- pool(fit.imp.pt)
summary(combine.pt)


penndf.ptt<-penndf%>%select(4,8,10,14)
imp.ptt <- mice(penndf.ptt, m=5, maxit = 50, method = 'pmm', seed = 500)
fit.imp.ptt <- with(data = imp.ptt, exp = lm(PTT ~ log(FactorIX)+log(FactorVIII)+log(FactorXI))) 

summary(pool(fit.imp.ptt))

#combine results of all 5 models
combine.ptt <- pool(fit.imp.ptt)
summary(combine.ptt)
names(combine.ptt)
combine.ptt$nmis

```

The results indicate an association of the predictor variables that were identified in univariate analysis, however the multivariate will likely require additional data without as many missing values. 
